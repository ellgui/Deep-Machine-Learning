{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fa9f96d4ca0144b2db877078cf7b2f8",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = \"HA3_EllaGuiladi_EmmaRydholm.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"Ella Guiladi\" \n",
    "NAME2 = \"Emma Rydholm\"\n",
    "GROUP = \"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a5517d7993b4b35049f0013dd6a3f55",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','7'), \"You are not running Python 3.7. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d199303c73ec86d25177caf39e385f",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nb_dirname = os.path.abspath('')\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in ['IHA1', 'IHA2', 'HA1', 'HA2', 'HA3'], \\\n",
    "    '[ERROR] The notebook appears to have been moved from its original directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78c7227b049bb147e6c363affb6dae8",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>if(\"HA3_EllaGuiladi_EmmaRydholm.ipynb\" != IPython.notebook.notebook_name) { alert(\"You have filled in nb_fname = \\\"HA3_EllaGuiladi_EmmaRydholm.ipynb\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    display(HTML(r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(nb_fname=nb_fname)))\n",
    "except NameError:\n",
    "    assert False, 'Make sure to fill in the nb_fname variable above!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb700528d4644601c1a8c91ef1d84635",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching current notebook against the following URL:\n",
      "http://raw.githubusercontent.com/JulianoLagana/deep-machine-learning/master/home-assignments/HA3/HA3.ipynb\n",
      "[SUCCESS] No major notebook mismatch found when comparing to latest GitHub version. (There might be minor updates, but even that is the case, submitting your work based on this notebook version would be acceptable.)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b52f414ceeae87bacca0b836a2bea2f2",
     "grade": false,
     "grade_id": "cell-2f332c3ca731afc6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Home Assignment 3\n",
    "This home assignment will focus on reinforcement learning and deep reinforcement learning. The first part will cover value-table reinforcement learning techniques, and the second part will include neural networks as function approximators, i.e., deep reinforcement learning. \n",
    "\n",
    "When handing in this assignment, make sure that you're handing in the correct version, and more importantly, *that you do no clear any output from your cells*. We'll use these outputs to aid us when grading your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0a6e11384d620f14e7f545a9130133c",
     "grade": false,
     "grade_id": "cell-8122dcb8d8ca1c9e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 1: Gridworld\n",
    "\n",
    "In this task, you will implement Value Iteration to solve for the optimal policy, $\\pi^*$, and the corresponding state value function, $V^*$.\n",
    "\n",
    "The MDP you will work with in this assignment is illustrated in the figure below.\n",
    "\n",
    "![title](./grid_world.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ddafb4a3ebc91d6f23acbcdbad9f176",
     "grade": false,
     "grade_id": "cell-b4e5d5337fbaa0e5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The agent starts in one of the non-black squares shown in the above figure, and then proceeds to take actions. The available actions at any time step are: **North, West, South,** and **East**. If an action would make the agent bump into a wall, or one of the black (unreachable) states, it instead does nothing, leaving the agent at the same place it was before.\n",
    "\n",
    "The reward $\\mathcal{R}_s^a$ of being in state $s$ and performing actions $a$ is zero for all states, regardless of the action taken, with the exception of the green and the red squares. For the green square, the reward is always 1, and for the red square, always -1, regardless of the action.\n",
    "\n",
    "When the agent is either in the green or the red square, it will be transported to the terminal state in the next time step, regardless of the action taken. The terminal state is shown as the white square with the \"T\" inside.\n",
    "\n",
    "#### State representation\n",
    "The notations used to define the states are illustrated in the table below\n",
    "\n",
    "| $S_0$ | $S_1$ | $S_2$ | $S_3$ | $S_4$ |    |\n",
    "|-------|-------|-------|-------|-------|----|\n",
    "| $S_5$ | $S_6$ | $S_7$ | $S_8$ | $S_9$ |    |\n",
    "| $S_{10}$ | $S_{11}$ | $S_{12}$ | $S_{13}$ | $S_{14}$ | $S_{15}$|\n",
    "\n",
    "where $S_{10}$ corresponds to the initial state of the environment, $S_4$ and $S_9$ to the green and red states of the environment, and $S_{15}$ to the terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "713a1bc6ae8d6053a7f7d48681edad45",
     "grade": false,
     "grade_id": "cell-c54a0f7162b1f260",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### Task 1.1: Solve for $V^*(s)$ and $Q^*(s,a)$\n",
    "For this task all transition probabilities are assumed to be 1 (that is, trying to move in a certain direction will definitely move the agent in the chosen direction), and a discount factor of 0.9, i.e., $\\gamma=0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9894346da453882ad420d049511a5b8b",
     "grade": false,
     "grade_id": "cell-c7fa1d00113f314e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "* Solve for $V^*(S_{10})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e998cc86ed986eeac11f54b0f6869a67",
     "grade": true,
     "grade_id": "cell-966bc6b1276b31f1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "This can be solved by analysing the Bellman Optimality Equation. The Bellman Optimality Equation can be interpreted as that the optimal value of a state is equal to the reward after taking the best action plus the discounted value of the next states. There are 6 steps in the path from the initial state towards the red/green states with a discount factor of 0.9 in each state, the result will be $0.9^6\\approx 0.531$, which can be presented below when following one of the paths,\n",
    "\n",
    "$V^*(S_{10})=[r_{10}+\\gamma r_{5}+\\gamma^2 r_{0}+\\gamma^3 r_{1}+\\gamma^4 r_{2}+\\gamma^5 r_{3}+\\gamma^6 r_{4}]=0.9^6 \\approx 0.53$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2d30fc8031fa1ff3f06019a9cf1ba27",
     "grade": false,
     "grade_id": "cell-4cc15316add9bd67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "* Solve $Q^*(S_{10},a)$ for all actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad5e1409a936a5c1ec6684a1edae79ee",
     "grade": true,
     "grade_id": "cell-0e5efad7ed72fdcb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "By again, using the Bellman Optimality Equation, it can be seen that $Q^*(S_{10}, a)$ can be formulated in a similar matter as $V^*(S_{10})$, i.e. the optimal value of an action is equal to the reward after taking the best action plus the discounted value of the next states. This can be expressed as,\n",
    "\n",
    "$$Q^*(S_{10}, North) = r_{10,north} + \\gamma V^*(S_5) = 0 + 0.9^6 \\approx 0.531$$\n",
    "$$Q^*(S_{10}, East) = r_{10,east} + \\gamma V^*(S_{11}) = 0 + 0.9^6 \\approx 0.531$$\n",
    "$$Q^*(S_{10}, South) = r_{10,south} + \\gamma V^*(S_{10}) = 0 + 0.9^7 \\approx 0.478$$\n",
    "$$Q^*(S_{10}, West) = r_{10,west} + \\gamma V^*(S_{10}) = 0 + 0.9^7 \\approx 0.478$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5bf77a9b3969614145135ad2f307a13",
     "grade": false,
     "grade_id": "cell-e426e3815f78930a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "\n",
    "### Task 1.2 Write a mathematical expression relating $V^\\pi(s)$ to $Q^\\pi(s,a)$ and $\\pi(a|s)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61bf64e2c38d41b0f0d024faba554e8f",
     "grade": true,
     "grade_id": "cell-343c3ea4883085e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "The expression that relates $V^\\pi(s)$ to $Q^\\pi(s,a)$ and $\\pi(a|s)$ can be formulated as,\n",
    "\n",
    "$$V^\\pi (s) = \\sum_{a}{\\pi(a|s) \\, Q^\\pi(s,a)}$$\n",
    "\n",
    "and is the Bellman Expectation Equation, which presents that the state-value is equal the sum of the policy determining actions times respective action-values, telling us how good it is to be in a particular state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12a81cd5b1ca7f2b7908d96cea0e6214",
     "grade": false,
     "grade_id": "cell-ab80df325256cf89",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "###  Task 1.3: Value Iteration\n",
    "For this task, the transitions are no longer deterministic. Instead, there is a 0.2 probability that the agent will try to travel in an orthogonal direction of the chosen action (0.1 probability for each of the two orthogonal directions). Note that the Markov decision process is still known and does not have to be learned from experience.\n",
    "\n",
    "Your task is to implement value iteration and solve for the:\n",
    "* optimal greedy policy $\\pi^*(s)$ \n",
    "* $V^*(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a57b4664ce69a0c862f1fd6f4e2e614",
     "grade": false,
     "grade_id": "cell-74497ad9b13e8362",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### The value iteration algorithm\n",
    "Value iteration is an iterative algorithm used to compute the optimal value function $V^*(s)$. Each iteration starts with a guess of what the value function is and then uses the Bellman equations to improve this guess iteratively. We can describe one iteration of the algorithm as\n",
    "\n",
    "$\n",
    "\\textbf{For} ~ s \\in {\\cal S}:\\qquad  \\\\\n",
    "\\quad V_{k+1}(s) = \\underset{a \\in {\\cal A}}{\\text{max}}~ \\left( \\mathcal{R}_s^a + \\gamma \\underset{{s'\\in \\mathcal{S}}}{\\sum} \\mathcal{P}_{ss'}^a \\cdot V_k(s') \\right)\n",
    "$\n",
    "\n",
    "where $\\mathcal{P}_{ss'}^a={\\mathrm Pr}[S'=s'\\big|S=s,A=a]$ is the probability to transition from state $s$ to $s'$ given action $a$.\n",
    "\n",
    "\n",
    "#### The MDP Python class\n",
    "The Markov Decision Process you will work with is defined in `gridworld_mdp.py`. In the implementation, the actions are represented by integers as, North = 0, West = 1, South = 2, and East = 3.\n",
    "To interact with the MDP, you need to instantiate an object as: \n",
    "\n",
    "\n",
    "```python\n",
    "mdp = GridWorldMDP()\n",
    "```\n",
    "\n",
    "At your disposal there are a number of instance-functions implemented for you, and presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "636b8c5ea0bb7d3b4798564dfc7d580d",
     "grade": false,
     "grade_id": "cell-21e5d7b3d3083cd6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_states in module gridworld_mdp:\n",
      "\n",
      "get_states(self)\n",
      "    Returns complete set of states for the MDP\n",
      "    :return: numpy array of shape [num states,]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gridworld_mdp import *\n",
    "import numpy as np\n",
    "\n",
    "help(GridWorldMDP.get_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "620f4674b0caadf475b4889bd8747b62",
     "grade": false,
     "grade_id": "cell-9706322eb34e16db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module gridworld_mdp:\n",
      "\n",
      "__init__(self, trans_prob=0.8)\n",
      "    Initializes an instance of the GridWorldMDP class\n",
      "    :param trans_prob: transition probabilities (e.g. =1 for deterministic MDP)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The constructor\n",
    "help(GridWorldMDP.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b14ba756ee79811ccba63e1e51f14a",
     "grade": false,
     "grade_id": "cell-38d3ab6fb24c1af8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_actions in module gridworld_mdp:\n",
      "\n",
      "get_actions(self)\n",
      "    Returns complete set of actions for the MDP\n",
      "    :return: numpy array of shape [num actions,]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.get_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0b98cbe90baa640578ed1991b4e3501",
     "grade": false,
     "grade_id": "cell-ecb00397472a5faa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function state_transition_func in module gridworld_mdp:\n",
      "\n",
      "state_transition_func(self, s, a)\n",
      "    Returns the transition probabilities to all states given current state and action\n",
      "    :param state: current state as integer\n",
      "    :param action: selected action as integer\n",
      "    :return: state-transition probabilities, i.e.\n",
      "     [P[S_0| S=s, A_t=a], P[S_1| S=s, A=a], ..., P[S_14| S=s, A=a]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.state_transition_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40120f95b5767060a8ca55455cf5d485",
     "grade": false,
     "grade_id": "cell-aa8e1498649053a5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reward_function in module gridworld_mdp:\n",
      "\n",
      "reward_function(self, s, a)\n",
      "    Returns the reward r(s,a)\n",
      "    :param state: current state as integer\n",
      "    :param action: selected action as integer\n",
      "    :return: r(s,a)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8defb43e783bcb9464571753a5c6e452",
     "grade": false,
     "grade_id": "cell-c1408cc9707dd7f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We also provide two helper functions for visualizing the value function and the policies you obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b39ad0b689edcb447e156b888fbc0515",
     "grade": false,
     "grade_id": "cell-b754590784e24eb1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for printing a policy pi\n",
    "def print_policy(pi):\n",
    "    print('Policy for non-terminal states: ')\n",
    "    indencies = np.arange(1, 16)\n",
    "    txt = '| '\n",
    "    hor_delimiter = '---------------------'\n",
    "    print(hor_delimiter)\n",
    "    for a, i in zip(pi, indencies):\n",
    "        txt += mdp.act_to_char_dict[a] + ' | '\n",
    "        if i % 5 == 0:\n",
    "            print(txt + '\\n' + hor_delimiter)\n",
    "            txt = '| '\n",
    "    print('                            ---')\n",
    "    print('Policy for terminal state: |', mdp.act_to_char_dict[pi[15]],'|')\n",
    "    print('                            ---')            \n",
    "\n",
    "# Function for printing a table with of the value function\n",
    "def print_value_table(values, num_iterations=None):            \n",
    "    if num_iterations:\n",
    "        print('Values for non-terminal states after: ', num_iterations, 'iterations \\n', np.reshape(values, [3, 5]), '\\n')\n",
    "        print('Value for terminal state:', terminal_value, '\\n')\n",
    "    else: \n",
    "        terminal_value = values[-1]\n",
    "        print('Values for non-terminal states: \\n', np.reshape(values[:-1], [3, 5]))\n",
    "        print('Value for terminal state:', terminal_value, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ef181cf6fbf6f2b9c2947aba5a8fdb6",
     "grade": false,
     "grade_id": "cell-87e02763b23fe1f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "(**1 POE**) Now it's time for you to implement your own version of value iteration to solve for the greedy policy and $V^*(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dc5e9fa3ab59487087475860c657e2f",
     "grade": true,
     "grade_id": "cell-d473b99fe1825067",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(gamma, mdp):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        V - state value table, numpy array of shape (16,)\n",
    "        pi - greedy policy table, numpy array of shape (16,)\n",
    "    \"\"\"\n",
    "    V = np.zeros([16]) # state value table\n",
    "    Q = np.zeros([16, 4]) # state action value table\n",
    "    pi = np.zeros([16]) #greedy policy table\n",
    "    \n",
    "    states=mdp.get_states()\n",
    "    actions = mdp.get_actions()\n",
    "\n",
    "    for i in range(1000):\n",
    "        for s in states:            \n",
    "            for a in actions:\n",
    "                P = mdp.state_transition_func(s, a)\n",
    "                R = mdp.reward_function(s, a)\n",
    "                Q[s, a] = R + gamma * np.sum(P * V)\n",
    "            V[s] = np.max(Q[s])\n",
    "    \n",
    "    pi = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41ca129edbe5353b31ec35933bf28cdb",
     "grade": false,
     "grade_id": "cell-99c149095318adac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run your implementation for the deterministic version of our MDP. As a sanity check, compare your analytical solutions with the output from your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a8722a6c33a96991f1091e2418def51",
     "grade": false,
     "grade_id": "cell-bd495acfe33d405f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.6561    0.729     0.81      0.9       1.      ]\n",
      " [ 0.59049   0.        0.        0.81     -1.      ]\n",
      " [ 0.531441  0.59049   0.6561    0.729     0.6561  ]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | E | E | N | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "mdp = GridWorldMDP(trans_prob=1.)\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "172eb8e58e4dbdcca83bb2bb8589032d",
     "grade": false,
     "grade_id": "cell-5a24214a0645d4b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Once your implementation passed the sanity check, run it for the stochastic case, where the probability of an action succeding is 0.8, and 0.2 of moving the agent in an orthogonal direction to the intended. Use $\\gamma = .99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60e210ba654df60fcc54a7d6eda59aae",
     "grade": false,
     "grade_id": "cell-c6d0282ee295bb85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.93861973  0.95193393  0.9639533   0.97612443  1.        ]\n",
      " [ 0.92691625  0.          0.          0.88371826 -1.        ]\n",
      " [ 0.91395196  0.90255605  0.89130223  0.88057656  0.79978972]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | W | N | \n",
      "---------------------\n",
      "| N | W | W | W | S | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "# Run for stochastic MDP, gamma = .99\n",
    "mdp = GridWorldMDP()\n",
    "v, pi = value_iteration(.99, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4765d7cb5812236bf49085b356079d0",
     "grade": false,
     "grade_id": "cell-b80f5f5b9d1398a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** Does the policy that the algorithm found look reasonable? \n",
    "\n",
    "In particular, what's the policy for state $S_8$? Is that a good idea? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12691470ab1f2881c2087ad32395b3a5",
     "grade": true,
     "grade_id": "cell-daff5655fe78f131",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "The found algorithm looks reasonable, which can be seen by looking at the policy for state $S_8$. At state $S_8$, the policy gives a 0% chance of stepping into to red state, since the policy is to go west which indicates 0.8 chance of sucess (going west and getting stuck in the same state), and a 0.1 to go north respectively south. The majority of states in the last row has a policy of stepping west, which implies a route to the green state through the top row. This route might be long but then the neighboring states to the red state are avoided. \n",
    "\n",
    "\n",
    "Another example that presents the algorithm as reasonable is by looking at state $S_3$, where the policy is go to east, i.e a 80% chance of stepping in to the green state. In some cases one would step into $S_8$ from $S_3$, but overall the value of stepping into the green state $S_4$ safely is relatively high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c94489bd5a5a7608e4fa04705176796",
     "grade": false,
     "grade_id": "cell-d4840da19cbbb63a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Test your implementation using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5071817f08d1145df1c5095dab2e7dc",
     "grade": false,
     "grade_id": "cell-f89a5e7709d41efc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: state-value test, for gamma=.99\n",
      "Passed: policy test, for gamma=.99\n"
     ]
    }
   ],
   "source": [
    "test_value_iteration(v, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a2a2376284478e5ff87e8fc45681df1",
     "grade": false,
     "grade_id": "cell-32b52b966ea12de5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run value iteration for the same scenario as above, but now with $\\gamma=.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45da583d4680e326a85cde59f0373b3e",
     "grade": false,
     "grade_id": "cell-3f797c0f704c2394",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.56631445  0.65360208  0.74438015  0.84776628  1.        ]\n",
      " [ 0.49725171  0.          0.          0.57185903 -1.        ]\n",
      " [ 0.43084446  0.37830245  0.41624465  0.47405641  0.2761765 ]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | W | E | N | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "# Run for stochastic MDP, gamma = .9\n",
    "mdp = GridWorldMDP()\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7386b2eeb00e002a5fd8dc96f763ebd0",
     "grade": false,
     "grade_id": "cell-9192d61af754d47b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** Do you notice any difference between the greedy policy for the two different discount factors. If so, what's the difference, and why do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9826b72c498da4e15a55628fe23d6cdb",
     "grade": true,
     "grade_id": "cell-1a675e7574dce1d5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "The two last rows has changed when lowering the discount factor which might be due to that more weight is put on the immediate reward, than the future reward. There is now a nonzero probability of 0.1 that the agent move in to the red state from $S_8$, i.e with the lower discount factor there is a risk of entering the red state through state $S_8$, which wasn't the case when using the higher discount factor . \n",
    "\n",
    "\n",
    "One can see that in the case with the higher discount factor $\\gamma = 0.99$, the optimal policy favors the longer route since it is safer to reach the green state when following that way. On the other hand, when lowering the discount factor to $\\gamma = 0.9$, one can see that the agent is less careful when trying to reach its goal. This can be visualized when looking at for example state $S_{12}$, when $\\gamma = 0.99$ the agent does not take any risks and will try to go west and choose a longer route in order to reach the goal. However when $\\gamma = 0.9$, the agent is less careful and will try to reach the green state by going east, which is a much riskier path since it can end up in state $S_{8}$, where there is a nonzero probability of stepping into the red state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d3dd2c8db164f28e15c2ebe5bcb04d9",
     "grade": false,
     "grade_id": "cell-01feb7e04644407c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 2: Q-learning\n",
    "\n",
    "In the previous task, you solved for $V^*(s)$ and the greedy policy $\\pi^*(s)$, with the entire model of the MDP being available to you. This is however not very practical since for most problems we are trying to solve, the model is not known, and estimating the model for some applications is a very demanding process which often also requires a lot of simplifications. \n",
    "\n",
    "In this task, you will implement the Q-learning algorithm, an alternative that learns $Q$-values from experience, without the need of a model.\n",
    "\n",
    "#### Q-learning algorithm\n",
    "$\n",
    "\\text{Initialize}~Q(s,a), ~ \\forall~ s \\in {\\cal S},~ a~\\in {\\cal A} \\\\\n",
    "\\textbf{Repeat}~\\text{(for each episode):}\\\\\n",
    "\\quad \\text{Initialize}~s\\\\\n",
    "\\qquad \\textbf{Repeat}~\\text{(for each step in episode):}\\\\\n",
    "\\qquad\\quad \\text{Chose $a$ from $s$ using poliy derived from $Q$ (e.g., $\\epsilon$-greedy)}\\\\\n",
    "\\qquad\\quad \\text{Take action a, observe r, s'}\\\\\n",
    "\\qquad\\quad Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left(r + \\gamma~\\underset{a}{\\text{max}}~Q(s',a) - Q(s,a) \\right) \\\\\n",
    "\\qquad\\quad s \\leftarrow s' \\\\\n",
    "\\qquad \\text{Until s is terminal}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70126ebccf30d948724a24815e4f9ce3",
     "grade": false,
     "grade_id": "cell-bbe7f99d1cc4e6af",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 2.1  Implement an $\\epsilon$-greedy policy\n",
    "\n",
    "**(1 POE)**\n",
    "\n",
    "The goal of the Q-learning algorithm is to find the optimal policy $\\pi^*$, by estimating the state action value function under the optimal policy, i.e., $Q^*(s, a)$. From $Q^*(s,a)$, the agent can follow $\\pi^*$ by choosing the action that yields the largest expected value for each state, i.e., $\\text{argmax}_a~Q^*(s, a)$. However, when training a Q-learning model, the agent typically follows another policy to explore the environment (instead of the one that maximizes the current Q-values). In reinforcement learning this is known as off-policy learning. \n",
    "\n",
    "Your task is to implement a widely popular exploration policy, known as  the $\\epsilon$-greedy policy, in the cell below.\n",
    "\n",
    "An $\\epsilon$-greedy policy should:\n",
    "* with probability $\\epsilon$ take an uniformly-random action.\n",
    "* otherwise choose the best action according to the estimated state action values.\n",
    "\n",
    "*Hint:* The $\\epsilon$-greedy policy can be implemented extra elegantly by calculating the actual resulting sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ea10393af19c22a02e445c765e89a55",
     "grade": true,
     "grade_id": "cell-48c826a87791fb56",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of Q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "    m = len(q_values)\n",
    " \n",
    "    policy = (eps / m) * np.ones(m)\n",
    "    \n",
    "    greedy_action = np.argmax(q_values)\n",
    "    policy[greedy_action] += (1 - eps)\n",
    "     \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "583492c65cb090c181c66b42c2a51eff",
     "grade": false,
     "grade_id": "cell-6d33489b428b1179",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a884d531a9dbbfeebaac3af3d5d309e6",
     "grade": false,
     "grade_id": "cell-80bd577e278ec0b0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, good job!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "mdp = GridWorldMDP()\n",
    "\n",
    "# Test shape of output\n",
    "actions = mdp.get_actions()\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "actions = [i for i in range(10)]\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "\n",
    "# Test for greedy actions\n",
    "for a in actions:\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[a] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, 0)\n",
    "    assert np.allclose(foo, eps_greedy, rtol=1e-03), \"policy is not greedy\"\n",
    "\n",
    "# Test for uniform distribution, when eps=1\n",
    "eps_greedy = eps_greedy_policy(foo, 1)\n",
    "assert all(math.isclose(p, eps_greedy[0], rel_tol=1e-03) for p in eps_greedy),\\\n",
    "    \"policy does not return a uniform distribution for eps=1\"\n",
    "assert math.isclose(np.sum(eps_greedy), 1.0, rel_tol=1e-03), \"policy distribution is not normalized\"\n",
    "\n",
    "\n",
    "print('Test passed, good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd7f0895be1c8a4b8eeb7e4d6a993ad2",
     "grade": false,
     "grade_id": "cell-1dccaeebe5a41325",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 2.2: Implement the Q-learning algorithm\n",
    "\n",
    "Now it's time to actually implement the Q-learning algorithm. Unlike the Value iteration where there are no direct interactions with the environment, the Q-learning algorithm builds up its estimates by interacting and exploring the environment. \n",
    "\n",
    "To enable the agent to explore the environment a set of helper functions are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02bb92b833a3dc4e535cda13a4a1fae4",
     "grade": false,
     "grade_id": "cell-881edd2be439489e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reset in module gridworld_mdp:\n",
      "\n",
      "reset(self)\n",
      "    Resets the environment and the agent is positioned in the initial state in the bottom left corner.\n",
      "    :return: state, reward, terminal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "589834a8662125b7792560c134990d91",
     "grade": false,
     "grade_id": "cell-061e7670ebd7b35c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function step in module gridworld_mdp:\n",
      "\n",
      "step(self, action)\n",
      "    Takes one step in the environment using the selected action\n",
      "    :param action: action to execute, integer\n",
      "    :return: state, reward, terminal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b92ae99e0b252016ed7040a0f706cec",
     "grade": false,
     "grade_id": "cell-15fa6bbf763cdc6f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Implement your version of Q-learning in the cell below. \n",
    "\n",
    "**Hint:** It might be useful to study the pseudocode provided above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6734d53de1918a438b931e06cefa19c6",
     "grade": true,
     "grade_id": "cell-3912d729d9527acd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(eps, gamma, mdp):\n",
    "    Q = np.zeros([16, 4]) # state action value table\n",
    "    pi = np.zeros([16]) # greedy policy table\n",
    "    alpha = .01\n",
    "    \n",
    "    episodes=10000\n",
    "\n",
    "    for i in range(episodes):\n",
    "        # to initialize the environment, we must reset it.\n",
    "        current_state, R, done = mdp.reset()\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            #define epsilon greedy policy for current state\n",
    "            policy = eps_greedy_policy(Q[current_state], eps)\n",
    "            \n",
    "            #take action \n",
    "            a = np.random.choice(Q.shape[1], p=policy)\n",
    "            \n",
    "            #take a step using the chosen action\n",
    "            next_state, R, done = mdp.step(a)\n",
    "            \n",
    "            #update state\n",
    "            Q[current_state, a] += alpha * (R + gamma * np.max(Q[next_state]) - Q[current_state, a])\n",
    "            current_state = next_state\n",
    "            \n",
    "    pi=Q.argmax(axis=1)\n",
    "    return pi, Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7826f01b01a42ef8f3bf3df0d1bddf9a",
     "grade": false,
     "grade_id": "cell-b48032d234ecb11d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run Q-learning with for the stochastic MDP with $\\epsilon = 1, \\gamma=0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66990ab000df5703e5d940fb7e43e3f6",
     "grade": false,
     "grade_id": "cell-0464324eb2e2bf9c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | W | \n",
      "---------------------\n",
      "| N | N | N | W | S | \n",
      "---------------------\n",
      "| N | W | W | W | S | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "mdp = GridWorldMDP()\n",
    "pi, Q = q_learning(1, .99, mdp)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bc5026e570acf5162847f04d4b2daf3",
     "grade": false,
     "grade_id": "cell-a424df8abe557f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Test your implementation by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe0420c08764bdf6ddec81e380ccf729",
     "grade": false,
     "grade_id": "cell-e2832d3538099d67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: policy test, for gamma=.99\n"
     ]
    }
   ],
   "source": [
    "test_q_learning(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49f7e5d4884836a1c3c968b767d51f40",
     "grade": false,
     "grade_id": "cell-d3623c3f5c170bd4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run Q-learning with $\\epsilon=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02e5ac1f58ad85d49dc7288ec4761b09",
     "grade": false,
     "grade_id": "cell-1c095409c30320d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | N | N | N | E | \n",
      "---------------------\n",
      "| N | N | N | N | S | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "mdp = GridWorldMDP()\n",
    "pi, Q = q_learning(0, .99, mdp)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4ed5b3b1c4eba1dbd9a0e1d02b109fd",
     "grade": false,
     "grade_id": "cell-d3383d13bae73e68",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** You ran your implementation with $\\epsilon$ set to both 0 and 1. Did both values yield the same solution? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baeb0f64a59a28003b946728ae322959",
     "grade": true,
     "grade_id": "cell-54eb158e84c99275",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "Choosing the $\\varepsilon = 1$ in the $\\varepsilon$-greedy policy implies that we follow a fully explorative approach, where each action is equally likely to be chosen at each step. The agent is able to explore completely, which will eventually (after letting the algortihm run long enough) lead to the agent finding the $Q^*$, i.e it will eventually converge to the optimal policy. This can also be seen by comparing the resemblance from the value iteration above when it converges to an optimal policy.\n",
    "\n",
    "Choosing the $\\varepsilon = 0$ in the $\\varepsilon$-greedy policy implies a completely greedy policy, i.e. the action corresponding to the maxmimum action value is chosen in each step. This results in no exploration, which can lead to getting stuck in a bad local optima due to not exploring all of the options. This is clearly visualised in the state representation above, where all policies for all states (except for state 14) are the same (north). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36c58de753b998098c72d6a636c15eca",
     "grade": false,
     "grade_id": "cell-ae2a001335118014",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Task 3: Deep Double Q-learning (DDQN)\n",
    "For this task, you will implement a DDQN (double deep Q-learning network) to solve one of the problems of the OpenAI gym. Before we get into details about these type of networks, let's first review the simpler, DQN (deep Q-learning network) version.\n",
    "\n",
    "**Again, note that in the end we will implement DDQN, and not DQN.**\n",
    "\n",
    "## Deep Q Networks (DQN)\n",
    "As we saw in the video lectures, using a neural network as a state action value approximator is a great idea. However, if one tries to use this approach with Q-learning, it's very likely that the optimization will be very unstable. To remediate this, two main ideas are used.\n",
    "- First, we use experience replay, in order to decorrelate the experience samples we obtain when exploring the environment.\n",
    "- Second, we use two networks instead of one, in order to fix the optimization targets. Each of these ideas are explained below.\n",
    "\n",
    "#### Experience replay\n",
    "Since Q-learning is an off-policy algorithm, we may collect data by navigating in the environment using some choice of behavioral / exploratory policy $\\mu$ (e.g. $\\epsilon$-greedy), while still learning the $Q$ values for an optimal policy. Experience replay exploits this even further, and re-uses old data, which was collected using whatever exploratory policy we used at that moment (e.g. $\\epsilon$-greedy w.r.t. to those approximate $Q$ values).\n",
    "\n",
    "Except for re-using already collected data, another advantage of experience replay is that it allows us to decorrelate the data, by sampling experience from very different parts of the environment, rather than using the samples for training in the order they were collected when walking around in the environment.\n",
    "\n",
    "Decorrelating the data is essential for training neural networks.\n",
    "\n",
    "#### Fixing the optimization target\n",
    "That is, for a given minibatch sampled from the replay buffer, we'll optimize the weights of only one of the networks (commonly denoted as the \"online\" network), using the gradients w.r.t a loss function. This loss function is computed as the mean squared error between the current action values, computed according to the **online** network, and the Q targets, computed using the other, **offline network** (which we'll also refer to as the fixed network or target network).\n",
    "\n",
    "That is, the loss function is \n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left(Q(s_i,a_i; \\theta\\right) - Y_i)^2~,$$\n",
    "\n",
    "where $N$ is the number of samples in your minibatch, $Q(s,a;\\theta)$ is the state action value estimate, according to the online network (with parameters $\\theta$), and $Y_t$ is the Q target, computed as\n",
    "\n",
    "$$ Y_i = r_i +  \\gamma ~\\underset{a}{\\text{max}}~Q(s_i', a; \\theta^-)~, $$\n",
    "\n",
    "where $Q(s', a;\\theta')$ is the action value estimate, according to the offline network (with parameters $\\theta^-$).\n",
    "\n",
    "Finally, so that the offline parameters are also updated, we periodically copy the parameters from the online to the offline network.\n",
    "\n",
    "#### Training loop essentials\n",
    "The following key components are repeated for every time step $t$ in an episode:\n",
    "1. Sample an action $a_t$ from the $\\epsilon$-greedy policy w.r.t. the current estimated $Q$ values (online network), and execute the action in the environment.\n",
    "1. The current transition $(s_t, a_t, r_{t+1}, s_{t+1})$ is stored in the replay buffer.\n",
    "1. An entire mini-batch of transitions is sampled from the replay buffer, and a gradient step is taken to improve the online network.\n",
    "\n",
    "## Double Deep Q Networks (DDQN)\n",
    "\n",
    "The idea explained above works well in practice, but later it was discovered that this approach is very prone to overestimating the state action values. The main reason for this is that the max operator, used to select the greedy action when computing the Q target, uses the same values both to select and to evaluate an action (this tends to prefer overestimated actions). In order to prevent this, we can decouple the selection from the evaluation, which is the idea that created DDQN. More concretely, the Q target for a DDQN is now \n",
    "\n",
    "$$ Y_i = r_i + \\gamma Q(s_i', \\underset{a}{\\text{argmax}}Q(s_i',a;\\theta); \\theta^-)~. $$\n",
    "\n",
    "Hence, we're using the **online** network to select which action is best, but we use the **offline** network to evaluate the state action value for that chosen action in the next state. This is what makes DDQN not overestimate (as much) the state action values, which in turn helps us to train faster and obtain better policies.\n",
    "\n",
    "Note that while the online network is constantly updated (as opposed to the offline network), the Q target (in which both the online and offline network are included for DDQN) should always be regarded as a constant when taking the gradient steps.\n",
    "\n",
    "## Target notation\n",
    "\n",
    "Several different optimization targets which are estimated with some $q$ function are often jointly referred to as \"TD targets\".\n",
    "We strive to be consistent and separate on-policy \"TD(0) targets\" and off-policy \"Q targets\" but in other places this distinction may be less clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfc49ab7f1405c6ecbebfcc1b01d9456",
     "grade": false,
     "grade_id": "cell-b37fc5ebe369b45a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Environment\n",
    "\n",
    "The problem you will solve for this task is the inverted pendulum problem. \n",
    "On [Open AIs environment documentation](https://gym.openai.com/envs/CartPole-v0) , the following description is provided:\n",
    "\n",
    "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time step that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.*\n",
    "\n",
    "Furthermore, the episode will automatically end if 200 steps are reached, as explained [here](https://github.com/openai/gym/wiki/CartPole-v0#episode-termination).\n",
    "\n",
    "![title](./cartpole.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74de453978cf5a4ced5167cc4bed7cc9",
     "grade": false,
     "grade_id": "cell-920f9802c12678ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Run the cell below to see a video illustration of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "185ad818161c47503d9eea9d466d890f",
     "grade": false,
     "grade_id": "cell-bc01c34a37f53b30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/46wjA6dqxOM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x114fd3210>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('46wjA6dqxOM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ac323ef2aa16dc7e60404564799578e",
     "grade": false,
     "grade_id": "cell-181515edc2a1f7a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementation\n",
    "We'll solve this task using a DDQN. Most of the code is provided for you, in the file **ddqn_model.py**. This file contains the implementation of a neural network, which is described in the table below (feel free to experiment with different architectures).\n",
    "\n",
    "|Layer 1: units, activation | Layer 2: units, activation | Layer 3: units, activation | Cost function |\n",
    "|---------------------------|----------------------------|----------------------------|---------------|\n",
    "| 100, ReLu                 | 60, ReLu                   | number of actions, linear | MSE           |\n",
    "\n",
    "There are however a few key parts missing from the code, that are to be implemented in the following three functions:\n",
    "- `calc_q_and_take_action`\n",
    "- `calculate_q_targets`\n",
    "- `sample_batch_and_calculate_loss`\n",
    "\n",
    "These will then be called from the function `train_loop_ddqn`, which runs the main loop for training the model in the cart-pole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a3f41b77af244ce39e5b0d67fb70574",
     "grade": false,
     "grade_id": "cell-8ef4581670ad2682",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.1:  Calculate Q-values & take action\n",
    "Calculate Q-values for the current state, and decide on which action to take. Use an epsilon-greedy behavioral policy, and feel free to re-use the `epsilon_greedy_policy` function that you defined for the Q-learning part.\n",
    "\n",
    "This function will be used to control the agent's behavior in the environment, but the actual training will be done later, for entire mini-batches sampled from the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e16c9f245324d9aa2cab62f223fcab6c",
     "grade": true,
     "grade_id": "cell-5ef2ca4ae83dd031",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_q_and_take_action(ddqn, state, eps):\n",
    "    '''\n",
    "    Calculate Q-values for current state, and take an action according to an epsilon-greedy policy.\n",
    "    Inputs:\n",
    "        ddqn   - DDQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        state  - Current state. Numpy array, shape (1, num_states).\n",
    "        eps    - Exploration parameter.\n",
    "    Returns:\n",
    "        q_online_curr   - Q(s,a) for current state s. Numpy array, shape (1, num_actions) or  (num_actions,).\n",
    "        curr_action     - Selected action (0 or 1, i.e., left or right), sampled from epsilon-greedy policy. Integer.\n",
    "    '''\n",
    "    # FYI:\n",
    "    # ddqn.online_model & ddqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "\n",
    "    # YOUR CODE HERE \n",
    "    \n",
    "    state = torch.from_numpy(state).float()\n",
    "    q_online_curr = ddqn.online_model(state)\n",
    "    q_online_curr = q_online_curr.detach().numpy()\n",
    "    q_online_curr = q_online_curr.reshape(-1)\n",
    "    \n",
    "    policy = eps_greedy_policy(q_online_curr, eps)\n",
    "    possible_actions = np.arange(len(policy))\n",
    "    curr_action = np.random.choice(possible_actions, p=policy)\n",
    "   \n",
    "    return q_online_curr, curr_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cce1e1a968c529d5c6f20832281d92f",
     "grade": false,
     "grade_id": "cell-ae647de789982b2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 3.2:  Calculate Q target\n",
    "\n",
    "**(2 POE)**\n",
    "\n",
    "For this task, you will calculate the temporal difference target used for the loss in the double Q-learning algorithm. Your implementation should follow precisely the equation defined above for the Q target of DDQNs, with one exception: when s' is terminal, the Q target for it should simply be $ Y_i = r_i$. Why is this necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d0922a476a654975d99168051d86bbd",
     "grade": true,
     "grade_id": "cell-d28bf13d3d581368",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "For the Q-target of DDQNs, we're using the online network to select which action is best, but we use the offline network to evaluate the state action value for that chosen action in the next state. This means that the TD target for the DDQN is computed by taking the best state action value in the next state. If $s'$ is the temrinal state, there will however not be any states after that one, which is why $Y_i = r_i$ needs to be stated in order for the algortihm to know what to do in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6a429aaede47e041414f7fac4093390",
     "grade": false,
     "grade_id": "cell-26a12456d7c70776",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Implement your function in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c687cccc16fa05e24be65eb204d4ed01",
     "grade": true,
     "grade_id": "cell-e73bca0bd9d5574a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_q_targets(q1_batch, q2_batch, r_batch, nonterminal_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the Q target used for the loss\n",
    "    : param q1_batch: Batch of Q(s', a) from online network. FloatTensor, shape (N, num actions)\n",
    "    : param q2_batch: Batch of Q(s', a) from target network. FloatTensor, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards. FloatTensor, shape (N,)\n",
    "    : param nonterminal_batch: Batch of booleans, with False elements if state s' is terminal and True otherwise. BoolTensor, shape (N,)\n",
    "    : param gamma: Discount factor, float.\n",
    "    : return: Q target. FloatTensor, shape (N,)\n",
    "    '''\n",
    "    N = q1_batch.shape[0]\n",
    "    best_action = torch.argmax(q1_batch, axis=1)\n",
    "    nonterminal_batch = nonterminal_batch.float() # convert BoolTensor to FloatTensor\n",
    "    \n",
    "    Y = r_batch + (gamma*q2_batch[np.arange(N), best_action] * nonterminal_batch)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "198cb1c800b2c103c9fcdfeb9fd45eda",
     "grade": false,
     "grade_id": "cell-b4a77bdb942ce919",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your implementation by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d83145c0697d0ac4b58857c62c33798a",
     "grade": false,
     "grade_id": "cell-bd5e9c36540130c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: Calculate Q targets test, for function \"calculate_q_targets\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dqn_model\n",
    "dqn_model.test_calculate_q_targets(calculate_q_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "417be01d6f312fba6f7cb28d49682db8",
     "grade": false,
     "grade_id": "cell-90601693d1f77faf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.3: Calculate mini-batch Q-values\n",
    "\n",
    "Use the online & offline Q-networks to calculate the Q-values for a minibatch. These will then be used to calculate the mini-batch loss by the end of the function.\n",
    "\n",
    "You will need to define three tensors:\n",
    "- `q_online_curr`: $Q(s,a; \\theta), \\ \\forall a$\n",
    "- `q_online_next`: $Q(s',a; \\theta), \\ \\forall a$\n",
    "- `q_offline_next`: $Q(s',a; \\theta^-), \\ \\forall a$\n",
    "\n",
    "Take great care to make sure gradient computation is enabled / disabled where it should. `torch.no_grad()` is your friend here (see [Pytorch docs](https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e46f9a558104abf7dd19ff91d9d028",
     "grade": true,
     "grade_id": "cell-f716681cdc0fca13",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma):\n",
    "    '''\n",
    "    Sample mini-batch from replay buffer, and compute the mini-batch loss\n",
    "    Inputs:\n",
    "        ddqn          - DDQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        replay_buffer - Replay buffer object (from which samples will be drawn)\n",
    "        batch_size    - Batch size\n",
    "        gamma         - Discount factor\n",
    "    Returns:\n",
    "        Mini-batch loss, on which .backward() will be called to compute gradient.\n",
    "    '''\n",
    "    # Sample a minibatch of transitions from replay buffer\n",
    "    curr_state, curr_action, reward, next_state, nonterminal = replay_buffer.sample_minibatch(batch_size)\n",
    "\n",
    "    # FYI:\n",
    "    # ddqn.online_model & ddqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    q_online_curr = ddqn.online_model(curr_state)\n",
    "    q_online_next = ddqn.online_model(next_state)\n",
    "    q_offline_next = ddqn.offline_model(next_state)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_target = calculate_q_targets(q_online_next, q_offline_next, reward, nonterminal, gamma=gamma)\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        loss = ddqn.calc_loss(q_online_curr, q_target, curr_action)\n",
    "   \n",
    "                \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d2adc8d2e6496636b23a31f36e394ac",
     "grade": false,
     "grade_id": "cell-401656fa71e3f227",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Test your implementation by trying to solve the reinforcement learning problem for the Cartpole environment. The `train_loop_ddqn` function defined below will be called later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb5047b8217c7e97d2c27e4e51a3f71b",
     "grade": false,
     "grade_id": "cell-f3d381be408141e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from dqn_model import DoubleQLearningModel, ExperienceReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU should be enough, but feel free to play around with this if you want to.\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff585600fcd9c1884e390e14530665b7",
     "grade": false,
     "grade_id": "cell-a9e3d9e027c537be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not finish_episode:\n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "\n",
    "            # Take one step in environment. No need to compute gradients,\n",
    "            # we will just store transition to replay buffer, and later sample a whole batch\n",
    "            # from the replay buffer to actually take a gradient step.\n",
    "            q_online_curr, curr_action = calc_q_and_take_action(ddqn, state, eps)\n",
    "            q_buffer.append(q_online_curr)\n",
    "            new_state, reward, finish_episode, _ = env.step(curr_action) # take one step in the evironment\n",
    "            new_state = new_state[None,:]\n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            nonterminal_to_buffer = not finish_episode or steps == 200\n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                loss = sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma)\n",
    "                ddqn.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ddqn.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    ddqn.update_target_network()\n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "\n",
    "        print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer))))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dbdd4e0e629188932bae9d37d05253c",
     "grade": false,
     "grade_id": "cell-75c84628ce999711",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following cell performs the actual training. \n",
    "\n",
    "A working implementation should start to improve after 500 episodes. An episodic reward of around 200 is likely to be achieved after 800 episodes for a batchsize of 128, and 1000 episodes for a batchsize of 64.\n",
    "\n",
    "**Note:** The `enable_visualization` flag controls whether a visualization of the cart-pole environment will be plotted. In many environments, this is however not working properly, for which reason it is disabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward (running avg):   20 (20.00) Epsilon: 0.999, Avg Q: 2.059e-07\n",
      "Episode: 1, Total Reward (running avg):   20 (20.00) Epsilon: 0.998, Avg Q: -1.427e-07\n",
      "Episode: 2, Total Reward (running avg):   19 (19.95) Epsilon: 0.997, Avg Q: 2.52e-07\n",
      "Episode: 3, Total Reward (running avg):   24 (20.15) Epsilon: 0.996, Avg Q: -1.203e-07\n",
      "Episode: 4, Total Reward (running avg):   19 (20.09) Epsilon: 0.995, Avg Q: -1.638e-07\n",
      "Episode: 5, Total Reward (running avg):   15 (19.84) Epsilon: 0.994, Avg Q: -2.344e-07\n",
      "Episode: 6, Total Reward (running avg):   15 (19.60) Epsilon: 0.993, Avg Q: -1.986e-07\n",
      "Episode: 7, Total Reward (running avg):   15 (19.37) Epsilon: 0.992, Avg Q: -2.248e-07\n",
      "Episode: 8, Total Reward (running avg):    9 (18.85) Epsilon: 0.991, Avg Q: -2.965e-07\n",
      "Episode: 9, Total Reward (running avg):   26 (19.21) Epsilon: 0.990, Avg Q: 1.564e-07\n",
      "Episode: 10, Total Reward (running avg):   23 (19.40) Epsilon: 0.989, Avg Q: 2.183e-07\n",
      "Episode: 11, Total Reward (running avg):   16 (19.23) Epsilon: 0.988, Avg Q: 3.414e-07\n",
      "Episode: 12, Total Reward (running avg):   18 (19.17) Epsilon: 0.987, Avg Q: -1.887e-07\n",
      "Episode: 13, Total Reward (running avg):   32 (19.81) Epsilon: 0.986, Avg Q: 1.714e-07\n",
      "Episode: 14, Total Reward (running avg):   17 (19.67) Epsilon: 0.985, Avg Q: -1.98e-07\n",
      "Episode: 15, Total Reward (running avg):   27 (20.03) Epsilon: 0.984, Avg Q: 2.337e-07\n",
      "Episode: 16, Total Reward (running avg):   40 (21.03) Epsilon: 0.983, Avg Q: 9.473e-08\n",
      "Episode: 17, Total Reward (running avg):   30 (21.48) Epsilon: 0.982, Avg Q: -1.247e-07\n",
      "Episode: 18, Total Reward (running avg):   15 (21.16) Epsilon: 0.981, Avg Q: -1.601e-07\n",
      "Episode: 19, Total Reward (running avg):   26 (21.40) Epsilon: 0.980, Avg Q: 1.213e-07\n",
      "Episode: 20, Total Reward (running avg):   12 (20.93) Epsilon: 0.979, Avg Q: 4.66e-07\n",
      "Episode: 21, Total Reward (running avg):   11 (20.43) Epsilon: 0.978, Avg Q: 3.625e-07\n",
      "Episode: 22, Total Reward (running avg):   38 (21.31) Epsilon: 0.977, Avg Q: -2.612e-08\n",
      "Episode: 23, Total Reward (running avg):   20 (21.25) Epsilon: 0.976, Avg Q: -1.718e-07\n",
      "Episode: 24, Total Reward (running avg):   10 (20.68) Epsilon: 0.975, Avg Q: 5.023e-07\n",
      "Episode: 25, Total Reward (running avg):   22 (20.75) Epsilon: 0.974, Avg Q: 2.569e-07\n",
      "Episode: 26, Total Reward (running avg):   25 (20.96) Epsilon: 0.973, Avg Q: 2.238e-07\n",
      "Episode: 27, Total Reward (running avg):   25 (21.16) Epsilon: 0.972, Avg Q: 2.2e-07\n",
      "Episode: 28, Total Reward (running avg):   54 (22.81) Epsilon: 0.971, Avg Q: 3.515e-08\n",
      "Episode: 29, Total Reward (running avg):   11 (22.21) Epsilon: 0.970, Avg Q: -2.563e-07\n",
      "Episode: 30, Total Reward (running avg):   15 (21.85) Epsilon: 0.969, Avg Q: -2.32e-07\n",
      "Episode: 31, Total Reward (running avg):   11 (21.31) Epsilon: 0.968, Avg Q: 4.729e-07\n",
      "Episode: 32, Total Reward (running avg):   14 (20.95) Epsilon: 0.967, Avg Q: -2.162e-07\n",
      "Episode: 33, Total Reward (running avg):   14 (20.60) Epsilon: 0.966, Avg Q: 3.071e-07\n",
      "Episode: 34, Total Reward (running avg):   35 (21.32) Epsilon: 0.965, Avg Q: -5.739e-08\n",
      "Episode: 35, Total Reward (running avg):   30 (21.75) Epsilon: 0.964, Avg Q: -1.032e-07\n",
      "Episode: 36, Total Reward (running avg):   11 (21.22) Epsilon: 0.963, Avg Q: -2.9e-07\n",
      "Episode: 37, Total Reward (running avg):   16 (20.95) Epsilon: 0.962, Avg Q: 3.649e-07\n",
      "Episode: 38, Total Reward (running avg):   33 (21.56) Epsilon: 0.961, Avg Q: -8.466e-08\n",
      "Episode: 39, Total Reward (running avg):   16 (21.28) Epsilon: 0.960, Avg Q: 2.886e-07\n",
      "Episode: 40, Total Reward (running avg):   29 (21.66) Epsilon: 0.959, Avg Q: -6.362e-08\n",
      "Episode: 41, Total Reward (running avg):   31 (22.13) Epsilon: 0.958, Avg Q: 1.975e-07\n",
      "Episode: 42, Total Reward (running avg):   48 (23.42) Epsilon: 0.957, Avg Q: 7.003e-08\n",
      "Episode: 43, Total Reward (running avg):   15 (23.00) Epsilon: 0.956, Avg Q: 2.951e-07\n",
      "Episode: 44, Total Reward (running avg):   29 (23.30) Epsilon: 0.955, Avg Q: 1.665e-07\n",
      "Episode: 45, Total Reward (running avg):   14 (22.84) Epsilon: 0.954, Avg Q: 0.03587\n",
      "Episode: 46, Total Reward (running avg):   13 (22.35) Epsilon: 0.953, Avg Q: 0.1555\n",
      "Episode: 47, Total Reward (running avg):   10 (21.73) Epsilon: 0.952, Avg Q: 0.2613\n",
      "Episode: 48, Total Reward (running avg):   16 (21.44) Epsilon: 0.951, Avg Q: 0.2648\n",
      "Episode: 49, Total Reward (running avg):   17 (21.22) Epsilon: 0.950, Avg Q: 0.3302\n",
      "Episode: 50, Total Reward (running avg):   12 (20.76) Epsilon: 0.949, Avg Q: 0.4749\n",
      "Episode: 51, Total Reward (running avg):   16 (20.52) Epsilon: 0.948, Avg Q: 0.5329\n",
      "Episode: 52, Total Reward (running avg):   19 (20.45) Epsilon: 0.947, Avg Q: 0.7396\n",
      "Episode: 53, Total Reward (running avg):   20 (20.42) Epsilon: 0.946, Avg Q: 0.7148\n",
      "Episode: 54, Total Reward (running avg):   14 (20.10) Epsilon: 0.945, Avg Q: 0.693\n",
      "Episode: 55, Total Reward (running avg):   15 (19.85) Epsilon: 0.944, Avg Q: 0.6736\n",
      "Episode: 56, Total Reward (running avg):   27 (20.20) Epsilon: 0.943, Avg Q: 0.6916\n",
      "Episode: 57, Total Reward (running avg):   18 (20.09) Epsilon: 0.942, Avg Q: 0.6873\n",
      "Episode: 58, Total Reward (running avg):   19 (20.04) Epsilon: 0.941, Avg Q: 0.806\n",
      "Episode: 59, Total Reward (running avg):   11 (19.59) Epsilon: 0.940, Avg Q: 1.055\n",
      "Episode: 60, Total Reward (running avg):   15 (19.36) Epsilon: 0.939, Avg Q: 0.894\n",
      "Episode: 61, Total Reward (running avg):   12 (18.99) Epsilon: 0.938, Avg Q: 1.201\n",
      "Episode: 62, Total Reward (running avg):   14 (18.74) Epsilon: 0.937, Avg Q: 0.895\n",
      "Episode: 63, Total Reward (running avg):   20 (18.80) Epsilon: 0.936, Avg Q: 0.7473\n",
      "Episode: 64, Total Reward (running avg):   42 (19.96) Epsilon: 0.935, Avg Q: 0.7056\n",
      "Episode: 65, Total Reward (running avg):   35 (20.72) Epsilon: 0.934, Avg Q: 0.8361\n",
      "Episode: 66, Total Reward (running avg):   10 (20.18) Epsilon: 0.933, Avg Q: 1.165\n",
      "Episode: 67, Total Reward (running avg):    9 (19.62) Epsilon: 0.932, Avg Q: 1.234\n",
      "Episode: 68, Total Reward (running avg):   47 (20.99) Epsilon: 0.931, Avg Q: 0.7679\n",
      "Episode: 69, Total Reward (running avg):   52 (22.54) Epsilon: 0.930, Avg Q: 0.7781\n",
      "Episode: 70, Total Reward (running avg):   15 (22.16) Epsilon: 0.929, Avg Q: 0.977\n",
      "Episode: 71, Total Reward (running avg):   31 (22.60) Epsilon: 0.928, Avg Q: 0.9242\n",
      "Episode: 72, Total Reward (running avg):   13 (22.12) Epsilon: 0.927, Avg Q: 1.076\n",
      "Episode: 73, Total Reward (running avg):   15 (21.77) Epsilon: 0.926, Avg Q: 0.9924\n",
      "Episode: 74, Total Reward (running avg):   14 (21.38) Epsilon: 0.925, Avg Q: 0.9932\n",
      "Episode: 75, Total Reward (running avg):   28 (21.71) Epsilon: 0.924, Avg Q: 0.9226\n",
      "Episode: 76, Total Reward (running avg):   24 (21.83) Epsilon: 0.923, Avg Q: 0.9516\n",
      "Episode: 77, Total Reward (running avg):   25 (21.98) Epsilon: 0.922, Avg Q: 0.9474\n",
      "Episode: 78, Total Reward (running avg):   22 (21.99) Epsilon: 0.921, Avg Q: 0.9366\n",
      "Episode: 79, Total Reward (running avg):   15 (21.64) Epsilon: 0.920, Avg Q: 1.019\n",
      "Episode: 80, Total Reward (running avg):   46 (22.85) Epsilon: 0.919, Avg Q: 0.9497\n",
      "Episode: 81, Total Reward (running avg):   12 (22.31) Epsilon: 0.918, Avg Q: 0.9903\n",
      "Episode: 82, Total Reward (running avg):   46 (23.50) Epsilon: 0.917, Avg Q: 1.23\n",
      "Episode: 83, Total Reward (running avg):   23 (23.47) Epsilon: 0.916, Avg Q: 0.9581\n",
      "Episode: 84, Total Reward (running avg):   59 (25.25) Epsilon: 0.915, Avg Q: 0.965\n",
      "Episode: 85, Total Reward (running avg):   11 (24.54) Epsilon: 0.914, Avg Q: 0.9607\n",
      "Episode: 86, Total Reward (running avg):   10 (23.81) Epsilon: 0.913, Avg Q: 0.9881\n",
      "Episode: 87, Total Reward (running avg):   15 (23.37) Epsilon: 0.912, Avg Q: 0.9886\n",
      "Episode: 88, Total Reward (running avg):   38 (24.10) Epsilon: 0.911, Avg Q: 0.9957\n",
      "Episode: 89, Total Reward (running avg):   21 (23.94) Epsilon: 0.910, Avg Q: 0.9644\n",
      "Episode: 90, Total Reward (running avg):   21 (23.80) Epsilon: 0.909, Avg Q: 0.9924\n",
      "Episode: 91, Total Reward (running avg):   20 (23.61) Epsilon: 0.908, Avg Q: 1.337\n",
      "Episode: 92, Total Reward (running avg):   46 (24.73) Epsilon: 0.907, Avg Q: 1.681\n",
      "Episode: 93, Total Reward (running avg):   37 (25.34) Epsilon: 0.906, Avg Q: 1.779\n",
      "Episode: 94, Total Reward (running avg):   14 (24.77) Epsilon: 0.905, Avg Q: 1.869\n",
      "Episode: 95, Total Reward (running avg):   31 (25.08) Epsilon: 0.904, Avg Q: 1.858\n",
      "Episode: 96, Total Reward (running avg):   17 (24.68) Epsilon: 0.903, Avg Q: 1.855\n",
      "Episode: 97, Total Reward (running avg):   78 (27.35) Epsilon: 0.902, Avg Q: 1.873\n",
      "Episode: 98, Total Reward (running avg):   14 (26.68) Epsilon: 0.901, Avg Q: 1.843\n",
      "Episode: 99, Total Reward (running avg):   13 (26.00) Epsilon: 0.900, Avg Q: 1.852\n",
      "Episode: 100, Total Reward (running avg):   24 (25.90) Epsilon: 0.899, Avg Q: 1.842\n",
      "Episode: 101, Total Reward (running avg):   20 (25.60) Epsilon: 0.898, Avg Q: 1.911\n",
      "Episode: 102, Total Reward (running avg):   35 (26.07) Epsilon: 0.897, Avg Q: 1.914\n",
      "Episode: 103, Total Reward (running avg):   18 (25.67) Epsilon: 0.896, Avg Q: 1.869\n",
      "Episode: 104, Total Reward (running avg):   14 (25.08) Epsilon: 0.895, Avg Q: 1.829\n",
      "Episode: 105, Total Reward (running avg):   14 (24.53) Epsilon: 0.894, Avg Q: 1.857\n",
      "Episode: 106, Total Reward (running avg):   25 (24.55) Epsilon: 0.893, Avg Q: 1.87\n",
      "Episode: 107, Total Reward (running avg):   17 (24.18) Epsilon: 0.892, Avg Q: 1.868\n",
      "Episode: 108, Total Reward (running avg):   16 (23.77) Epsilon: 0.891, Avg Q: 1.836\n",
      "Episode: 109, Total Reward (running avg):   12 (23.18) Epsilon: 0.890, Avg Q: 1.82\n",
      "Episode: 110, Total Reward (running avg):   17 (22.87) Epsilon: 0.889, Avg Q: 1.854\n",
      "Episode: 111, Total Reward (running avg):   32 (23.33) Epsilon: 0.888, Avg Q: 1.891\n",
      "Episode: 112, Total Reward (running avg):   13 (22.81) Epsilon: 0.887, Avg Q: 1.815\n",
      "Episode: 113, Total Reward (running avg):   20 (22.67) Epsilon: 0.886, Avg Q: 1.847\n",
      "Episode: 114, Total Reward (running avg):   19 (22.49) Epsilon: 0.885, Avg Q: 1.901\n",
      "Episode: 115, Total Reward (running avg):    9 (21.81) Epsilon: 0.884, Avg Q: 1.819\n",
      "Episode: 116, Total Reward (running avg):   24 (21.92) Epsilon: 0.883, Avg Q: 1.902\n",
      "Episode: 117, Total Reward (running avg):   18 (21.72) Epsilon: 0.882, Avg Q: 1.861\n",
      "Episode: 118, Total Reward (running avg):   14 (21.34) Epsilon: 0.881, Avg Q: 1.869\n",
      "Episode: 119, Total Reward (running avg):   22 (21.37) Epsilon: 0.880, Avg Q: 1.899\n",
      "Episode: 120, Total Reward (running avg):   20 (21.30) Epsilon: 0.879, Avg Q: 1.882\n",
      "Episode: 121, Total Reward (running avg):   31 (21.79) Epsilon: 0.878, Avg Q: 1.929\n",
      "Episode: 122, Total Reward (running avg):   36 (22.50) Epsilon: 0.877, Avg Q: 1.907\n",
      "Episode: 123, Total Reward (running avg):   10 (21.87) Epsilon: 0.876, Avg Q: 1.811\n",
      "Episode: 124, Total Reward (running avg):   18 (21.68) Epsilon: 0.875, Avg Q: 1.926\n",
      "Episode: 125, Total Reward (running avg):   17 (21.45) Epsilon: 0.874, Avg Q: 1.86\n",
      "Episode: 126, Total Reward (running avg):   18 (21.27) Epsilon: 0.873, Avg Q: 1.891\n",
      "Episode: 127, Total Reward (running avg):   23 (21.36) Epsilon: 0.872, Avg Q: 1.876\n",
      "Episode: 128, Total Reward (running avg):   19 (21.24) Epsilon: 0.871, Avg Q: 1.895\n",
      "Episode: 129, Total Reward (running avg):   17 (21.03) Epsilon: 0.870, Avg Q: 1.891\n",
      "Episode: 130, Total Reward (running avg):   12 (20.58) Epsilon: 0.869, Avg Q: 1.83\n",
      "Episode: 131, Total Reward (running avg):   53 (22.20) Epsilon: 0.868, Avg Q: 2.229\n",
      "Episode: 132, Total Reward (running avg):   19 (22.04) Epsilon: 0.867, Avg Q: 1.861\n",
      "Episode: 133, Total Reward (running avg):   26 (22.24) Epsilon: 0.866, Avg Q: 1.892\n",
      "Episode: 134, Total Reward (running avg):   19 (22.08) Epsilon: 0.865, Avg Q: 1.897\n",
      "Episode: 135, Total Reward (running avg):   15 (21.72) Epsilon: 0.864, Avg Q: 2.05\n",
      "Episode: 136, Total Reward (running avg):   40 (22.64) Epsilon: 0.863, Avg Q: 2.534\n",
      "Episode: 137, Total Reward (running avg):   21 (22.55) Epsilon: 0.862, Avg Q: 2.645\n",
      "Episode: 138, Total Reward (running avg):   11 (21.98) Epsilon: 0.861, Avg Q: 2.695\n",
      "Episode: 139, Total Reward (running avg):   13 (21.53) Epsilon: 0.860, Avg Q: 2.624\n",
      "Episode: 140, Total Reward (running avg):   12 (21.05) Epsilon: 0.859, Avg Q: 2.631\n",
      "Episode: 141, Total Reward (running avg):   19 (20.95) Epsilon: 0.858, Avg Q: 2.648\n",
      "Episode: 142, Total Reward (running avg):   14 (20.60) Epsilon: 0.857, Avg Q: 2.628\n",
      "Episode: 143, Total Reward (running avg):   18 (20.47) Epsilon: 0.856, Avg Q: 2.663\n",
      "Episode: 144, Total Reward (running avg):   19 (20.40) Epsilon: 0.855, Avg Q: 2.676\n",
      "Episode: 145, Total Reward (running avg):   24 (20.58) Epsilon: 0.854, Avg Q: 2.674\n",
      "Episode: 146, Total Reward (running avg):   16 (20.35) Epsilon: 0.853, Avg Q: 2.64\n",
      "Episode: 147, Total Reward (running avg):   12 (19.93) Epsilon: 0.852, Avg Q: 2.527\n",
      "Episode: 148, Total Reward (running avg):   30 (20.43) Epsilon: 0.851, Avg Q: 2.684\n",
      "Episode: 149, Total Reward (running avg):   38 (21.31) Epsilon: 0.850, Avg Q: 2.754\n",
      "Episode: 150, Total Reward (running avg):   28 (21.65) Epsilon: 0.849, Avg Q: 2.722\n",
      "Episode: 151, Total Reward (running avg):   10 (21.06) Epsilon: 0.848, Avg Q: 2.551\n",
      "Episode: 152, Total Reward (running avg):   14 (20.71) Epsilon: 0.847, Avg Q: 2.699\n",
      "Episode: 153, Total Reward (running avg):   10 (20.18) Epsilon: 0.846, Avg Q: 2.566\n",
      "Episode: 154, Total Reward (running avg):   15 (19.92) Epsilon: 0.845, Avg Q: 2.614\n",
      "Episode: 155, Total Reward (running avg):   11 (19.47) Epsilon: 0.844, Avg Q: 2.503\n",
      "Episode: 156, Total Reward (running avg):   15 (19.25) Epsilon: 0.843, Avg Q: 2.578\n",
      "Episode: 157, Total Reward (running avg):   11 (18.84) Epsilon: 0.842, Avg Q: 2.516\n",
      "Episode: 158, Total Reward (running avg):   41 (19.94) Epsilon: 0.841, Avg Q: 2.783\n",
      "Episode: 159, Total Reward (running avg):   10 (19.45) Epsilon: 0.840, Avg Q: 2.485\n",
      "Episode: 160, Total Reward (running avg):   11 (19.02) Epsilon: 0.839, Avg Q: 2.524\n",
      "Episode: 161, Total Reward (running avg):   12 (18.67) Epsilon: 0.838, Avg Q: 2.502\n",
      "Episode: 162, Total Reward (running avg):   10 (18.24) Epsilon: 0.837, Avg Q: 2.539\n",
      "Episode: 163, Total Reward (running avg):   45 (19.58) Epsilon: 0.836, Avg Q: 3.023\n",
      "Episode: 164, Total Reward (running avg):   10 (19.10) Epsilon: 0.835, Avg Q: 2.489\n",
      "Episode: 165, Total Reward (running avg):   25 (19.39) Epsilon: 0.834, Avg Q: 2.711\n",
      "Episode: 166, Total Reward (running avg):   11 (18.97) Epsilon: 0.833, Avg Q: 2.633\n",
      "Episode: 167, Total Reward (running avg):   19 (18.98) Epsilon: 0.832, Avg Q: 2.651\n",
      "Episode: 168, Total Reward (running avg):   39 (19.98) Epsilon: 0.831, Avg Q: 2.762\n",
      "Episode: 169, Total Reward (running avg):   15 (19.73) Epsilon: 0.830, Avg Q: 2.617\n",
      "Episode: 170, Total Reward (running avg):   19 (19.69) Epsilon: 0.829, Avg Q: 2.68\n",
      "Episode: 171, Total Reward (running avg):   13 (19.36) Epsilon: 0.828, Avg Q: 2.557\n",
      "Episode: 172, Total Reward (running avg):   21 (19.44) Epsilon: 0.827, Avg Q: 2.733\n",
      "Episode: 173, Total Reward (running avg):   15 (19.22) Epsilon: 0.826, Avg Q: 2.605\n",
      "Episode: 174, Total Reward (running avg):   19 (19.21) Epsilon: 0.825, Avg Q: 2.695\n",
      "Episode: 175, Total Reward (running avg):   21 (19.30) Epsilon: 0.824, Avg Q: 2.693\n",
      "Episode: 176, Total Reward (running avg):   16 (19.13) Epsilon: 0.823, Avg Q: 2.657\n",
      "Episode: 177, Total Reward (running avg):   19 (19.12) Epsilon: 0.822, Avg Q: 2.697\n",
      "Episode: 178, Total Reward (running avg):   40 (20.17) Epsilon: 0.821, Avg Q: 2.733\n",
      "Episode: 179, Total Reward (running avg):   13 (19.81) Epsilon: 0.820, Avg Q: 2.59\n",
      "Episode: 180, Total Reward (running avg):   37 (20.67) Epsilon: 0.819, Avg Q: 2.772\n",
      "Episode: 181, Total Reward (running avg):   24 (20.84) Epsilon: 0.818, Avg Q: 2.719\n",
      "Episode: 182, Total Reward (running avg):   30 (21.29) Epsilon: 0.817, Avg Q: 2.696\n",
      "Episode: 183, Total Reward (running avg):   45 (22.48) Epsilon: 0.816, Avg Q: 3.021\n",
      "Episode: 184, Total Reward (running avg):   33 (23.01) Epsilon: 0.815, Avg Q: 2.999\n",
      "Episode: 185, Total Reward (running avg):   69 (25.31) Epsilon: 0.814, Avg Q: 3.58\n",
      "Episode: 186, Total Reward (running avg):   41 (26.09) Epsilon: 0.813, Avg Q: 3.452\n",
      "Episode: 187, Total Reward (running avg):   14 (25.49) Epsilon: 0.812, Avg Q: 3.209\n",
      "Episode: 188, Total Reward (running avg):  175 (32.96) Epsilon: 0.811, Avg Q: 5.19\n",
      "Episode: 189, Total Reward (running avg):   83 (35.46) Epsilon: 0.810, Avg Q: 3.557\n",
      "Episode: 190, Total Reward (running avg):   29 (35.14) Epsilon: 0.809, Avg Q: 3.465\n",
      "Episode: 191, Total Reward (running avg):   30 (34.88) Epsilon: 0.808, Avg Q: 3.595\n",
      "Episode: 192, Total Reward (running avg):   28 (34.54) Epsilon: 0.807, Avg Q: 3.471\n",
      "Episode: 193, Total Reward (running avg):   30 (34.31) Epsilon: 0.806, Avg Q: 3.6\n",
      "Episode: 194, Total Reward (running avg):   12 (33.20) Epsilon: 0.805, Avg Q: 3.107\n",
      "Episode: 195, Total Reward (running avg):   10 (32.04) Epsilon: 0.804, Avg Q: 3.181\n",
      "Episode: 196, Total Reward (running avg):   61 (33.48) Epsilon: 0.803, Avg Q: 3.69\n",
      "Episode: 197, Total Reward (running avg):   38 (33.71) Epsilon: 0.802, Avg Q: 3.588\n",
      "Episode: 198, Total Reward (running avg):   12 (32.62) Epsilon: 0.801, Avg Q: 3.261\n",
      "Episode: 199, Total Reward (running avg):   36 (32.79) Epsilon: 0.800, Avg Q: 3.424\n",
      "Episode: 200, Total Reward (running avg):   57 (34.00) Epsilon: 0.799, Avg Q: 3.6\n",
      "Episode: 201, Total Reward (running avg):   30 (33.80) Epsilon: 0.798, Avg Q: 3.499\n",
      "Episode: 202, Total Reward (running avg):   46 (34.41) Epsilon: 0.797, Avg Q: 3.478\n",
      "Episode: 203, Total Reward (running avg):   12 (33.29) Epsilon: 0.796, Avg Q: 3.287\n",
      "Episode: 204, Total Reward (running avg):   43 (33.78) Epsilon: 0.795, Avg Q: 3.533\n",
      "Episode: 205, Total Reward (running avg):   24 (33.29) Epsilon: 0.794, Avg Q: 3.425\n",
      "Episode: 206, Total Reward (running avg):   26 (32.92) Epsilon: 0.793, Avg Q: 3.413\n",
      "Episode: 207, Total Reward (running avg):   38 (33.18) Epsilon: 0.792, Avg Q: 3.736\n",
      "Episode: 208, Total Reward (running avg):   26 (32.82) Epsilon: 0.791, Avg Q: 3.399\n",
      "Episode: 209, Total Reward (running avg):   93 (35.83) Epsilon: 0.790, Avg Q: 4.123\n",
      "Episode: 210, Total Reward (running avg):   45 (36.29) Epsilon: 0.789, Avg Q: 4.236\n",
      "Episode: 211, Total Reward (running avg):   21 (35.52) Epsilon: 0.788, Avg Q: 4.056\n",
      "Episode: 212, Total Reward (running avg):   26 (35.05) Epsilon: 0.787, Avg Q: 4.139\n",
      "Episode: 213, Total Reward (running avg):   58 (36.19) Epsilon: 0.786, Avg Q: 4.344\n",
      "Episode: 214, Total Reward (running avg):   55 (37.13) Epsilon: 0.785, Avg Q: 4.594\n",
      "Episode: 215, Total Reward (running avg):   55 (38.03) Epsilon: 0.784, Avg Q: 4.45\n",
      "Episode: 216, Total Reward (running avg):   15 (36.88) Epsilon: 0.783, Avg Q: 3.775\n",
      "Episode: 217, Total Reward (running avg):   14 (35.73) Epsilon: 0.782, Avg Q: 3.879\n",
      "Episode: 218, Total Reward (running avg):   27 (35.30) Epsilon: 0.781, Avg Q: 4.183\n",
      "Episode: 219, Total Reward (running avg):   38 (35.43) Epsilon: 0.780, Avg Q: 4.143\n",
      "Episode: 220, Total Reward (running avg):   67 (37.01) Epsilon: 0.779, Avg Q: 4.307\n",
      "Episode: 221, Total Reward (running avg):   33 (36.81) Epsilon: 0.778, Avg Q: 4.199\n",
      "Episode: 222, Total Reward (running avg):   13 (35.62) Epsilon: 0.777, Avg Q: 3.868\n",
      "Episode: 223, Total Reward (running avg):   71 (37.39) Epsilon: 0.776, Avg Q: 5.224\n",
      "Episode: 224, Total Reward (running avg):   29 (36.97) Epsilon: 0.775, Avg Q: 4.193\n",
      "Episode: 225, Total Reward (running avg):   25 (36.37) Epsilon: 0.774, Avg Q: 4.232\n",
      "Episode: 226, Total Reward (running avg):   45 (36.80) Epsilon: 0.773, Avg Q: 4.313\n",
      "Episode: 227, Total Reward (running avg):   59 (37.91) Epsilon: 0.772, Avg Q: 4.389\n",
      "Episode: 228, Total Reward (running avg):   37 (37.87) Epsilon: 0.771, Avg Q: 4.363\n",
      "Episode: 229, Total Reward (running avg):   75 (39.72) Epsilon: 0.770, Avg Q: 4.519\n",
      "Episode: 230, Total Reward (running avg):   25 (38.99) Epsilon: 0.769, Avg Q: 4.107\n",
      "Episode: 231, Total Reward (running avg):   14 (37.74) Epsilon: 0.768, Avg Q: 3.885\n",
      "Episode: 232, Total Reward (running avg):   12 (36.45) Epsilon: 0.767, Avg Q: 3.823\n",
      "Episode: 233, Total Reward (running avg):   17 (35.48) Epsilon: 0.766, Avg Q: 3.928\n",
      "Episode: 234, Total Reward (running avg):   14 (34.40) Epsilon: 0.765, Avg Q: 3.817\n",
      "Episode: 235, Total Reward (running avg):   39 (34.63) Epsilon: 0.764, Avg Q: 4.53\n",
      "Episode: 236, Total Reward (running avg):   10 (33.40) Epsilon: 0.763, Avg Q: 4.402\n",
      "Episode: 237, Total Reward (running avg):   25 (32.98) Epsilon: 0.762, Avg Q: 4.787\n",
      "Episode: 238, Total Reward (running avg):   31 (32.88) Epsilon: 0.761, Avg Q: 4.779\n",
      "Episode: 239, Total Reward (running avg):   12 (31.84) Epsilon: 0.760, Avg Q: 4.542\n",
      "Episode: 240, Total Reward (running avg):   57 (33.10) Epsilon: 0.759, Avg Q: 5.321\n",
      "Episode: 241, Total Reward (running avg):   21 (32.49) Epsilon: 0.758, Avg Q: 4.665\n",
      "Episode: 242, Total Reward (running avg):   86 (35.17) Epsilon: 0.757, Avg Q: 5.374\n",
      "Episode: 243, Total Reward (running avg):   37 (35.26) Epsilon: 0.756, Avg Q: 4.79\n",
      "Episode: 244, Total Reward (running avg):   32 (35.10) Epsilon: 0.755, Avg Q: 4.914\n",
      "Episode: 245, Total Reward (running avg):   27 (34.69) Epsilon: 0.754, Avg Q: 4.912\n",
      "Episode: 246, Total Reward (running avg):   36 (34.76) Epsilon: 0.753, Avg Q: 4.948\n",
      "Episode: 247, Total Reward (running avg):   76 (36.82) Epsilon: 0.752, Avg Q: 5.145\n",
      "Episode: 248, Total Reward (running avg):   60 (37.98) Epsilon: 0.751, Avg Q: 5.132\n",
      "Episode: 249, Total Reward (running avg):   21 (37.13) Epsilon: 0.750, Avg Q: 4.727\n",
      "Episode: 250, Total Reward (running avg):   88 (39.67) Epsilon: 0.749, Avg Q: 5.053\n",
      "Episode: 251, Total Reward (running avg):  104 (42.89) Epsilon: 0.748, Avg Q: 5.677\n",
      "Episode: 252, Total Reward (running avg):   50 (43.24) Epsilon: 0.747, Avg Q: 5.043\n",
      "Episode: 253, Total Reward (running avg):   38 (42.98) Epsilon: 0.746, Avg Q: 4.783\n",
      "Episode: 254, Total Reward (running avg):   33 (42.48) Epsilon: 0.745, Avg Q: 4.756\n",
      "Episode: 255, Total Reward (running avg):   18 (41.26) Epsilon: 0.744, Avg Q: 4.619\n",
      "Episode: 256, Total Reward (running avg):   15 (39.95) Epsilon: 0.743, Avg Q: 4.483\n",
      "Episode: 257, Total Reward (running avg):   48 (40.35) Epsilon: 0.742, Avg Q: 5.197\n",
      "Episode: 258, Total Reward (running avg):   99 (43.28) Epsilon: 0.741, Avg Q: 5.933\n",
      "Episode: 259, Total Reward (running avg):   52 (43.72) Epsilon: 0.740, Avg Q: 5.624\n",
      "Episode: 260, Total Reward (running avg):   48 (43.93) Epsilon: 0.739, Avg Q: 5.704\n",
      "Episode: 261, Total Reward (running avg):   27 (43.08) Epsilon: 0.738, Avg Q: 5.509\n",
      "Episode: 262, Total Reward (running avg):   12 (41.53) Epsilon: 0.737, Avg Q: 4.902\n",
      "Episode: 263, Total Reward (running avg):   78 (43.35) Epsilon: 0.736, Avg Q: 5.839\n",
      "Episode: 264, Total Reward (running avg):   24 (42.39) Epsilon: 0.735, Avg Q: 5.352\n",
      "Episode: 265, Total Reward (running avg):   55 (43.02) Epsilon: 0.734, Avg Q: 5.715\n",
      "Episode: 266, Total Reward (running avg):   23 (42.02) Epsilon: 0.733, Avg Q: 5.392\n",
      "Episode: 267, Total Reward (running avg):   15 (40.67) Epsilon: 0.732, Avg Q: 5.215\n",
      "Episode: 268, Total Reward (running avg):   95 (43.38) Epsilon: 0.731, Avg Q: 5.932\n",
      "Episode: 269, Total Reward (running avg):   86 (45.51) Epsilon: 0.730, Avg Q: 5.776\n",
      "Episode: 270, Total Reward (running avg):   53 (45.89) Epsilon: 0.729, Avg Q: 5.736\n",
      "Episode: 271, Total Reward (running avg):   78 (47.49) Epsilon: 0.728, Avg Q: 5.746\n",
      "Episode: 272, Total Reward (running avg):   19 (46.07) Epsilon: 0.727, Avg Q: 5.241\n",
      "Episode: 273, Total Reward (running avg):   31 (45.31) Epsilon: 0.726, Avg Q: 5.523\n",
      "Episode: 274, Total Reward (running avg):   40 (45.05) Epsilon: 0.725, Avg Q: 5.655\n",
      "Episode: 275, Total Reward (running avg):   19 (43.75) Epsilon: 0.724, Avg Q: 5.261\n",
      "Episode: 276, Total Reward (running avg):   40 (43.56) Epsilon: 0.723, Avg Q: 5.67\n",
      "Episode: 277, Total Reward (running avg):   46 (43.68) Epsilon: 0.722, Avg Q: 5.796\n",
      "Episode: 278, Total Reward (running avg):   22 (42.60) Epsilon: 0.721, Avg Q: 5.399\n",
      "Episode: 279, Total Reward (running avg):   56 (43.27) Epsilon: 0.720, Avg Q: 5.889\n",
      "Episode: 280, Total Reward (running avg):  144 (48.30) Epsilon: 0.719, Avg Q: 6.253\n",
      "Episode: 281, Total Reward (running avg):   16 (46.69) Epsilon: 0.718, Avg Q: 5.679\n",
      "Episode: 282, Total Reward (running avg):   14 (45.05) Epsilon: 0.717, Avg Q: 5.398\n",
      "Episode: 283, Total Reward (running avg):   36 (44.60) Epsilon: 0.716, Avg Q: 6.19\n",
      "Episode: 284, Total Reward (running avg):   27 (43.72) Epsilon: 0.715, Avg Q: 6.071\n",
      "Episode: 285, Total Reward (running avg):   13 (42.19) Epsilon: 0.714, Avg Q: 5.631\n",
      "Episode: 286, Total Reward (running avg):   97 (44.93) Epsilon: 0.713, Avg Q: 6.442\n",
      "Episode: 287, Total Reward (running avg):   21 (43.73) Epsilon: 0.712, Avg Q: 5.922\n",
      "Episode: 288, Total Reward (running avg):  109 (46.99) Epsilon: 0.711, Avg Q: 6.291\n",
      "Episode: 289, Total Reward (running avg):  100 (49.64) Epsilon: 0.710, Avg Q: 6.792\n",
      "Episode: 290, Total Reward (running avg):   33 (48.81) Epsilon: 0.709, Avg Q: 6.046\n",
      "Episode: 291, Total Reward (running avg):   14 (47.07) Epsilon: 0.708, Avg Q: 5.679\n",
      "Episode: 292, Total Reward (running avg):   82 (48.82) Epsilon: 0.707, Avg Q: 6.186\n",
      "Episode: 293, Total Reward (running avg):   20 (47.38) Epsilon: 0.706, Avg Q: 5.651\n",
      "Episode: 294, Total Reward (running avg):   18 (45.91) Epsilon: 0.705, Avg Q: 5.789\n",
      "Episode: 295, Total Reward (running avg):  106 (48.91) Epsilon: 0.704, Avg Q: 6.438\n",
      "Episode: 296, Total Reward (running avg):   23 (47.62) Epsilon: 0.703, Avg Q: 5.975\n",
      "Episode: 297, Total Reward (running avg):   30 (46.74) Epsilon: 0.702, Avg Q: 6.081\n",
      "Episode: 298, Total Reward (running avg):   88 (48.80) Epsilon: 0.701, Avg Q: 6.479\n",
      "Episode: 299, Total Reward (running avg):   17 (47.21) Epsilon: 0.700, Avg Q: 5.557\n",
      "Episode: 300, Total Reward (running avg):   79 (48.80) Epsilon: 0.699, Avg Q: 6.731\n",
      "Episode: 301, Total Reward (running avg):   27 (47.71) Epsilon: 0.698, Avg Q: 6.568\n",
      "Episode: 302, Total Reward (running avg):   22 (46.42) Epsilon: 0.697, Avg Q: 6.552\n",
      "Episode: 303, Total Reward (running avg):   88 (48.50) Epsilon: 0.696, Avg Q: 7.008\n",
      "Episode: 304, Total Reward (running avg):   58 (48.98) Epsilon: 0.695, Avg Q: 6.953\n",
      "Episode: 305, Total Reward (running avg):   55 (49.28) Epsilon: 0.694, Avg Q: 7.016\n",
      "Episode: 306, Total Reward (running avg):   63 (49.96) Epsilon: 0.693, Avg Q: 6.868\n",
      "Episode: 307, Total Reward (running avg):   42 (49.57) Epsilon: 0.692, Avg Q: 6.836\n",
      "Episode: 308, Total Reward (running avg):   21 (48.14) Epsilon: 0.691, Avg Q: 6.407\n",
      "Episode: 309, Total Reward (running avg):  114 (51.43) Epsilon: 0.690, Avg Q: 6.923\n",
      "Episode: 310, Total Reward (running avg):   40 (50.86) Epsilon: 0.689, Avg Q: 6.755\n",
      "Episode: 311, Total Reward (running avg):   16 (49.12) Epsilon: 0.688, Avg Q: 6.153\n",
      "Episode: 312, Total Reward (running avg):  129 (53.11) Epsilon: 0.687, Avg Q: 7.053\n",
      "Episode: 313, Total Reward (running avg):   23 (51.61) Epsilon: 0.686, Avg Q: 6.391\n",
      "Episode: 314, Total Reward (running avg):   67 (52.37) Epsilon: 0.685, Avg Q: 7.006\n",
      "Episode: 315, Total Reward (running avg):   40 (51.76) Epsilon: 0.684, Avg Q: 6.784\n",
      "Episode: 316, Total Reward (running avg):   16 (49.97) Epsilon: 0.683, Avg Q: 6.125\n",
      "Episode: 317, Total Reward (running avg):   57 (50.32) Epsilon: 0.682, Avg Q: 6.864\n",
      "Episode: 318, Total Reward (running avg):   81 (51.85) Epsilon: 0.681, Avg Q: 6.975\n",
      "Episode: 319, Total Reward (running avg):   99 (54.21) Epsilon: 0.680, Avg Q: 7.792\n",
      "Episode: 320, Total Reward (running avg):   56 (54.30) Epsilon: 0.679, Avg Q: 7.498\n",
      "Episode: 321, Total Reward (running avg):   82 (55.69) Epsilon: 0.678, Avg Q: 7.488\n",
      "Episode: 322, Total Reward (running avg):   67 (56.25) Epsilon: 0.677, Avg Q: 7.487\n",
      "Episode: 323, Total Reward (running avg):   89 (57.89) Epsilon: 0.676, Avg Q: 7.609\n",
      "Episode: 324, Total Reward (running avg):   14 (55.69) Epsilon: 0.675, Avg Q: 6.333\n",
      "Episode: 325, Total Reward (running avg):   58 (55.81) Epsilon: 0.674, Avg Q: 7.551\n",
      "Episode: 326, Total Reward (running avg):  124 (59.22) Epsilon: 0.673, Avg Q: 7.778\n",
      "Episode: 327, Total Reward (running avg):   29 (57.71) Epsilon: 0.672, Avg Q: 7.199\n",
      "Episode: 328, Total Reward (running avg):   49 (57.27) Epsilon: 0.671, Avg Q: 7.447\n",
      "Episode: 329, Total Reward (running avg):   59 (57.36) Epsilon: 0.670, Avg Q: 7.427\n",
      "Episode: 330, Total Reward (running avg):   32 (56.09) Epsilon: 0.669, Avg Q: 7.258\n",
      "Episode: 331, Total Reward (running avg):   53 (55.94) Epsilon: 0.668, Avg Q: 7.518\n",
      "Episode: 332, Total Reward (running avg):   83 (57.29) Epsilon: 0.667, Avg Q: 7.612\n",
      "Episode: 333, Total Reward (running avg):   31 (55.98) Epsilon: 0.666, Avg Q: 7.208\n",
      "Episode: 334, Total Reward (running avg):   33 (54.83) Epsilon: 0.665, Avg Q: 7.419\n",
      "Episode: 335, Total Reward (running avg):   42 (54.19) Epsilon: 0.664, Avg Q: 7.33\n",
      "Episode: 336, Total Reward (running avg):   36 (53.28) Epsilon: 0.663, Avg Q: 7.753\n",
      "Episode: 337, Total Reward (running avg):   66 (53.91) Epsilon: 0.662, Avg Q: 7.997\n",
      "Episode: 338, Total Reward (running avg):   30 (52.72) Epsilon: 0.661, Avg Q: 7.715\n",
      "Episode: 339, Total Reward (running avg):   64 (53.28) Epsilon: 0.660, Avg Q: 8.003\n",
      "Episode: 340, Total Reward (running avg):   70 (54.12) Epsilon: 0.659, Avg Q: 7.933\n",
      "Episode: 341, Total Reward (running avg):   82 (55.51) Epsilon: 0.658, Avg Q: 7.83\n",
      "Episode: 342, Total Reward (running avg):  138 (59.64) Epsilon: 0.657, Avg Q: 8.229\n",
      "Episode: 343, Total Reward (running avg):   38 (58.55) Epsilon: 0.656, Avg Q: 7.802\n",
      "Episode: 344, Total Reward (running avg):  107 (60.98) Epsilon: 0.655, Avg Q: 8.168\n",
      "Episode: 345, Total Reward (running avg):   46 (60.23) Epsilon: 0.654, Avg Q: 7.936\n",
      "Episode: 346, Total Reward (running avg):   60 (60.22) Epsilon: 0.653, Avg Q: 8.066\n",
      "Episode: 347, Total Reward (running avg):   16 (58.00) Epsilon: 0.652, Avg Q: 6.91\n",
      "Episode: 348, Total Reward (running avg):   48 (57.50) Epsilon: 0.651, Avg Q: 7.782\n",
      "Episode: 349, Total Reward (running avg):   80 (58.63) Epsilon: 0.650, Avg Q: 8.132\n",
      "Episode: 350, Total Reward (running avg):   68 (59.10) Epsilon: 0.649, Avg Q: 8.031\n",
      "Episode: 351, Total Reward (running avg):   32 (57.74) Epsilon: 0.648, Avg Q: 7.788\n",
      "Episode: 352, Total Reward (running avg):   39 (56.81) Epsilon: 0.647, Avg Q: 8.107\n",
      "Episode: 353, Total Reward (running avg):   46 (56.27) Epsilon: 0.646, Avg Q: 8.283\n",
      "Episode: 354, Total Reward (running avg):  102 (58.55) Epsilon: 0.645, Avg Q: 8.726\n",
      "Episode: 355, Total Reward (running avg):   36 (57.42) Epsilon: 0.644, Avg Q: 8.269\n",
      "Episode: 356, Total Reward (running avg):  103 (59.70) Epsilon: 0.643, Avg Q: 8.801\n",
      "Episode: 357, Total Reward (running avg):   60 (59.72) Epsilon: 0.642, Avg Q: 8.613\n",
      "Episode: 358, Total Reward (running avg):  118 (62.63) Epsilon: 0.641, Avg Q: 8.71\n",
      "Episode: 359, Total Reward (running avg):   65 (62.75) Epsilon: 0.640, Avg Q: 8.54\n",
      "Episode: 360, Total Reward (running avg):   22 (60.71) Epsilon: 0.639, Avg Q: 8.028\n",
      "Episode: 361, Total Reward (running avg):   58 (60.58) Epsilon: 0.638, Avg Q: 8.504\n",
      "Episode: 362, Total Reward (running avg):  102 (62.65) Epsilon: 0.637, Avg Q: 8.732\n",
      "Episode: 363, Total Reward (running avg):   91 (64.07) Epsilon: 0.636, Avg Q: 8.667\n",
      "Episode: 364, Total Reward (running avg):   24 (62.06) Epsilon: 0.635, Avg Q: 7.998\n",
      "Episode: 365, Total Reward (running avg):   30 (60.46) Epsilon: 0.634, Avg Q: 8.119\n",
      "Episode: 366, Total Reward (running avg):   14 (58.14) Epsilon: 0.633, Avg Q: 7.167\n",
      "Episode: 367, Total Reward (running avg):  100 (60.23) Epsilon: 0.632, Avg Q: 8.729\n",
      "Episode: 368, Total Reward (running avg):   33 (58.87) Epsilon: 0.631, Avg Q: 8.522\n",
      "Episode: 369, Total Reward (running avg):   87 (60.28) Epsilon: 0.630, Avg Q: 9.313\n",
      "Episode: 370, Total Reward (running avg):   26 (58.56) Epsilon: 0.629, Avg Q: 8.51\n",
      "Episode: 371, Total Reward (running avg):  115 (61.38) Epsilon: 0.628, Avg Q: 9.113\n",
      "Episode: 372, Total Reward (running avg):  168 (66.71) Epsilon: 0.627, Avg Q: 9.312\n",
      "Episode: 373, Total Reward (running avg):  121 (69.43) Epsilon: 0.626, Avg Q: 8.815\n",
      "Episode: 374, Total Reward (running avg):  104 (71.16) Epsilon: 0.625, Avg Q: 9.199\n",
      "Episode: 375, Total Reward (running avg):   26 (68.90) Epsilon: 0.624, Avg Q: 8.454\n",
      "Episode: 376, Total Reward (running avg):   22 (66.55) Epsilon: 0.623, Avg Q: 8.39\n",
      "Episode: 377, Total Reward (running avg):   58 (66.13) Epsilon: 0.622, Avg Q: 9.08\n",
      "Episode: 378, Total Reward (running avg):   66 (66.12) Epsilon: 0.621, Avg Q: 9.169\n",
      "Episode: 379, Total Reward (running avg):   98 (67.71) Epsilon: 0.620, Avg Q: 9.354\n",
      "Episode: 380, Total Reward (running avg):   30 (65.83) Epsilon: 0.619, Avg Q: 8.751\n",
      "Episode: 381, Total Reward (running avg):   30 (64.04) Epsilon: 0.618, Avg Q: 8.674\n",
      "Episode: 382, Total Reward (running avg):   48 (63.24) Epsilon: 0.617, Avg Q: 9.218\n",
      "Episode: 383, Total Reward (running avg):   71 (63.62) Epsilon: 0.616, Avg Q: 9.723\n",
      "Episode: 384, Total Reward (running avg):   70 (63.94) Epsilon: 0.615, Avg Q: 9.727\n",
      "Episode: 385, Total Reward (running avg):   27 (62.10) Epsilon: 0.614, Avg Q: 8.941\n",
      "Episode: 386, Total Reward (running avg):  113 (64.64) Epsilon: 0.613, Avg Q: 9.79\n",
      "Episode: 387, Total Reward (running avg):   76 (65.21) Epsilon: 0.612, Avg Q: 9.867\n",
      "Episode: 388, Total Reward (running avg):   40 (63.95) Epsilon: 0.611, Avg Q: 9.277\n",
      "Episode: 389, Total Reward (running avg):   48 (63.15) Epsilon: 0.610, Avg Q: 9.487\n",
      "Episode: 390, Total Reward (running avg):  149 (67.44) Epsilon: 0.609, Avg Q: 9.588\n",
      "Episode: 391, Total Reward (running avg):   44 (66.27) Epsilon: 0.608, Avg Q: 9.426\n",
      "Episode: 392, Total Reward (running avg):   41 (65.01) Epsilon: 0.607, Avg Q: 9.413\n",
      "Episode: 393, Total Reward (running avg):   69 (65.21) Epsilon: 0.606, Avg Q: 9.634\n",
      "Episode: 394, Total Reward (running avg):   17 (62.80) Epsilon: 0.605, Avg Q: 8.441\n",
      "Episode: 395, Total Reward (running avg):   62 (62.76) Epsilon: 0.604, Avg Q: 9.636\n",
      "Episode: 396, Total Reward (running avg):   88 (64.02) Epsilon: 0.603, Avg Q: 9.815\n",
      "Episode: 397, Total Reward (running avg):   82 (64.92) Epsilon: 0.602, Avg Q: 9.649\n",
      "Episode: 398, Total Reward (running avg):   32 (63.27) Epsilon: 0.601, Avg Q: 9.53\n",
      "Episode: 399, Total Reward (running avg):  104 (65.31) Epsilon: 0.600, Avg Q: 10.35\n",
      "Episode: 400, Total Reward (running avg):   23 (63.19) Epsilon: 0.599, Avg Q: 9.178\n",
      "Episode: 401, Total Reward (running avg):   53 (62.68) Epsilon: 0.598, Avg Q: 10.02\n",
      "Episode: 402, Total Reward (running avg):   56 (62.35) Epsilon: 0.597, Avg Q: 10.09\n",
      "Episode: 403, Total Reward (running avg):   96 (64.03) Epsilon: 0.596, Avg Q: 10.36\n",
      "Episode: 404, Total Reward (running avg):  160 (68.83) Epsilon: 0.595, Avg Q: 10.39\n",
      "Episode: 405, Total Reward (running avg):   42 (67.49) Epsilon: 0.594, Avg Q: 9.937\n",
      "Episode: 406, Total Reward (running avg):   78 (68.01) Epsilon: 0.593, Avg Q: 10.16\n",
      "Episode: 407, Total Reward (running avg):  200 (74.61) Epsilon: 0.592, Avg Q: 10.46\n",
      "Episode: 408, Total Reward (running avg):   39 (72.83) Epsilon: 0.591, Avg Q: 9.829\n",
      "Episode: 409, Total Reward (running avg):  200 (79.19) Epsilon: 0.590, Avg Q: 10.78\n",
      "Episode: 410, Total Reward (running avg):   54 (77.93) Epsilon: 0.589, Avg Q: 10.63\n",
      "Episode: 411, Total Reward (running avg):   25 (75.29) Epsilon: 0.588, Avg Q: 9.85\n",
      "Episode: 412, Total Reward (running avg):  155 (79.27) Epsilon: 0.587, Avg Q: 10.58\n",
      "Episode: 413, Total Reward (running avg):   40 (77.31) Epsilon: 0.586, Avg Q: 10.3\n",
      "Episode: 414, Total Reward (running avg):  139 (80.39) Epsilon: 0.585, Avg Q: 10.57\n",
      "Episode: 415, Total Reward (running avg):   62 (79.47) Epsilon: 0.584, Avg Q: 10.67\n",
      "Episode: 416, Total Reward (running avg):   66 (78.80) Epsilon: 0.583, Avg Q: 10.61\n",
      "Episode: 417, Total Reward (running avg):   29 (76.31) Epsilon: 0.582, Avg Q: 10.07\n",
      "Episode: 418, Total Reward (running avg):   15 (73.24) Epsilon: 0.581, Avg Q: 8.822\n",
      "Episode: 419, Total Reward (running avg):  143 (76.73) Epsilon: 0.580, Avg Q: 10.83\n",
      "Episode: 420, Total Reward (running avg):   70 (76.39) Epsilon: 0.579, Avg Q: 10.63\n",
      "Episode: 421, Total Reward (running avg):   78 (76.47) Epsilon: 0.578, Avg Q: 10.68\n",
      "Episode: 422, Total Reward (running avg):   63 (75.80) Epsilon: 0.577, Avg Q: 10.96\n",
      "Episode: 423, Total Reward (running avg):   30 (73.51) Epsilon: 0.576, Avg Q: 10.74\n",
      "Episode: 424, Total Reward (running avg):  134 (76.54) Epsilon: 0.575, Avg Q: 11.24\n",
      "Episode: 425, Total Reward (running avg):   30 (74.21) Epsilon: 0.574, Avg Q: 10.62\n",
      "Episode: 426, Total Reward (running avg):   50 (73.00) Epsilon: 0.573, Avg Q: 10.75\n",
      "Episode: 427, Total Reward (running avg):  135 (76.10) Epsilon: 0.572, Avg Q: 11.19\n",
      "Episode: 428, Total Reward (running avg):   98 (77.19) Epsilon: 0.571, Avg Q: 11.3\n",
      "Episode: 429, Total Reward (running avg):   72 (76.93) Epsilon: 0.570, Avg Q: 11.11\n",
      "Episode: 430, Total Reward (running avg):  106 (78.39) Epsilon: 0.569, Avg Q: 10.91\n",
      "Episode: 431, Total Reward (running avg):  147 (81.82) Epsilon: 0.568, Avg Q: 11.3\n",
      "Episode: 432, Total Reward (running avg):  200 (87.73) Epsilon: 0.567, Avg Q: 11.57\n",
      "Episode: 433, Total Reward (running avg):   58 (86.24) Epsilon: 0.566, Avg Q: 11.46\n",
      "Episode: 434, Total Reward (running avg):   37 (83.78) Epsilon: 0.565, Avg Q: 11.14\n",
      "Episode: 435, Total Reward (running avg):   15 (80.34) Epsilon: 0.564, Avg Q: 9.86\n",
      "Episode: 436, Total Reward (running avg):  138 (83.22) Epsilon: 0.563, Avg Q: 11.65\n",
      "Episode: 437, Total Reward (running avg):   45 (81.31) Epsilon: 0.562, Avg Q: 11.44\n",
      "Episode: 438, Total Reward (running avg):   92 (81.85) Epsilon: 0.561, Avg Q: 11.5\n",
      "Episode: 439, Total Reward (running avg):  111 (83.30) Epsilon: 0.560, Avg Q: 11.77\n",
      "Episode: 440, Total Reward (running avg):   66 (82.44) Epsilon: 0.559, Avg Q: 11.27\n",
      "Episode: 441, Total Reward (running avg):   24 (79.52) Epsilon: 0.558, Avg Q: 10.34\n",
      "Episode: 442, Total Reward (running avg):  107 (80.89) Epsilon: 0.557, Avg Q: 11.68\n",
      "Episode: 443, Total Reward (running avg):  105 (82.10) Epsilon: 0.556, Avg Q: 11.61\n",
      "Episode: 444, Total Reward (running avg):  189 (87.44) Epsilon: 0.555, Avg Q: 11.75\n",
      "Episode: 445, Total Reward (running avg):  129 (89.52) Epsilon: 0.554, Avg Q: 12.25\n",
      "Episode: 446, Total Reward (running avg):  104 (90.24) Epsilon: 0.553, Avg Q: 12.06\n",
      "Episode: 447, Total Reward (running avg):  123 (91.88) Epsilon: 0.552, Avg Q: 12.1\n",
      "Episode: 448, Total Reward (running avg):  162 (95.39) Epsilon: 0.551, Avg Q: 12.21\n",
      "Episode: 449, Total Reward (running avg):  165 (98.87) Epsilon: 0.550, Avg Q: 11.94\n",
      "Episode: 450, Total Reward (running avg):   99 (98.87) Epsilon: 0.549, Avg Q: 11.65\n",
      "Episode: 451, Total Reward (running avg):  184 (103.13) Epsilon: 0.548, Avg Q: 11.92\n",
      "Episode: 452, Total Reward (running avg):   27 (99.32) Epsilon: 0.547, Avg Q: 11.28\n",
      "Episode: 453, Total Reward (running avg):  109 (99.81) Epsilon: 0.546, Avg Q: 12.32\n",
      "Episode: 454, Total Reward (running avg):   55 (97.57) Epsilon: 0.545, Avg Q: 12.24\n",
      "Episode: 455, Total Reward (running avg):  165 (100.94) Epsilon: 0.544, Avg Q: 12.46\n",
      "Episode: 456, Total Reward (running avg):  174 (104.59) Epsilon: 0.543, Avg Q: 12.87\n",
      "Episode: 457, Total Reward (running avg):  200 (109.36) Epsilon: 0.542, Avg Q: 12.58\n",
      "Episode: 458, Total Reward (running avg):  176 (112.69) Epsilon: 0.541, Avg Q: 12.09\n",
      "Episode: 459, Total Reward (running avg):   34 (108.76) Epsilon: 0.540, Avg Q: 11.78\n",
      "Episode: 460, Total Reward (running avg):   98 (108.22) Epsilon: 0.539, Avg Q: 12.57\n",
      "Episode: 461, Total Reward (running avg):   84 (107.01) Epsilon: 0.538, Avg Q: 12.87\n",
      "Episode: 462, Total Reward (running avg):  200 (111.66) Epsilon: 0.537, Avg Q: 13.01\n",
      "Episode: 463, Total Reward (running avg):  172 (114.68) Epsilon: 0.536, Avg Q: 13.08\n",
      "Episode: 464, Total Reward (running avg):   72 (112.54) Epsilon: 0.535, Avg Q: 12.79\n",
      "Episode: 465, Total Reward (running avg):  200 (116.92) Epsilon: 0.534, Avg Q: 13.16\n",
      "Episode: 466, Total Reward (running avg):   22 (112.17) Epsilon: 0.533, Avg Q: 11.42\n",
      "Episode: 467, Total Reward (running avg):  118 (112.46) Epsilon: 0.532, Avg Q: 12.7\n",
      "Episode: 468, Total Reward (running avg):  133 (113.49) Epsilon: 0.531, Avg Q: 12.83\n",
      "Episode: 469, Total Reward (running avg):   65 (111.06) Epsilon: 0.530, Avg Q: 13.08\n",
      "Episode: 470, Total Reward (running avg):  162 (113.61) Epsilon: 0.529, Avg Q: 12.64\n",
      "Episode: 471, Total Reward (running avg):  178 (116.83) Epsilon: 0.528, Avg Q: 13.13\n",
      "Episode: 472, Total Reward (running avg):   22 (112.09) Epsilon: 0.527, Avg Q: 12.01\n",
      "Episode: 473, Total Reward (running avg):   94 (111.18) Epsilon: 0.526, Avg Q: 13.13\n",
      "Episode: 474, Total Reward (running avg):   59 (108.58) Epsilon: 0.525, Avg Q: 12.83\n",
      "Episode: 475, Total Reward (running avg):  154 (110.85) Epsilon: 0.524, Avg Q: 13.45\n",
      "Episode: 476, Total Reward (running avg):   57 (108.15) Epsilon: 0.523, Avg Q: 12.97\n",
      "Episode: 477, Total Reward (running avg):  144 (109.95) Epsilon: 0.522, Avg Q: 13.23\n",
      "Episode: 478, Total Reward (running avg):  200 (114.45) Epsilon: 0.521, Avg Q: 13.76\n",
      "Episode: 479, Total Reward (running avg):   65 (111.98) Epsilon: 0.520, Avg Q: 13.22\n",
      "Episode: 480, Total Reward (running avg):  153 (114.03) Epsilon: 0.519, Avg Q: 13.49\n",
      "Episode: 481, Total Reward (running avg):   87 (112.68) Epsilon: 0.518, Avg Q: 13.33\n",
      "Episode: 482, Total Reward (running avg):   99 (111.99) Epsilon: 0.517, Avg Q: 13.4\n",
      "Episode: 483, Total Reward (running avg):  101 (111.44) Epsilon: 0.516, Avg Q: 13.5\n",
      "Episode: 484, Total Reward (running avg):  200 (115.87) Epsilon: 0.515, Avg Q: 13.85\n",
      "Episode: 485, Total Reward (running avg):  200 (120.08) Epsilon: 0.514, Avg Q: 13.84\n",
      "Episode: 486, Total Reward (running avg):   47 (116.42) Epsilon: 0.513, Avg Q: 13.49\n",
      "Episode: 487, Total Reward (running avg):   44 (112.80) Epsilon: 0.512, Avg Q: 13.41\n",
      "Episode: 488, Total Reward (running avg):  200 (117.16) Epsilon: 0.511, Avg Q: 14.1\n",
      "Episode: 489, Total Reward (running avg):   74 (115.00) Epsilon: 0.510, Avg Q: 13.85\n",
      "Episode: 490, Total Reward (running avg):  157 (117.10) Epsilon: 0.509, Avg Q: 13.82\n",
      "Episode: 491, Total Reward (running avg):   22 (112.35) Epsilon: 0.508, Avg Q: 12.75\n",
      "Episode: 492, Total Reward (running avg):   76 (110.53) Epsilon: 0.507, Avg Q: 13.65\n",
      "Episode: 493, Total Reward (running avg):  108 (110.40) Epsilon: 0.506, Avg Q: 13.84\n",
      "Episode: 494, Total Reward (running avg):   18 (105.78) Epsilon: 0.505, Avg Q: 12\n",
      "Episode: 495, Total Reward (running avg):   13 (101.15) Epsilon: 0.504, Avg Q: 10.79\n",
      "Episode: 496, Total Reward (running avg):  200 (106.09) Epsilon: 0.503, Avg Q: 13.77\n",
      "Episode: 497, Total Reward (running avg):  187 (110.13) Epsilon: 0.502, Avg Q: 13.94\n",
      "Episode: 498, Total Reward (running avg):  185 (113.88) Epsilon: 0.501, Avg Q: 13.81\n",
      "Episode: 499, Total Reward (running avg):   41 (110.23) Epsilon: 0.500, Avg Q: 13.62\n",
      "Episode: 500, Total Reward (running avg):   46 (107.02) Epsilon: 0.499, Avg Q: 13.71\n",
      "Episode: 501, Total Reward (running avg):  148 (109.07) Epsilon: 0.498, Avg Q: 13.96\n",
      "Episode: 502, Total Reward (running avg):  200 (113.62) Epsilon: 0.497, Avg Q: 14.19\n",
      "Episode: 503, Total Reward (running avg):   87 (112.29) Epsilon: 0.496, Avg Q: 14.03\n",
      "Episode: 504, Total Reward (running avg):   70 (110.17) Epsilon: 0.495, Avg Q: 13.94\n",
      "Episode: 505, Total Reward (running avg):  197 (114.51) Epsilon: 0.494, Avg Q: 14.2\n",
      "Episode: 506, Total Reward (running avg):  121 (114.84) Epsilon: 0.493, Avg Q: 14.38\n",
      "Episode: 507, Total Reward (running avg):   22 (110.20) Epsilon: 0.492, Avg Q: 12.75\n",
      "Episode: 508, Total Reward (running avg):   86 (108.99) Epsilon: 0.491, Avg Q: 14.26\n",
      "Episode: 509, Total Reward (running avg):  200 (113.54) Epsilon: 0.490, Avg Q: 14.8\n",
      "Episode: 510, Total Reward (running avg):  191 (117.41) Epsilon: 0.489, Avg Q: 13.97\n",
      "Episode: 511, Total Reward (running avg):  200 (121.54) Epsilon: 0.488, Avg Q: 14.37\n",
      "Episode: 512, Total Reward (running avg):  146 (122.76) Epsilon: 0.487, Avg Q: 14.75\n",
      "Episode: 513, Total Reward (running avg):  182 (125.72) Epsilon: 0.486, Avg Q: 14.77\n",
      "Episode: 514, Total Reward (running avg):  200 (129.44) Epsilon: 0.485, Avg Q: 14.41\n",
      "Episode: 515, Total Reward (running avg):   35 (124.72) Epsilon: 0.484, Avg Q: 13.89\n",
      "Episode: 516, Total Reward (running avg):   29 (119.93) Epsilon: 0.483, Avg Q: 13.43\n",
      "Episode: 517, Total Reward (running avg):   41 (115.98) Epsilon: 0.482, Avg Q: 14.12\n",
      "Episode: 518, Total Reward (running avg):  200 (120.18) Epsilon: 0.481, Avg Q: 14.57\n",
      "Episode: 519, Total Reward (running avg):   32 (115.78) Epsilon: 0.480, Avg Q: 13.43\n",
      "Episode: 520, Total Reward (running avg):  171 (118.54) Epsilon: 0.479, Avg Q: 14.29\n",
      "Episode: 521, Total Reward (running avg):  104 (117.81) Epsilon: 0.478, Avg Q: 14.81\n",
      "Episode: 522, Total Reward (running avg):  200 (121.92) Epsilon: 0.477, Avg Q: 15.41\n",
      "Episode: 523, Total Reward (running avg):  137 (122.67) Epsilon: 0.476, Avg Q: 15.25\n",
      "Episode: 524, Total Reward (running avg):   76 (120.34) Epsilon: 0.475, Avg Q: 14.93\n",
      "Episode: 525, Total Reward (running avg):  200 (124.32) Epsilon: 0.474, Avg Q: 15.42\n",
      "Episode: 526, Total Reward (running avg):   88 (122.51) Epsilon: 0.473, Avg Q: 14.97\n",
      "Episode: 527, Total Reward (running avg):   35 (118.13) Epsilon: 0.472, Avg Q: 13.81\n",
      "Episode: 528, Total Reward (running avg):  200 (122.22) Epsilon: 0.471, Avg Q: 14.96\n",
      "Episode: 529, Total Reward (running avg):  200 (126.11) Epsilon: 0.470, Avg Q: 14.96\n",
      "Episode: 530, Total Reward (running avg):  200 (129.81) Epsilon: 0.469, Avg Q: 15.67\n",
      "Episode: 531, Total Reward (running avg):   30 (124.82) Epsilon: 0.468, Avg Q: 13.84\n",
      "Episode: 532, Total Reward (running avg):  152 (126.18) Epsilon: 0.467, Avg Q: 15.29\n",
      "Episode: 533, Total Reward (running avg):   33 (121.52) Epsilon: 0.466, Avg Q: 14.21\n",
      "Episode: 534, Total Reward (running avg):  200 (125.44) Epsilon: 0.465, Avg Q: 15.78\n",
      "Episode: 535, Total Reward (running avg):   87 (123.52) Epsilon: 0.464, Avg Q: 15.2\n",
      "Episode: 536, Total Reward (running avg):   60 (120.34) Epsilon: 0.463, Avg Q: 15.21\n",
      "Episode: 537, Total Reward (running avg):  200 (124.33) Epsilon: 0.462, Avg Q: 15.89\n",
      "Episode: 538, Total Reward (running avg):  144 (125.31) Epsilon: 0.461, Avg Q: 15.5\n",
      "Episode: 539, Total Reward (running avg):  171 (127.59) Epsilon: 0.460, Avg Q: 15.6\n",
      "Episode: 540, Total Reward (running avg):  102 (126.31) Epsilon: 0.459, Avg Q: 15.6\n",
      "Episode: 541, Total Reward (running avg):   93 (124.65) Epsilon: 0.458, Avg Q: 15.39\n",
      "Episode: 542, Total Reward (running avg):   48 (120.82) Epsilon: 0.457, Avg Q: 14.9\n",
      "Episode: 543, Total Reward (running avg):  199 (124.73) Epsilon: 0.456, Avg Q: 14.87\n",
      "Episode: 544, Total Reward (running avg):  200 (128.49) Epsilon: 0.455, Avg Q: 16.2\n",
      "Episode: 545, Total Reward (running avg):  138 (128.97) Epsilon: 0.454, Avg Q: 15.77\n",
      "Episode: 546, Total Reward (running avg):  130 (129.02) Epsilon: 0.453, Avg Q: 15.77\n",
      "Episode: 547, Total Reward (running avg):  163 (130.72) Epsilon: 0.452, Avg Q: 15.94\n",
      "Episode: 548, Total Reward (running avg):  108 (129.58) Epsilon: 0.451, Avg Q: 15.78\n",
      "Episode: 549, Total Reward (running avg):   63 (126.25) Epsilon: 0.450, Avg Q: 15.55\n",
      "Episode: 550, Total Reward (running avg):  200 (129.94) Epsilon: 0.449, Avg Q: 16.2\n",
      "Episode: 551, Total Reward (running avg):  200 (133.44) Epsilon: 0.448, Avg Q: 15.55\n",
      "Episode: 552, Total Reward (running avg):  185 (136.02) Epsilon: 0.447, Avg Q: 15.76\n",
      "Episode: 553, Total Reward (running avg):  200 (139.22) Epsilon: 0.446, Avg Q: 16.42\n",
      "Episode: 554, Total Reward (running avg):  200 (142.26) Epsilon: 0.445, Avg Q: 16.35\n",
      "Episode: 555, Total Reward (running avg):  200 (145.14) Epsilon: 0.444, Avg Q: 16.39\n",
      "Episode: 556, Total Reward (running avg):  200 (147.89) Epsilon: 0.443, Avg Q: 16.55\n",
      "Episode: 557, Total Reward (running avg):  190 (149.99) Epsilon: 0.442, Avg Q: 15.94\n",
      "Episode: 558, Total Reward (running avg):  184 (151.69) Epsilon: 0.441, Avg Q: 16.35\n",
      "Episode: 559, Total Reward (running avg):  200 (154.11) Epsilon: 0.440, Avg Q: 16.33\n",
      "Episode: 560, Total Reward (running avg):   91 (150.95) Epsilon: 0.439, Avg Q: 16.1\n",
      "Episode: 561, Total Reward (running avg):  185 (152.66) Epsilon: 0.438, Avg Q: 15.65\n",
      "Episode: 562, Total Reward (running avg):  200 (155.02) Epsilon: 0.437, Avg Q: 16.68\n",
      "Episode: 563, Total Reward (running avg):  187 (156.62) Epsilon: 0.436, Avg Q: 16.34\n",
      "Episode: 564, Total Reward (running avg):  116 (154.59) Epsilon: 0.435, Avg Q: 16.35\n",
      "Episode: 565, Total Reward (running avg):  200 (156.86) Epsilon: 0.434, Avg Q: 16.02\n",
      "Episode: 566, Total Reward (running avg):  200 (159.02) Epsilon: 0.433, Avg Q: 16.45\n",
      "Episode: 567, Total Reward (running avg):  137 (157.92) Epsilon: 0.432, Avg Q: 16.41\n",
      "Episode: 568, Total Reward (running avg):  200 (160.02) Epsilon: 0.431, Avg Q: 16.17\n",
      "Episode: 569, Total Reward (running avg):  200 (162.02) Epsilon: 0.430, Avg Q: 16.86\n",
      "Episode: 570, Total Reward (running avg):  200 (163.92) Epsilon: 0.429, Avg Q: 16.72\n",
      "Episode: 571, Total Reward (running avg):  200 (165.72) Epsilon: 0.428, Avg Q: 16.48\n",
      "Episode: 572, Total Reward (running avg):  114 (163.14) Epsilon: 0.427, Avg Q: 15.93\n",
      "Episode: 573, Total Reward (running avg):   46 (157.28) Epsilon: 0.426, Avg Q: 16.13\n",
      "Episode: 574, Total Reward (running avg):   43 (151.57) Epsilon: 0.425, Avg Q: 15.75\n",
      "Episode: 575, Total Reward (running avg):  182 (153.09) Epsilon: 0.424, Avg Q: 16.18\n",
      "Episode: 576, Total Reward (running avg):  200 (155.43) Epsilon: 0.423, Avg Q: 16.83\n",
      "Episode: 577, Total Reward (running avg):  172 (156.26) Epsilon: 0.422, Avg Q: 15.78\n",
      "Episode: 578, Total Reward (running avg):   62 (151.55) Epsilon: 0.421, Avg Q: 16.14\n",
      "Episode: 579, Total Reward (running avg):  159 (151.92) Epsilon: 0.420, Avg Q: 16.48\n",
      "Episode: 580, Total Reward (running avg):  125 (150.58) Epsilon: 0.419, Avg Q: 16.71\n",
      "Episode: 581, Total Reward (running avg):   70 (146.55) Epsilon: 0.418, Avg Q: 16.38\n",
      "Episode: 582, Total Reward (running avg):  200 (149.22) Epsilon: 0.417, Avg Q: 17.26\n",
      "Episode: 583, Total Reward (running avg):  144 (148.96) Epsilon: 0.416, Avg Q: 16.39\n",
      "Episode: 584, Total Reward (running avg):  124 (147.71) Epsilon: 0.415, Avg Q: 16.8\n",
      "Episode: 585, Total Reward (running avg):  200 (150.32) Epsilon: 0.414, Avg Q: 17.12\n",
      "Episode: 586, Total Reward (running avg):  200 (152.81) Epsilon: 0.413, Avg Q: 17.09\n",
      "Episode: 587, Total Reward (running avg):  150 (152.67) Epsilon: 0.412, Avg Q: 16.71\n",
      "Episode: 588, Total Reward (running avg):  200 (155.03) Epsilon: 0.411, Avg Q: 17.4\n",
      "Episode: 589, Total Reward (running avg):  200 (157.28) Epsilon: 0.410, Avg Q: 17.02\n",
      "Episode: 590, Total Reward (running avg):  200 (159.42) Epsilon: 0.409, Avg Q: 16.67\n",
      "Episode: 591, Total Reward (running avg):  200 (161.45) Epsilon: 0.408, Avg Q: 17.1\n",
      "Episode: 592, Total Reward (running avg):  200 (163.38) Epsilon: 0.407, Avg Q: 16.88\n",
      "Episode: 593, Total Reward (running avg):  200 (165.21) Epsilon: 0.406, Avg Q: 16.5\n",
      "Episode: 594, Total Reward (running avg):   74 (160.65) Epsilon: 0.405, Avg Q: 16.64\n",
      "Episode: 595, Total Reward (running avg):  200 (162.61) Epsilon: 0.404, Avg Q: 16.63\n",
      "Episode: 596, Total Reward (running avg):   65 (157.73) Epsilon: 0.403, Avg Q: 16.36\n",
      "Episode: 597, Total Reward (running avg):  200 (159.85) Epsilon: 0.402, Avg Q: 17.06\n",
      "Episode: 598, Total Reward (running avg):  200 (161.85) Epsilon: 0.401, Avg Q: 17.25\n",
      "Episode: 599, Total Reward (running avg):  200 (163.76) Epsilon: 0.400, Avg Q: 17.3\n",
      "Episode: 600, Total Reward (running avg):  200 (165.57) Epsilon: 0.399, Avg Q: 17.59\n",
      "Episode: 601, Total Reward (running avg):  195 (167.04) Epsilon: 0.398, Avg Q: 16.59\n",
      "Episode: 602, Total Reward (running avg):  200 (168.69) Epsilon: 0.397, Avg Q: 17.41\n",
      "Episode: 603, Total Reward (running avg):  200 (170.26) Epsilon: 0.396, Avg Q: 17.51\n",
      "Episode: 604, Total Reward (running avg):  195 (171.50) Epsilon: 0.395, Avg Q: 16.71\n",
      "Episode: 605, Total Reward (running avg):  200 (172.92) Epsilon: 0.394, Avg Q: 17.74\n",
      "Episode: 606, Total Reward (running avg):  200 (174.27) Epsilon: 0.393, Avg Q: 17.32\n",
      "Episode: 607, Total Reward (running avg):   96 (170.36) Epsilon: 0.392, Avg Q: 17.26\n",
      "Episode: 608, Total Reward (running avg):   73 (165.49) Epsilon: 0.391, Avg Q: 16.97\n",
      "Episode: 609, Total Reward (running avg):  200 (167.22) Epsilon: 0.390, Avg Q: 17.71\n",
      "Episode: 610, Total Reward (running avg):  200 (168.86) Epsilon: 0.389, Avg Q: 17.5\n",
      "Episode: 611, Total Reward (running avg):  200 (170.41) Epsilon: 0.388, Avg Q: 17.81\n",
      "Episode: 612, Total Reward (running avg):  184 (171.09) Epsilon: 0.387, Avg Q: 16.33\n",
      "Episode: 613, Total Reward (running avg):  143 (169.69) Epsilon: 0.386, Avg Q: 17.25\n",
      "Episode: 614, Total Reward (running avg):   78 (165.10) Epsilon: 0.385, Avg Q: 17.07\n",
      "Episode: 615, Total Reward (running avg):   31 (158.40) Epsilon: 0.384, Avg Q: 16.15\n",
      "Episode: 616, Total Reward (running avg):  200 (160.48) Epsilon: 0.383, Avg Q: 16.69\n",
      "Episode: 617, Total Reward (running avg):  109 (157.91) Epsilon: 0.382, Avg Q: 17.51\n",
      "Episode: 618, Total Reward (running avg):  139 (156.96) Epsilon: 0.381, Avg Q: 17.57\n",
      "Episode: 619, Total Reward (running avg):   14 (149.81) Epsilon: 0.380, Avg Q: 13.62\n",
      "Episode: 620, Total Reward (running avg):  167 (150.67) Epsilon: 0.379, Avg Q: 16.64\n",
      "Episode: 621, Total Reward (running avg):  196 (152.94) Epsilon: 0.378, Avg Q: 16.8\n",
      "Episode: 622, Total Reward (running avg):  200 (155.29) Epsilon: 0.377, Avg Q: 17.78\n",
      "Episode: 623, Total Reward (running avg):  200 (157.53) Epsilon: 0.376, Avg Q: 17.67\n",
      "Episode: 624, Total Reward (running avg):   98 (154.55) Epsilon: 0.375, Avg Q: 17.52\n",
      "Episode: 625, Total Reward (running avg):  200 (156.82) Epsilon: 0.374, Avg Q: 17.87\n",
      "Episode: 626, Total Reward (running avg):  166 (157.28) Epsilon: 0.373, Avg Q: 17.27\n",
      "Episode: 627, Total Reward (running avg):  161 (157.47) Epsilon: 0.372, Avg Q: 17.79\n",
      "Episode: 628, Total Reward (running avg):  200 (159.59) Epsilon: 0.371, Avg Q: 18.02\n",
      "Episode: 629, Total Reward (running avg):  137 (158.46) Epsilon: 0.370, Avg Q: 17.4\n",
      "Episode: 630, Total Reward (running avg):  200 (160.54) Epsilon: 0.369, Avg Q: 18.21\n",
      "Episode: 631, Total Reward (running avg):  200 (162.51) Epsilon: 0.368, Avg Q: 17.92\n",
      "Episode: 632, Total Reward (running avg):  200 (164.39) Epsilon: 0.367, Avg Q: 18.15\n",
      "Episode: 633, Total Reward (running avg):  200 (166.17) Epsilon: 0.366, Avg Q: 17.85\n",
      "Episode: 634, Total Reward (running avg):  200 (167.86) Epsilon: 0.365, Avg Q: 17.52\n",
      "Episode: 635, Total Reward (running avg):  200 (169.47) Epsilon: 0.364, Avg Q: 17.06\n",
      "Episode: 636, Total Reward (running avg):  200 (170.99) Epsilon: 0.363, Avg Q: 17.98\n",
      "Episode: 637, Total Reward (running avg):  200 (172.44) Epsilon: 0.362, Avg Q: 18.14\n",
      "Episode: 638, Total Reward (running avg):  200 (173.82) Epsilon: 0.361, Avg Q: 18.33\n",
      "Episode: 639, Total Reward (running avg):  200 (175.13) Epsilon: 0.360, Avg Q: 18.06\n",
      "Episode: 640, Total Reward (running avg):  200 (176.37) Epsilon: 0.359, Avg Q: 17.26\n",
      "Episode: 641, Total Reward (running avg):  165 (175.81) Epsilon: 0.358, Avg Q: 16.51\n",
      "Episode: 642, Total Reward (running avg):  200 (177.02) Epsilon: 0.357, Avg Q: 17.97\n",
      "Episode: 643, Total Reward (running avg):  200 (178.16) Epsilon: 0.356, Avg Q: 18.15\n",
      "Episode: 644, Total Reward (running avg):  200 (179.26) Epsilon: 0.355, Avg Q: 17.75\n",
      "Episode: 645, Total Reward (running avg):  200 (180.29) Epsilon: 0.354, Avg Q: 17.98\n",
      "Episode: 646, Total Reward (running avg):  200 (181.28) Epsilon: 0.353, Avg Q: 18.35\n",
      "Episode: 647, Total Reward (running avg):   35 (173.97) Epsilon: 0.352, Avg Q: 16.65\n",
      "Episode: 648, Total Reward (running avg):  200 (175.27) Epsilon: 0.351, Avg Q: 17.98\n",
      "Episode: 649, Total Reward (running avg):   19 (167.45) Epsilon: 0.350, Avg Q: 15.21\n",
      "Episode: 650, Total Reward (running avg):  200 (169.08) Epsilon: 0.349, Avg Q: 17.31\n",
      "Episode: 651, Total Reward (running avg):  200 (170.63) Epsilon: 0.348, Avg Q: 18.35\n",
      "Episode: 652, Total Reward (running avg):  200 (172.10) Epsilon: 0.347, Avg Q: 18.49\n",
      "Episode: 653, Total Reward (running avg):  200 (173.49) Epsilon: 0.346, Avg Q: 18.54\n",
      "Episode: 654, Total Reward (running avg):  200 (174.82) Epsilon: 0.345, Avg Q: 18.57\n",
      "Episode: 655, Total Reward (running avg):  200 (176.08) Epsilon: 0.344, Avg Q: 18.5\n",
      "Episode: 656, Total Reward (running avg):  200 (177.27) Epsilon: 0.343, Avg Q: 18.3\n",
      "Episode: 657, Total Reward (running avg):  200 (178.41) Epsilon: 0.342, Avg Q: 18.41\n",
      "Episode: 658, Total Reward (running avg):   78 (173.39) Epsilon: 0.341, Avg Q: 17.71\n",
      "Episode: 659, Total Reward (running avg):  200 (174.72) Epsilon: 0.340, Avg Q: 18.32\n",
      "Episode: 660, Total Reward (running avg):  200 (175.98) Epsilon: 0.339, Avg Q: 18.25\n",
      "Episode: 661, Total Reward (running avg):  200 (177.18) Epsilon: 0.338, Avg Q: 17.76\n",
      "Episode: 662, Total Reward (running avg):  200 (178.32) Epsilon: 0.337, Avg Q: 18.24\n",
      "Episode: 663, Total Reward (running avg):  200 (179.41) Epsilon: 0.336, Avg Q: 18.31\n",
      "Episode: 664, Total Reward (running avg):  200 (180.44) Epsilon: 0.335, Avg Q: 17.89\n",
      "Episode: 665, Total Reward (running avg):  200 (181.42) Epsilon: 0.334, Avg Q: 18.09\n",
      "Episode: 666, Total Reward (running avg):   58 (175.24) Epsilon: 0.333, Avg Q: 17.45\n",
      "Episode: 667, Total Reward (running avg):  200 (176.48) Epsilon: 0.332, Avg Q: 18.43\n",
      "Episode: 668, Total Reward (running avg):  200 (177.66) Epsilon: 0.331, Avg Q: 17.84\n",
      "Episode: 669, Total Reward (running avg):   75 (172.53) Epsilon: 0.330, Avg Q: 17.44\n",
      "Episode: 670, Total Reward (running avg):  174 (172.60) Epsilon: 0.329, Avg Q: 17.76\n",
      "Episode: 671, Total Reward (running avg):   68 (167.37) Epsilon: 0.328, Avg Q: 17.51\n",
      "Episode: 672, Total Reward (running avg):  200 (169.00) Epsilon: 0.327, Avg Q: 17.3\n",
      "Episode: 673, Total Reward (running avg):  200 (170.55) Epsilon: 0.326, Avg Q: 17.52\n",
      "Episode: 674, Total Reward (running avg):  200 (172.02) Epsilon: 0.325, Avg Q: 17.83\n",
      "Episode: 675, Total Reward (running avg):  200 (173.42) Epsilon: 0.324, Avg Q: 18.4\n",
      "Episode: 676, Total Reward (running avg):  200 (174.75) Epsilon: 0.323, Avg Q: 18.5\n",
      "Episode: 677, Total Reward (running avg):  200 (176.01) Epsilon: 0.322, Avg Q: 18.42\n",
      "Episode: 678, Total Reward (running avg):  200 (177.21) Epsilon: 0.321, Avg Q: 18.47\n",
      "Episode: 679, Total Reward (running avg):  200 (178.35) Epsilon: 0.320, Avg Q: 18.1\n",
      "Episode: 680, Total Reward (running avg):  200 (179.43) Epsilon: 0.319, Avg Q: 18.05\n",
      "Episode: 681, Total Reward (running avg):  200 (180.46) Epsilon: 0.318, Avg Q: 18.45\n",
      "Episode: 682, Total Reward (running avg):  200 (181.44) Epsilon: 0.317, Avg Q: 17.49\n",
      "Episode: 683, Total Reward (running avg):  200 (182.37) Epsilon: 0.316, Avg Q: 18.15\n",
      "Episode: 684, Total Reward (running avg):  200 (183.25) Epsilon: 0.315, Avg Q: 18.09\n",
      "Episode: 685, Total Reward (running avg):  200 (184.09) Epsilon: 0.314, Avg Q: 18.32\n",
      "Episode: 686, Total Reward (running avg):  200 (184.88) Epsilon: 0.313, Avg Q: 17.77\n",
      "Episode: 687, Total Reward (running avg):  182 (184.74) Epsilon: 0.312, Avg Q: 18\n",
      "Episode: 688, Total Reward (running avg):  200 (185.50) Epsilon: 0.311, Avg Q: 18.6\n",
      "Episode: 689, Total Reward (running avg):  200 (186.23) Epsilon: 0.310, Avg Q: 18.11\n",
      "Episode: 690, Total Reward (running avg):  200 (186.92) Epsilon: 0.309, Avg Q: 18.68\n",
      "Episode: 691, Total Reward (running avg):  200 (187.57) Epsilon: 0.308, Avg Q: 18.49\n",
      "Episode: 692, Total Reward (running avg):  200 (188.19) Epsilon: 0.307, Avg Q: 18.32\n",
      "Episode: 693, Total Reward (running avg):  200 (188.78) Epsilon: 0.306, Avg Q: 18.56\n",
      "Episode: 694, Total Reward (running avg):  200 (189.34) Epsilon: 0.305, Avg Q: 17.6\n",
      "Episode: 695, Total Reward (running avg):  200 (189.88) Epsilon: 0.304, Avg Q: 18.68\n",
      "Episode: 696, Total Reward (running avg):  200 (190.38) Epsilon: 0.303, Avg Q: 18.7\n",
      "Episode: 697, Total Reward (running avg):  168 (189.26) Epsilon: 0.302, Avg Q: 18.38\n",
      "Episode: 698, Total Reward (running avg):  200 (189.80) Epsilon: 0.301, Avg Q: 18.56\n",
      "Episode: 699, Total Reward (running avg):  200 (190.31) Epsilon: 0.300, Avg Q: 18.52\n",
      "Episode: 700, Total Reward (running avg):  200 (190.79) Epsilon: 0.299, Avg Q: 18.7\n",
      "Episode: 701, Total Reward (running avg):   70 (184.75) Epsilon: 0.298, Avg Q: 17.89\n",
      "Episode: 702, Total Reward (running avg):  200 (185.52) Epsilon: 0.297, Avg Q: 18.56\n",
      "Episode: 703, Total Reward (running avg):  200 (186.24) Epsilon: 0.296, Avg Q: 18.75\n",
      "Episode: 704, Total Reward (running avg):  200 (186.93) Epsilon: 0.295, Avg Q: 18.45\n",
      "Episode: 705, Total Reward (running avg):   31 (179.13) Epsilon: 0.294, Avg Q: 16.66\n",
      "Episode: 706, Total Reward (running avg):  200 (180.18) Epsilon: 0.293, Avg Q: 18.56\n",
      "Episode: 707, Total Reward (running avg):  200 (181.17) Epsilon: 0.292, Avg Q: 18.91\n",
      "Episode: 708, Total Reward (running avg):  200 (182.11) Epsilon: 0.291, Avg Q: 18.62\n",
      "Episode: 709, Total Reward (running avg):  200 (183.00) Epsilon: 0.290, Avg Q: 18.83\n",
      "Episode: 710, Total Reward (running avg):   18 (174.75) Epsilon: 0.289, Avg Q: 15.18\n",
      "Episode: 711, Total Reward (running avg):  200 (176.02) Epsilon: 0.288, Avg Q: 18.61\n",
      "Episode: 712, Total Reward (running avg):  200 (177.21) Epsilon: 0.287, Avg Q: 18.89\n",
      "Episode: 713, Total Reward (running avg):  200 (178.35) Epsilon: 0.286, Avg Q: 18.96\n",
      "Episode: 714, Total Reward (running avg):  200 (179.44) Epsilon: 0.285, Avg Q: 18.64\n",
      "Episode: 715, Total Reward (running avg):  200 (180.46) Epsilon: 0.284, Avg Q: 19.03\n",
      "Episode: 716, Total Reward (running avg):  200 (181.44) Epsilon: 0.283, Avg Q: 18.47\n",
      "Episode: 717, Total Reward (running avg):  200 (182.37) Epsilon: 0.282, Avg Q: 18.89\n",
      "Episode: 718, Total Reward (running avg):  200 (183.25) Epsilon: 0.281, Avg Q: 19.03\n",
      "Episode: 719, Total Reward (running avg):  200 (184.09) Epsilon: 0.280, Avg Q: 18.54\n",
      "Episode: 720, Total Reward (running avg):  200 (184.88) Epsilon: 0.279, Avg Q: 19.08\n",
      "Episode: 721, Total Reward (running avg):  200 (185.64) Epsilon: 0.278, Avg Q: 19\n",
      "Episode: 722, Total Reward (running avg):  127 (182.71) Epsilon: 0.277, Avg Q: 18.45\n",
      "Episode: 723, Total Reward (running avg):  200 (183.57) Epsilon: 0.276, Avg Q: 18.47\n",
      "Episode: 724, Total Reward (running avg):  200 (184.39) Epsilon: 0.275, Avg Q: 18.81\n",
      "Episode: 725, Total Reward (running avg):  200 (185.17) Epsilon: 0.274, Avg Q: 18.56\n",
      "Episode: 726, Total Reward (running avg):  200 (185.92) Epsilon: 0.273, Avg Q: 19\n",
      "Episode: 727, Total Reward (running avg):  200 (186.62) Epsilon: 0.272, Avg Q: 18.48\n",
      "Episode: 728, Total Reward (running avg):  200 (187.29) Epsilon: 0.271, Avg Q: 18.94\n",
      "Episode: 729, Total Reward (running avg):  200 (187.92) Epsilon: 0.270, Avg Q: 18.5\n",
      "Episode: 730, Total Reward (running avg):  200 (188.53) Epsilon: 0.269, Avg Q: 18.21\n",
      "Episode: 731, Total Reward (running avg):  200 (189.10) Epsilon: 0.268, Avg Q: 18.93\n",
      "Episode: 732, Total Reward (running avg):  200 (189.65) Epsilon: 0.267, Avg Q: 17.78\n",
      "Episode: 733, Total Reward (running avg):  200 (190.16) Epsilon: 0.266, Avg Q: 18.82\n",
      "Episode: 734, Total Reward (running avg):  200 (190.66) Epsilon: 0.265, Avg Q: 18.88\n",
      "Episode: 735, Total Reward (running avg):  200 (191.12) Epsilon: 0.264, Avg Q: 18.13\n",
      "Episode: 736, Total Reward (running avg):  200 (191.57) Epsilon: 0.263, Avg Q: 18.95\n",
      "Episode: 737, Total Reward (running avg):  200 (191.99) Epsilon: 0.262, Avg Q: 18.57\n",
      "Episode: 738, Total Reward (running avg):  200 (192.39) Epsilon: 0.261, Avg Q: 18.9\n",
      "Episode: 739, Total Reward (running avg):  200 (192.77) Epsilon: 0.260, Avg Q: 18.91\n",
      "Episode: 740, Total Reward (running avg):  200 (193.13) Epsilon: 0.259, Avg Q: 18.48\n",
      "Episode: 741, Total Reward (running avg):  200 (193.47) Epsilon: 0.258, Avg Q: 18.89\n",
      "Episode: 742, Total Reward (running avg):  200 (193.80) Epsilon: 0.257, Avg Q: 18.73\n",
      "Episode: 743, Total Reward (running avg):  200 (194.11) Epsilon: 0.256, Avg Q: 18.85\n",
      "Episode: 744, Total Reward (running avg):  200 (194.41) Epsilon: 0.255, Avg Q: 18.84\n",
      "Episode: 745, Total Reward (running avg):  200 (194.69) Epsilon: 0.254, Avg Q: 18.54\n",
      "Episode: 746, Total Reward (running avg):  200 (194.95) Epsilon: 0.253, Avg Q: 18.45\n",
      "Episode: 747, Total Reward (running avg):  200 (195.20) Epsilon: 0.252, Avg Q: 18.69\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_episodes = 1200 \n",
    "batch_size = 128\n",
    "gamma = .94\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "ddqn = DoubleQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)\n",
    "\n",
    "# Train\n",
    "R, R_avg = train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52dee2317f03bced29c85c28462d4ad5",
     "grade": false,
     "grade_id": "cell-4757be1a3ec18b56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# close window\n",
    "if enable_visualization:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb3f82e40e11a139f40e181799b40a93",
     "grade": false,
     "grade_id": "cell-8f1ad36de733ed92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "According to the code above, the code in the provided .py file, and the documentation of the environment, answer the following questions:\n",
    "    \n",
    "What is the state for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f28740b9ccf7bf2fa7f166c9fbde003",
     "grade": true,
     "grade_id": "cell-0a780f1afdcd6b1a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "The state is the information used by the agent to determine what action to do in the next step. \n",
    "In this problem the agents state is described by the position of the cart, with respect to the initial place in centre, the velocity of the cart and the angle of the pole and its velocity. The state for this problem is therefore the information that the agent can recieve from the environment.\n",
    "\n",
    "The num_state is 4 for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5c4c4717846aa74e9713f9e67c5c7e8",
     "grade": false,
     "grade_id": "cell-50a080269bf6f296",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** How often is the offline network updated to match the online one? Why do we need to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5dba2847e8894bd0aa6d6fddbe84cf22",
     "grade": true,
     "grade_id": "cell-099530ded38d7038",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "                    \n",
    "Then offline network is updated when the method ddqn.update_target_network() is called, this is done in the training loop after every 1000 training steps (with one training step meaning calling: ddqn.optimizer.step()).\n",
    "\n",
    "The role of the offline network is to evaluate the state action value for an action in the next state. The online network is then used to choose which action that is the best. The reason that we don't want to update the offline network as frequently is that we want to avoid to overestimate the state action values, which then makes the training faster and the policies better.\n",
    "\n",
    "We want to explore and learn a good policy but also learn a baseline to compare our performance against. The offline network serves as our baseline. However, if our baseline changes too much, it will become hard to learn which is why we need to only update periodically, making divergence or oscillations much more unlikely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf22be529ec727e123d981715c493c09",
     "grade": false,
     "grade_id": "cell-db1ad2492dd6680a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(4 POE) Optional**\n",
    "\n",
    "There may be three reasons for the episode to end:\n",
    "\n",
    "1. The cart slides too far away\n",
    "1. The pendulum falls too low\n",
    "1. 200 time steps have passed\n",
    "\n",
    "As mentioned before, we replace the Q target with the immediate reward only in case 1 and 2. In the third case however, the Q target remains untouched.\n",
    "\n",
    "Please answer the following questions:\n",
    "\n",
    "1. How we treat this matter will have an influence on the Q-values being learned, and how they may be interpreted. Assuming we treat it as explained, and that we have managed to converge successfully, describe (in words and/or with mathematical expressions) what the Q-values we have learnt represent.\n",
    "\n",
    "1. If we would treat case 3 the same as case 1 and 2, we would actually end up with a Partially Observable Markov Decision Process (POMDP). What would we need to add to our observations, in order to obtain an observable MDP again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9072e9591cd5b0d65e20e660eadcf6f",
     "grade": true,
     "grade_id": "cell-629034eb3ae20711",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "1. The learned Q-values represent how good an action is given the agents current state and can be used to guide the agent what to do next. These are uppdated when the pendulum falls too low or the cart slide too far, from these experiences the agent can learn and improve, so it won't repeat the mistakes the over and over again. For the third reason, the learned Q-values has a different meaning. After 200 time steps, the Q target remains untouched and the termination is therefor treated as an interuption of the training. These corresponding Q-values learns how good the action is for every state in order to avoid termination for as long as possible.\n",
    "\n",
    "\n",
    "2. The reason why we would end up with an POMDP if cases 3 would be treated the same as case 1 and 2 is because the model is unaware of if the training stops because it took a bad action or if it was becaues it was on the 200 timestep. Exactly the same action from the same states could either end in termination (if number of time step = 200) or not if the time is less than 200. In order to obtain an observable MDP again, one needs to add time steps to the state of the problem. In that case the agent will obtain information in the time steps that the algortithm will terminate after 200 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a6271028b977453e6a9c87ee6f23130",
     "grade": false,
     "grade_id": "cell-0836fc1b783d1158",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to visualize your final policy (the greedy rather than epsilon-greedy one) in an episode from this environment.\n",
    "\n",
    "**Note:** In order to visualize, the env.render() command needs to work out on your system (see comment a few cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63b264cd696948f8294236bb5c4e4fa5",
     "grade": false,
     "grade_id": "cell-1e8a9b49909882ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "num_episodes = 10\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if enable_visualization:\n",
    "    for i in range(num_episodes):\n",
    "            state = env.reset() #reset to initial state\n",
    "            state = state[None,:]\n",
    "            terminal = False # reset terminal flag\n",
    "            while not terminal:\n",
    "                #env.render()\n",
    "                time.sleep(.05)\n",
    "                with torch.no_grad():\n",
    "                    q_values = ddqn.online_model(torch.tensor(state, dtype=torch.float, device=device)).cpu().numpy()\n",
    "                policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "                action = np.random.choice(num_actions, p=policy)\n",
    "                state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "                state = state[None,:]\n",
    "    # close window\n",
    "    env.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91c95cdee8f1715e789b8bdf2a4e2ff8",
     "grade": false,
     "grade_id": "cell-0bb5d237ca6839d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Plot the episodic rewards obtained throughout the optimization, together with a moving average of it (since the episodic reward is usually very noisy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a49cda53e12c1b8a976338c0f8bff7b9",
     "grade": false,
     "grade_id": "cell-a3c72b1dbffd2db4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAEGCAYAAAD8PTu1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABkyklEQVR4nO29d5xcV3n//z7TdrYXbdXuSqvebTVLLtiWccEVGzBgOyZOYnAMpH0hhBpDSPL9QkLMLyQkwQECBGzAGIPB3ZZ7l2SrWJastpJ2tb33aef3x50+987cmbmzO7t73q+XXnvn3HPPeWZ2dT7zPOec5wgpJQqFQqFQzGVsM22AQqFQKBS5RomdQqFQKOY8SuwUCoVCMedRYqdQKBSKOY8SO4VCoVDMeRwzbQBAdXW1bGlpmWkzFAqFYlaxe/fuXillTfC61uFwfB9Yz/xzZALAAZ/P9/EtW7Z061XIC7FraWlh165dM22GQqFQzCqEECdD1w6H4/v19fVrampqBmw227zaUxYIBERPT8/azs7O7wPv16sz39RfoVAo5irra2pqhueb0AHYbDZZU1MzhObV6teZRnsUCoVCkTts81HoQgTfu6GmKbFTKBQKxZwnL+bsFAqFQjH7sdvtW1asWDHh9/tFc3Pz1C9/+csT1dXV/pm2C5Rnp1AoFAqLKCgoCBw6dOjgkSNH3q6oqPD98z//c81M2xRCiZ1CoVAoLOfcc88da29vd820HSFShjGFEM3AT4B6tL0M90gp/1UIUQX8AmgBWoGPSCkHgs98Ebgd8AN/IaV8PCfWKxQKhSKBF470FPePeSydpqoqdvkuXFEzZqauz+fjmWeeKb399tt7rbQhG8x8GD7gs1LKPUKIUmC3EOJJ4I+Ap6WU3xBCfAH4AvB5IcRa4CZgHbAQeEoIsVJKmRdxW4VirhM6tksIkVU7gYDWjs1m3E4gIBEC/AGJ3SbCfYZsiC+PtlFKCATr2YN9hOprdSL1420IBGS4zB+QRB9VZhOCgJQIIbDbBIGAjOnHH3pfQZtswTK9486i64deBySErJFo1zLqPUfbYAabEEk/49nE1NSUbfXq1Wvb29td69evH7/hhhuGZ9qmECnFTkrZAXQEr0eEEO8AjcD1wI5gtR8DzwKfD5b/XEo5BZwQQhwFtgGvWG28QpGPnOwb46WjfQDcsn2RYb2RSS+/29vBe1fXUl/uTtrmva+d4qymcva1DQFQVezkyvUN4fuHO0fYfXKAj2xt4qG9Z5j0BgD44OZGAH69p53zly3gZP84A2MebtjUyEN7zzA66QOgxO1gdNJHRZGTwXFvTN8LSlz0jXoAuHFLE3vbBjkzOMHVGxp4cE87vkDyQb2yyEljZSEH2of58NYm7t/VlrS+HheuqOaFI71ctraWQx0jtA1MpN2GES3VRZzsG2emjvZcvKCIC5ZXW9qmWQ/MakJzdn19ffYrrrhi+Te+8Y3ar3zlK7oZTaabtNxcIUQLsAl4DagLCiFSyg4hRG2wWiPwatRjbcGy+LbuAO4AWLTIeEBQKGYb73aNmqrXPTIFQGvfWFKxC3lYIaED6B+LFaSDHdq9KV8gLHQAQxPesKd0qHOE/jFP+F5I6KKv44UOCAsdwNiUjyPB9+fxBVIKHcDIlI8D7doX/AlvYoAnWkyNONaj9Xm6f1xX6M5uLgegZ2SKM4OTCffXNJTSP+aha3iKQpeNheWFHOvR9KB9YAIpYd3CMhz2iIfV2jvO0ISXBSUumioLOd0/EfP5mbEhVJaMMrczZZ3ZxoIFC/zf+c53Tt14443LP/e5z/UUFBTM+P4/02InhCgBHgD+Sko5nCREoncj4Y1KKe8B7gHYunXrjH8QCsV0Y9aTMBMOC1XR+29pDxaaDasl7cfih1wOG1XFqcUulenrFmqicrhzhDODk7gcNjy+iOivqCvleM+oJnZOOy3VxWGxC7GyrpRClz38emDMq4ldsYt1C8sZnfQlFbt4G5x2ES6bj1xwwQUTa9asmfj+979f+elPf7p/pu0xJXZCCCea0P1MSvnrYHGXEKIh6NU1ACFXtQ1ojnq8CThjlcEKRb6T7uxLqvrpCIyeKETPbYV4YHf6oUQryFRvQ0Lt9SdvwNzUl9D9zOO/KNhsoXIR81NhzPj4+JvRr3fu3Hl0pmyJJ+XWA6H9hn8AvCOlvDvq1kPAbcHr24DfRpXfJIQoEEIsAVYAr1tnskKR35gfz83VzNYhC4Uxoz27qSivJ1PMmiWja+o8ZEZCQkKtFwaNaSsDPTJ6H+EFLIKYn6lIy4bJIRjLmwWLcxoz++wuAD4GvFcI8Vbw39XAN4DLhRBHgMuDr5FSvg38EjgIPAZ8Wq3EVCiMSeUxpBPGTIbfxPxa6n7027Cb3LErMwuEEjLdl8KzS9+vNiYkdqEvC2ZFzLQFx3bCd7fD7/9P+sYp0sbMaswXMf79XWrwzD8C/5iFXQrFnMesx5aOZ5dsQLZE7AzKXQ4bEx59bzHa/lTvZUVdCS67ja7hSXqj5vFCgm8ktiFC7z++niAueBn9ORk0KcIeXaiyRUIaCMAz/wAv/AtUr4L3KLGbDlQGFYVihknlMZjxhszImNVL66MFxWnStdMzIfr9u+w2zm6uYENT7MKOkNil0utczKqF7LNkys7ngV9/QhO6TbfCHc9C42YLGlakQiWCVijynHQcMj1BC4mSmW0CmeJKInbRvabyzELELyEJmZ7aszNWpEzFKjJ3Z64Bw2oBP/zqj+HQ7+HSr2oenVr0Mm0oz06hmCFML/AwNWc3s7t3ovenJSNTK0Pvz4oZu3h9SeU5h0ROAEL6WXzmEYomOtKzQkp4+DOa0F35DbjwM0rophkldgrFDBHeG5eiXlqenUEfVhEz/xZVbrcl8ezSmLMzGv9D842pFuvYTGwTkFJ/84HRI0JoDy3e+SlufmwjF+z9POuPfi+pHQns+iHs/pHmzZ37yfSened85jOfWVhbW3vW6tWr1y5btmzd9773vapM2lFip1DMMNGDrMcXYO/pwXDWFCAtdygdDy+TdIzRXlB0V07Tnp3urF1iSVxR6ONIOWeXTLCyYfePqDjxcPilTCdc2nkAHvsiLLsU3ntXlobMT+68886uQ4cOHfzNb35z9LOf/eziqamptH+jSuwUihlCb+Df3z7I22eGae2LZPcwtfXAUstMEtVpsjm7mEey3FSei3BtxMPWHz+dwyfh8S8zVncOu1f/DVPOclw+cynh8Hu1BSmFlfCB70V2qs9RLrvssmXr1q1bs3z58nXf+ta3qgG++c1v1tx5551NoTrf+c53Ftx2223NAJ/73OcalixZsu78889fcd111y2566676pK1v2HDhim32x3o7e21J6unh1qgolDkEaF9ZNECl1YGFZNl6bar/3ykBZfD3CCuJ9xmvK6Qp5tpGDTTegC1z38JbHbaL/0OhwdKaOl4hOLxMyxte5DjjTckNBbz6rXvQfdB+OjPoGQazzH9zaeb6T5YZGmbtWvHueG7p5NV+dnPftZaV1fnHx0dFZs2bVp76623DnzsYx8bOPfcc1ejZdfiV7/6VdWXv/zljueff77od7/7XeX+/fsPer1esXHjxrWbNm0aT9b+iy++WLR48eLJxsZGX7J6esztrxkKRR4TGbiz31Qe0p10HJ+MnCSD+TfzYpf8fsi7iv9EwqsxU0h0/BFDsfdif5qhoeclik8/Bzu+gK9Ec048zjKqh/Zx7v67KB0/lfiQlLg8Q7gnu+HZ/wcrroDV15jvdBbzzW9+s27VqlVrt2zZsqazs9P59ttvuxcuXOhrbm6eevrpp4s7Ozvtx48fd19++eWjzz77bMlVV101WFJSIisrKwOXX375oFG7//Vf/1XX0tKyfseOHavvuuuujNJPKs9OoZhhUg2+ViRwzgUxC1RMKkggy+0PgRRZzozmIY1ClMkoGDrOll2fwlO2GNc5n0D0aCcZuKd6wnUcOuHMirf+k+tfu5sz9ZeCbwqu+ub0r7xM4YHlgt///velzz33XOmuXbsOlZaWBrZt27ZqYmLCBnDjjTcO3HfffZWrV6+evOqqqwZsNltaIek777yz6+tf/3rXj3/844pPfOITSy6//PL9RUVFaf0xKc9OoZgh9P6vRzwTmPL5aR+cyD6OaSFGe+ZSnceXNgbakHrrQWpRif/cQy9j9CjgZ+VTt2MjQO+5XwSHK3x/sHRVuJrTFxd1mxqlfM9/4PRPsLj999rG8aqlKW2aCwwODtrLy8v9paWlgTfffNO9d+/e4tC9W2+9deCxxx6rvP/++6tuueWWfoAdO3aMPv744+Xj4+NiaGjI9tRTT1Wk6uO2224b3LBhw9h3v/vdBenap8ROobAYqxZRvPBuL88d7ok5n86wT2TMz1h7LDEnoa3Q5QXLF1Bq8kw23Tm76OssvVzLHKh3fod7pJUDyz7B6LJrY269vu5veW7zdwAo8AzQ1PlU5IPZ/SPskwOarcIxr1KBfehDHxry+Xxi5cqVa7/0pS8tPPvss8OrrGpqavwrVqyYaG9vL7jkkkvGAS6++OLxK6+8cmjt2rXrrr766mVnnXXWWHl5eco8yl/72tc6vvvd79b7/emlXFZhTIVihggJk9H4PDypHaTqSxW7w/r9dJmwfWkVrx1PfmxZtklcUmZQSXpPmKoHwGvfY7J0MftXfJotcc/7HUUMF7cAsP7YPVSOHOb37/kN+Brg5X9jqm4Tzu59nGy6liWVi1P1NGcoLCyUzz///BGj+88880zCcT9f/epXO+++++4zIyMjtvPOO2/V3/zN33TF17n77rtj5uguvPDC8dbW1gPp2qfETqHIc9KKYuY8jBnpIORlpTMfZnb+0ajNlI+HHpOStce+T3vtxQyVrkioZhvp4MI9n+H19V9lylUZ8yhdb8Opl/FceBdS2KkpKUh43ufQInSVI4cBcPmG4fDDMNrJ4CXfYm+PZLRiFUtSvdF5zq233rr4yJEjhVNTU+Kmm27qe8973pN0NWY2KLFTKGaY6GwfemG4tFZYWmBP0vaz9swSy9I5FNXsas6qoQNsfPdfaeh9iae3/w9CxK7GLHnkk1R2vUxrw1WcbnhfbCNv/ADsBZSd98fcUqSfrMPrKI557fSNw55fQnkzU4t20OsbML3Rfj7zu9/97sR09aXm7BSKGcL8ET/mFWZac2TGdWVmaDedCDpDnQg9t+TUrwEYdzfoGOHHefplAGzxR21OjcK+X8D6D4GB0AH47IUABIJDaPnoUTj2jLYgxZb2fmerCAQCgXmrsMH3bhjzV2KnUMwwRqNTSBeSeTMjwXm96ZI4aXBtlhwevABouTFtfg9NZx5LuOfu2ce6o/dQ1fVauMzpHYmtdOhh8IzC5j9M3pGwMVS8lONNHwBgVetPtfJNt85kfucDPT095fNR8AKBgOjp6SkHDOfyUoYxhRA/BK4FuqWU64NlvwBC628rgEEp5UYhRAvwDnA4eO9VKeWdGb8DhWKeEisq+gpxrGeU1473c9na2rAw6tbMkcCE02ylMbRmuxozFQJo6H0Jl08Tseh9cEt+rW3s7pi8BmkvQPin2HbwHxgtaqaz5nwtnHrgAShvhubtCW3H/x4evvBBCjwDLG97gOLJTlh6CZQ3QX/Opp2S4vP5Pt7Z2fn9zs7O9cw/RyYAHPD5fB83qmBmzu5HwL8DPwkVSCk/GroWQvwLMBRV/5iUcmO6lioU85WUh7caiFX38BQAo5M+w7pmDn5Nh+gwZHzbZubezEZZM9U82/BpLt7zF3icZQwXteD0BVe/TwyG69S1PY5n/Uco2H8vAGcf+Tc6a86HiQE49jSc+ylzOSyFLbxQBYB1H8jQamvYsmVLN/D+GTUij0n5G5VSPg/oricW2l/3R4D7LLZLkSMCARk+LkUxs5ifszMq125EHypqtbglsyWT6cFcZ4MpfO7vAehecA4eVxn1/a9jC3ixHYmENW3Sh2dlZO/cYMkyAMSh30PAB+s/qNu23gpRvy1qpebqaxPuK/KHbF3dC4EuKWX03oolQog3hRDPCSEuzLJ9hcU8c7ibX7wx7ZmE5i3JFmRE9tkl7v+K3byt30boO0uqE7RzJYDxrZpboJJYpme+kAEael5MO9mno12bj3tr3RdZ2PMiAMtO/wr70cfD1XyOInyLLmS0cCEQyYIijjwOZY3QsNF8n9HGFy8wfD+KmSdbsbuZWK+uA1gkpdwEfAa4VwhRpvegEOIOIcQuIcSunp4evSqKHNAVDH0p8p/QMG803of3uUUPrtO5GDMDLy1lIuegZBa+/E9csuuT1Pa/Yb7xzn3YRs7w6oavM1FYj8dRAoAt4MN2fCcTdZsB6K27EBxuHrr4UborN1HgHcDm92irKVe+L221emXDP/Do+T+PvAeldnlJxmInhHAAHwR+ESqTUk5JKfuC17uBY8BKveellPdIKbdKKbfW1Ezj0RcKRY5JVwJSjY0+g7CzPxTGtEWHMXNL9mFMc/Vchx/SfsavlkzG0acAOFOjBZSe2v4jAGoHdiGmRhhcdxtj7gbaltyofebCxpSrkrr+XTR170R4x2Dlleb7C3Ki6XoGytel/ZxieslmU/llwCEpZVuoQAhRA/RLKf1CiKXACuB4ljYqFLMWKY3FzKxY+Pz6W4cic3aZWJYZyTwzU+fSmXnTAT/2fi2zlNvThy3gJSDsIFJ8Nz/2DP7adUwWVOMEhou1/CXNXTuRdhcjS67kmYL3UlXsYnnwkdFC7diejYe/jXS4ES3Zz7wovy4/SenZCSHuA14BVgkh2oQQtwdv3UTiwpSLgH1CiL3Ar4A7pZTJk+UpFIow0YIREjOvXydcSeS4m+jjdXK9p/ylo30JfaUTtdOds4uesxRA+57w66LJTq5/5jK2vf315A17xuH0awSWXBIuCthd+IX2fV42bQNXccJjh1tuBaBk4gwsOhdc1p53qsgfUnp2UsqbDcr/SKfsAeCB7M1SKOY+ZnXJb3BKtz88Zzd9qzGjSdh6YG6JSuoqx55GIhBIlrb9lkJPP8tPP8Dr6+4y9u5Ovgx+D4ElO2I2QoUypAQWvydiZ5SZU87yyAsLvLr49hX5w3zbeKhQ5B26+TCjRMFrcOqB2QUiufL2Ip5ddqN7wuNHn8LfsJmh4iUUTXWHi91TfRhy4jmwu5CLzottO3T0UcuFsZvXQycY2AvxBbcPiCUXZfweFPmPEjuFwoAD7UM8e7g7dcUkJNMZs2Ll8yffehDbpqkmZ4xU9tk8o9C+G1/LRUwWxJ7P6fYkmRE5/Ros3ATOQv1+G7fq2yAEHmc5Xnuh9nwS7CYnRzM5FV2Re5TYKRQG7Gsb4szg5Eybgd/Aswst9oj2AqdT63KxQbywezfIAP7mC5h0aWLXVaUJVYGB2Nn9k9o8X/P2BC9xzF3HpLMS7C7DPkeLGumsPg/syQ+gXVpdzNnN5UnrgApj5ivqiB+FYoYISUUqT8Br4NnNdCachE3lOm+jdKyViuF3Od1whe4z8RR1vAbCjm/hViYLtJMLTtdfTl3/LkPPrmrobQh4IS6ECfC7ix4GtD1SscZGLl/Y/K8EhIPmFLbZbIJ1C8vZe3ooRU1FPqI8O4Uih5gJVcaKhPZiwuMPi5zRSeV6Tev1lytJDM/ZJalz9Qsf5MK3PosIeGOeiab+1b/n2ue0VFtFHa9Dw9lQUMJwcQseRwntde8FjOfsagbe1C6at4e/OIRCjgF7AQF7cE7OwNApVyVeZ2mSd6GYCyixUyhmiGQ6+E5HZDO1gdbphhHzbc7OLjWRK5loB/TFuHr/f1M2fhKbfwp391uw+HwAjjZ/mIcufpQJdy0Sgcs7rNtHzcBbsGAFFC/A5bCxvrGMS1fXGdqU6zCjimLmJ0rsFIo8x0i/ZjqM+fqJ5Ftond5IuK9stFW3jghETmxY3PEoNv9U+HgdaXPgcVVgs9nxOopxRR3XE03l8EFo3BJ+fVZTBeVFsfNvmsBNkwwptctLlNgpFDOG/mbxhFoG7lq4eIa9OSP7q4YPha/dnl4g0dTS0RPh6/P2/612Eb8qUoDXURJzNl243ckeiqZ6YOFG0/Ymy2qjmLsosVMoZpiEDCIm0dtAPpNhzHjbK4feCV+HQpDx9lUOxR4s7XNXQnlTTFsCTezcnoGEPquGD2oX6ZxUkGPU1oP8RImdQmEh/WMe+kY94dfJ99mZazNb/crkdAIrqBo+yJi7Ab9w4vLqr2CsHHw75vVU9YYE1bQJQenYSRp7nqeu77XIDSlp7H4eiYD6DUltUfKjUGKnUFjIYwc6p62vkIZFS9l0pguLJzqTStXQ27R0PEp/+Vo8zrKwZ3eidyzmmcrBtxlr2M7J+vcBMFWtnR4Qk+1ERBa6LOx+Ply+uOMxVpz+pZYlpaDEhH2Rn7kUPxUizU+U2CkUOSSZU2ValtI7v3Ta0Qvbve9lLaXupLsGj7OcAj3PTgYoHznC5IJ1yGDOS2/5ksT2o5q3ByJec83AbgDG3PXZmK+YJyixUyjynHS8tfiaZwYnrDXGBLaAN5yT8mTjNTGeXTQl4+04/BNMVa2mt+JsACbrtyTUswnBw+/R8svX9b1Oc8fj3PLoBlae0o7SfGr7/5iya7ocLuXY5SdK7BSKGSaXocfDnaPTFtgMDfI3PnUBAPuXf5L+yk14nOUUTnazff9dMasvK0bfBWCqajXvLr6Z3+x4Ek/VKq2tuFjgUOlKdq/5POVjx9nyzj+Fy5/a9kPGiprSsjPX3q86qTw/UWKnUOSQ9iSelf7ZbubqGd2ficUo0WO70zuEw6+958HSFQB4nGWUj51gWduDLG+LnABWMXIEgKmqlSBsjBfqhyND4jFRUA2Ae6o3fK97wTkZ2qwEab6hxE6hyCGvHOuje1g/mbTpI3pS3B/zRDZmz+SWO/tET4zXNVSyDETsmXF+WyQhc/nIUUaLmpHOyKGq4UUkUe2GDhvwOrR6NrSUMjFn0aVgOsVNyWh+ohJBKxQ5ZspnkO8rSLbO2GvHI5lMhia82TWWBZUP/TF1HbsA2LXmCwyXLMWJ5tmFCCdzlgHq+l6nv2pjynZDC2C8jsiKy/6yNbyw6e607AsJnnLq5icpPTshxA+FEN1CiANRZV8TQrQLId4K/rs66t4XhRBHhRCHhRDvy5XhCsV8IR0xPBSVU3O6cXbsDl+/u/iW8HW0N1c1pG0CX9r2G9zeAYZKV6QUHxHn2QEcWP6nac/VxbSZ8ZMm2lZimpeYCWP+CLhSp/zbUsqNwX+PAAgh1gI3AeuCz/yHEMJulbEKxVxCT8PSCbeZ1cCcL8gI/4zqKOp9OP3j4euq4XdYMLiPBUP7ATi69Fbd1uIzqECsZzdYsiJtG5UGzW9ShjGllM8LIVpMtnc98HMp5RRwQghxFNgGvJK5iQrFzCKlzMmcT76dUJAVY5FFI8carw9fCyF4Z8kfErA5GS1s5Px9X6Kp6xnKxlrpqdjIVMGClOm1Qp99tGc3VtSYsam5F38lq/lINgtU/kwIsS8Y5qwMljUCp6PqtAXLEhBC3CGE2CWE2NXT05OFGQpFbjEaHMejFobMezr3AfDKhr/n9fVfDRcLwOss58DyO2ltvI7BkmWsav1f6vp3MVzcQlNloW5zevlCfXZN7Lz2YqQKGCnSJFOx+09gGbAR6AD+JViuu3JarwEp5T1Syq1Syq01NTUZmqHIlJnKlzgb0fukjveM8ps3z9AzMpV9+7P9VyGADk3s2msvQdqcxlVlAEdA+8xKGlezraXKVPOgHfnz9Dn38NCOR7O1OLfzasqxy0syEjspZZeU0i+lDAD/jRaqBM2Tiz7dvgk4k52JCsXMovfFICRy2ax+nK48ltPST9cB/KWNeFyx2wHiRcUZdUyPKKnFZhNJ62tlkcKu6vOYclUmVjJBdG5MxfwjI7ETQjREvfwAEFqp+RBwkxCiQAixBFgBvJ6diYpcMOu9iTlEpmKUV7/DvmP4qpanrGYPenVHmz7E+Eptbk9f4CLXtlkmTkpM85OUC1SEEPcBO4BqIUQb8FVghxBiI1qEpxX4UwAp5dtCiF8CBwEf8GkppT8nlisU00TONMVkBpVZwcAJ/Cven1AcP/A/v/nfWNr2a15ffxfnOfTn61K1kQnaSQdaQ1LmdhHJrP0dznHMrMa8Waf4B0nq/yPwj9kYpcg9+eQU5Du58qDy4Xewqr6EruEpBsczD8faJodhYoBA+eKUdXuqNtNTtRmI3rIQQU8o1OpGhRWodGGKGWfS6+fe107RGnfWWb6QqzmvfAhDFjjsVBQaLygxg32oFQB/ZUvCPSuEaraFBVXezfxEid08JZ9WYw4HF3kc7R5NUXNmsOKjeqdjmHtfO5Xyc8+f34p57IOtAPjLW9J6zqwmxItHJloS3YbSovmJEjvFjDMbB/h02Xt6EIBA9AkFwXee8+8dqdrPcvAPe3YViWHMTIVFL4OK0euM2syh4CktzU+U2CkUKbBCjPQGV712p9vjtmLQtw+2QnENuErT61svNZiOQcoTU1iBErt5ynzwpvKRaDGbK78D2+BJqFxiaZsxGVTIPowZTe4Pb81t+4rMUGKnUKQg2wUqLxzp1S3X8+JmowDaB1tBZ3FKKnT315mo509+YtKMo1aP5idK7OYpebQ+JUyuvxG/0drPka70j8DJ9Wd1sm+cl47qC6IVpJ6yy/yDt/k92EbaoWpJyuwnySzI/K5CYQ4ldop5w5GuUd5oHUj7OSu0LryhWafdoQkvJ/u0Y3Cm+0tItl5I8US7drSPQRgz0+wnMRppsdqpMOP8RIndPGW68jLOBXK2aGQO/ApKx4OHnFQlil1taQGbFmWWxzKaZIK8ZXEll6+tM9fONImcEtP8JGUGFYVivmOlJkkJPn+A3ScHmPLpTT5Zr4DJtDrbgbkkJHaVSxIkacviSvwmvijobQkQBvfjWVWf3gpQM20q5iZK7OYp+Thnl69Y+VkNT3rZ+U43vsDc+AWUjLchnUWIkloY8+SkD6t0KTR/qP725ycqjKmYcebT4LPzUHKhy8VnkSpknY2XUzLRhr98MQihm+kkWdNz9feuvMb8RImdQpEKCwflQJ55dNkOzEWTXciyxiTtG3cQCKpd7FoUvU3l1quH2h4w/1Bip8gb8vUb8XQu5slFTznxoGSAiuHDFE12EShZqFsllaAEkhiWy1yW+fp3psgtSuwUihRYKRb55ddpGI79UlLTv0f3A1hx6hdc/dKNuD39BMoW6raT6kuC2c/Csjk7i9pRzE6U2M1T5sJ8SWvvGFO+3J8NbMVHNW1Jn3X7NkYgWLnz45y778sJ95a0/5bLX7uN5s4nYp8J+Fhz4seR9kv1PTut/SR2JVslGn2tVEphAUrsFDNOJmHCkUkvLx/r4+WjfTmwyHrMitxMiGFl206Wtj+UUF4xcgSAkon2SKGUrDr5s5iyQFDs9DOoGPert39Rv741ahdqW0oloPORlGInhPihEKJbCHEgquyfhRCHhBD7hBAPCiEqguUtQogJIcRbwX//lUPbFfOYQHCL2rgn955dsrklq8nFBvakbQaMPz+b1M4ZDAhth9KmQ9/iI09up3L4HbxR83SBJJ5dUrtM1lPCpLACM57dj4Ar48qeBNZLKc8C3gW+GHXvmJRyY/DfndaYqbAalUFFAeCc6I68iBNFe0DbN7fu2PcBWHPixzj8EzR2P48vWuxKGjLqO/kClajrjFo3Jtfi6bRpw+rahWW57UiRFinFTkr5PNAfV/aElNIXfPkq0JQD2xQ5ZC7M2U0X+fBZZTNAJ7PfMXomcu0fj7lX4BkEwO0d4Ly9XwiXu3wj+Ivr2HnO9zja9EFEgXEWk2QrMs1+rrnZepA7bDbBLdsXsW5heQ57UaSLFXN2fwI8GvV6iRDiTSHEc0KIC40eEkLcIYTYJYTY1dPTY4EZitmOFXufAgFJv8WZPGb71oNkOEcjc2/uKe07rcszxOZ3vknJ+KnwvSVnHo55zl/SQGf1+by+4e/CZbq/vyS/UtNbD4ybSAu1t25+k5XYCSG+DPiAnwWLOoBFUspNwGeAe4UQur68lPIeKeVWKeXWmpqabMxQKMK8eXqQxw50MjThta5RCxQo2yZyNUw7R9rC126Ptthn1cmfsbr1p1QGF6hEM1C6AgB/sbnky8moKSkwVU/N2SmsIGOxE0LcBlwL/IEMzoBLKaeklH3B693AMWClFYYqrCUPInM5IeTVWbklYbo+KymlYWgvV2FM51Br+Nrt6aNwopMNR/8zXNZWuyOmvtehhSwDxfVRxsX9jMLI7svW1lJb5k5idXTz1q/GVMw/MhI7IcSVwOeB90spx6PKa4QQ9uD1UmAFcNwKQxUKPdRCmyyQkqJTOxmtPgsA91QfK07fH1NlMOjJhR8JCk+gqDqrrkOLOOJJd/tCpuRiHlCR35jZenAf8AqwSgjRJoS4Hfh3oBR4Mm6LwUXAPiHEXuBXwJ1Syn7dhhUzSs7OaJthcvG+pmvrgZTG4p2Od+PwjlI00Zm6nm8M51gng4uuAGBxx2MUT3TE1BkoXc3BJX8Ufj1cshSAQKG5c+qMrJ5JrVE6Nz9JecSPlPJmneIfGNR9AHggW6MU84t81N1o0bQid3PW7zGNAfqso/9BU9dOHtrxmNa3gYAWenoBmCrWEjnX9b+RUGfCXctbDZ+lZLyN0aIm9q/4FB01F7Ck9mwY6o01MUciosRJYQXqPDuFAvAHJHZbZFSNFqdpPanAaM4ujSZKxk9TMtGOze8hYHcZCm3hpLYK2lsYu0Csq2orCwb34whMMe6uBeDFzd8O32+ru5Toc8mNxEjLVJKeUpmpL0RmXx6EwbVifqDShc1T8tCZsmQECr2v+LBfsvDmid4xfvHGaYYn9VdwTpfnaVU37iltVWV936u4PEPG9YKrL31xYlc8cYaTDVcBMOkyPzeXKwGxKddOYQHKs1PMe073a2ushsa9lLmdQKzw+KczXZhBuebxaHd3vHEnQgbYs+av2bHrUzx+/s+ZLIiIUkjEduz+NACta84ktAdQOKWFIeM9u+e2/BsjxUvYu/IvCNhdKW1OJkWGc3Yz4Vup1ZjzGuXZzVPy6T/8dJiS7vuNnbNLfDg3584l2WQddb2w9yUa+l5h3bHvUzzZRUPPi9GNhEUsXOSf0m3TPdWLtDnxF5QzESWWQ6UrCdicTLqz2//qsFsjaPGenfLzFJmgPDvFvCAbcZrOObtUolw98Fb42ukbAyBgc+L0DlE9uI/eio3hnJYhbOO9QGRPW+FkF+ft+zKlY6fwF1UjbDYeO//nrD3+P3RXbs7Y9tB8mxBw8coaSt1Oxj0+g8ppFVuKiorOT5TYzVfyyLNLl56RKZ482AWYD4cl9Zp0moiuPV1aZ6abK179WPi6sed5QMthed6+r9DU/azuM/aJforH/Zy/94scXHo764/+FwuGDwIwWXs2ABPuOnav/YLu8+nisAkWVhQC1oQrc7PPzvo2FfmNEjvFrONYz2jKOtkOZjGrMeOE8qG9ZxidNPBYssRom0DV4D4KBxLTdwEUeAYoGW/TvQdgn+hj3bEHqBncy8V7/iLmnr+oNiM7Y04lCHl0GbVk3G64TK9ORqsxlcLNZ5TYzVPmSuYRs+8jm3frj3PtciZ0SYy85AW97a4axZMdTDljM+wfXnwzHmcFG47+J/bRDqTQn57PVOymE6u9sEy2RChmP2qBiiJvMDv8ZDJMmVmgYlRnuhbzPLT3DB5fIGW9t5fezv7lnwy/Xtr+EHUDuwE4U30BAPtW/Dn7l9+Jx1FC0YnHMZJ7f1Fuk7AbaUp6WqOESZE9yrNTzCmsEqZoj3G6th54fAG6hvVXTkYzUVDDu4tvYcK1AHtgii2H/hmA07WX8MLmf8XpG8Hr1BI291WcRUPrE9SULNdtSxO79MUkF0fwGPcV95rM4pihdpRTNz9Rnt08JZ+2HsTz1ulBjpuYl0uHbMK205Ub0wghY09wmCxYAEJwdPFH6a04O1w+VVAFQuB1Rk7VOtSiLWgpHz0WLmur3RH2AK0c+a1oSm9eTWmTwgqUZ6fIOw6eGQZgaU2JZW2mv88u+nr6xE4vFZYreGJ4iClXJAnzcElL5LqohXjG3dpRPCJK7I833kDXgm1sPPz/UbDierDuNCTTpCNgan5NYQXKs5un5LFjZwnJhsfB8dh9aKlW6flTT6NZhp6uuj0DMa+jF6N4neXcf9lL7Fn1Wd5dfEvCs5OuqoSygbLVeJ2lvLH+bwm4zZ1eYAZdr8wSby9VgUKRGiV2ihknmed0oH0oQZxStJayxs5D3Wm0N/NhzIKg2L2+7m851ngDwyXLYu57nWUcWvpHuqm9PK5yAsH/5qfqL+edlj9krHBhTJ1MBCkmqXKS5/Nxn10+h/AVuUOFMRV5SyAg2dc2xMEzw3zknOas2ooNS6Zvx0xS4NGOhOyp3MjRRR9J61kp7HhcFbg9/ZypvpDjzR/IhYk5Re2PU1iB8uzmKbmehzraPcrAmDmPLJUpVnhWZhaoRNeJ3VSeuv2ekSlae8cyMS0l7qDYTemEJM0QCmVOFixIuGel15TOKeNZzcNl+eegpgDnJ8qzm6fk2ld5/YQ2QN+yfZHpZ2ZiIYKUklPBUw+MMCO2ofRlVtBcVcikN4DnzH6WnX6QgM1JQDh159/MEBa7DJ/XIyaDSo49LyVOCitI6dkJIX4ohOgWQhyIKqsSQjwphDgS/FkZde+LQoijQojDQoj35cpwxdwnG0GOf9ZIr1493h++7h2domt4MoteraGpsojL19Zx0e6/ZPXJn1I9uJfJwlowyIKSiqmCkGeXKHYix1JlfMRPDhpVKJJg5n/Pj4Ar48q+ADwtpVwBPB18jRBiLXATsC74zH8IIeyWWatQZIiRcJ6ICj0e7hzl6Xe6g/WjQpq5NCwJodMLqoYOMlHYkHE7IY8u0zDoTJPg2akFJooMSCl2Usrngf644uuBHwevfwzcEFX+cynllJTyBHAU2GaNqQoryeWUnVULOkLhw3RaM075Zb6VoXEvvSPprAC1ltDYHspn6QhMMllUn3F7bbU7ONr8Ifx2d+rKFmNFaNrq8LZajTk/yXTOrk5K2QEgpewQQoSyyTYCr0bVawuWJSCEuAO4A2DRIvPzOor8Jz69VvvgBDUlBbgc+t+tjAaf+ATMIfQGv1N94zRXFaZs0wwP7++Is28GRsdAgOLJzvDLycLMxa6r+jy6qs+zwipdcp2GS0UtFVZg9WpMvb9L3ZFCSnmPlHKrlHJrTU1uk9EqEsnlqQfRIjUy6eW5wz28erwv7XZCGpNqsDveM8qLR3t5tyuSYiz+/WX7bofGvYxMerNsJQ1Ovhjz0u8ozkk3QmTmOZmd6TOcs0ujS6tFVC14mZ9kKnZdQogGgODP0C7dNiB6Q1QTcCZz8xSzkWhHKCR8o1Opj8URwFhUPbOCPO7R8l1NeI3zXmXjnEk0b+93ezt0vDzrvzQIAfSfAOBQy60A+J1FlveTK6wOOxoJ64o669LJKeY+mYYxHwJuA74R/PnbqPJ7hRB3AwuBFcDr2RqpmF2kuy8uWtR++1bku1GoGbOtpTvEzkh40gQCAcPtSAR7V/4lw0UtDCz7CEzMtGXJ0fs4LUkXptNGOltaFAowIXZCiPuAHUC1EKIN+CqayP1SCHE7cAr4MICU8m0hxC+Bg4AP+LSUcgbSzCpSksNxPjRnZzMY6IYmvBS77DjsWmDBcFGJyf7C4U4R9YwJB8y01iXJvpIzvRxqJ1Bci9/u5ujij1JlcwEzt2gmnmxFLJ0NDyrqqLCClGInpTQ6IvlSg/r/CPxjNkYpZjeh1Zi2qBEx7KVJycP7Omgod3PJ6uSnZBt5XvEDbcgzTDaAzrqT2YfbCJRG1nbN1nkmS0Kas/S9K/ILlS5snpJs6B/3+LLaPhB+VGeQCulXx1DqzduZeHbpPGvesZvePXdCAMNnCJQuTFnXir7yXUtUbkyFFSixU8Tg8QX4zZtn2HVyIHVlAwIy0bNLhpGAmA0RmhI23TCmuQ6mfWpPSi2MWRbl2U2zCemQ0akJM7gaUzE/UWI3T0m1t619MHm+yGREwpg6/abRjnkxCoYxkwyKVulVvE250EHbZD94x/BHeXa5yhuaabvTqT9K6xRWoMROYTn+JJ6dnoBluyoyEjVNnCNM2m9WveYOV8duAHx1GzN63p7m/2qrFps4gt9uaksLsmswvn3l2iksQJ16ME/J5YKNgM4cmhni65udNoxZjZnGyk7TYdLo1ZgZtpEOxYcfAGcx/oZNMDyU9vM2IfDPgJQ77Dau2dBAcYFKh6vIP5TYKXTJZhAPhTHNfiO34rQ6gDdPDUaVGLcqTdTRqz8diICPwuOPw5bbwOEG0hc7u03g9efYaoNfbXmR0/KujLawKBTpoMROYTmBJPvs0hmCw4mgJTx2oBNfIABAXVlsQmMzwqy/QCUNY5I888s3TqffEODwjXPu/q+wZ/VfM16ozc+VjZ1A+KegcWtGbYImdmZROqKYL6g5u3lKLlcYxoT9TGzCNmNL/5iH4QkfwxOJacfMbTPIzRuWSHwZbtOoGdjDos4nOXff34bLSsdOahe1qzOeqyp2Te932FxPqamtBworUGKnsITdJ/t58M02IFZ80g0ZRmM27ZhVXptx+7kRShFMLlQ5cihcVuAd1C6KqjNud1V9qXkbRJrZTCzSnbTaUVqnsAAVxpynpBq+0x3fD3eOpq5EmqJnegGJmTim3nPpm5FwmkIWOujyDgNQEPwJUOAZ1C6KqjLKDuawCWxzbJLLKGOOQpEOSuwUMVgxkESLjzmvSD/dl9GTZg6uztXZBFY6ea4okXN5BrEHpnB5h5COQoSzEDwZqF26K2DTfECgfZbRz2UirTOZGzNP838rcowSu3mKkQhZPRCYWapvfLq4yT4M6j39TheLFxSxvFY/rDfTHkK02N349IUADBctJuCuYLoW72vn2aVXf7o/tvi5SzWHp8gENWensByrxsKdh7pTV0JftKSUdA1P8fqJgeBrneeyFNN02tAjWuxClI2fJOCuBGI9mnTmDeeaFCR68tn9hak96vMTJXbzlFSbr7MZTpKtxswJJvrwZ2VI5s+KgBdXaB4uDrenD58tMduIt+7szPvLeX3tiekUDCVOCitQYqfIGVKa+xaeCz2M17boUxyklLzR2k/3yFRGbcXcS2H9WUf+nRufvpCCqf6Ee0WTnfSVrw+/9tq108g9y680ZVe+kMkWibRCp3POV1XMBErsFDFYs8zeOL9WLjw9M9vc/NFiBxzpGuWVY30pnxMitn2z9i9p+w23PLqBdcd/CMDq1p8k1Cme6GC8cCGPXPAr7r/sRVoXXg3A1KKLwn1H7MjR9oBcu4IWoDw7hRUoscshbQPjHOsxtyR/upm2TeVR5W+dHmR40ht+fbp/3BJbzHiPMRu/0+hPpGjfyPbtB74W87qpa2fMa7t/ksKpHsYK6xksW4XXWc7utV/igfc+i3AW6fSTH0sIle4oZisZi50QYpUQ4q2of8NCiL8SQnxNCNEeVX61lQbPJp5/t5fXjieGr/IBowE8PGdn8eA64fFz8MxwzKKTF470WtK2mcUn0RvUzW5WBx3PzuQ+u/6ydTGvy8dO0NT5NA7fGAVT/Vz62u3YpJ/eyo0Ru2xOpgoWmLbNCjINEQqD60yeT1nXIoWtKnKxqKqIc5dM72esyA8y3nogpTwMbAQQQtiBduBB4I+Bb0spv2WFgYrckO1yfyP6xzwc7Y54s6bShYX22VnoNsSLUnwY0ywCkVT4jYTT5ygMX7+99HbWHf8BF735Vwn1uqrO0et02kj/ZIrp33tg1ZydzSZ4z4rMM9MoZjdWhTEvBY5JKU9a1J4ijznZN8bAmP6G58cOdDIwHglV5mIvW+JRQIl9xM/jxYhdemoX034yjzFSKUDpWGv4ZUf1+bpNP3Hu/+K3F+re07qOOp/PnLUJz5mqH1f9rKbyJG1PP2rOTmEFVondTcB9Ua//TAixTwjxQyFEpd4DQog7hBC7hBC7enp6LDJDYRbDwdPEqPrS0T4ePdBpqo9MTyRIB73H40XIn2GyZgEED1vQRa/ZFad+SfFkFwA7z/keI0WLdJ8dLWoy6HP6Rne9ntY3Gotd+LlpUKBQF0rrFFaQtdgJIVzA+4H7g0X/CSxDC3F2AP+i95yU8h4p5VYp5daamppszVCkiWEGFYs9sWkJeJmYs8tmn12yOT69z7Fq6ED4urP6fCbctbrPTrqsnztKV4TSr2+uLJ1+G8r1vdtQik91UrnCCqxIF3YVsEdK2QUQ+gkghPhv4PcW9KFQGKInRvEilKlnZxMiRijjW9FrdsJdB8DhxTdrBcLGntV/zUDpSrzOMtYc/x/txAODQTxcnOEYP9u0odBlp66sgK7h2H2PWxZXsefkgDq8VWEJVojdzUSFMIUQDVLKjuDLDwAHdJ9SzAhChDZ76xMa1+Pv+wOSM4MTNFclLotPhplVndmGMfUEJ9mcXTo0tT/KuLOcjqpzAW2+MrafxHYd/nECwsHuNV8Ilx1aclv4+qVNkbVbxQV2xqb8pu0pdTsYmUw80y9T0t5mN43Cs7y2hOW1JdPXoWJOk1UYUwhRBFwO/Dqq+J+EEPuFEPuAS4D/k00fityQrsC8eWqAF4700j0ymbN+Mh1H9ReoxJYFMhE7Kdm+56+55LVPhIv2nh6Kr5KA0zvKpKsKRPL/XrdsX4TTnlgn2edQ6LSzos46AUj7PDudupmEGbPZ2pInWw4Vs4ysPDsp5TiwIK7sY1lZpMgZQ+PeqIEi+T67+NtjHs378PiSrNZIQrLxKdt5Qr2BM17svBmIXfSKSqTUdWvixd/un6Rk/DQeZ1na/c0msnXwbLMt1hpFS3URrb3jM22GIk1UBpV5xMP7O8LXmX47Tve56fgWrqdj8f1mItIVo0fD126Pfmqx+BDkBW99jrqB3fgc6YV7o0nmKVmhETdsWhhpL8ttCplwy/ZFWR0wO9M6ef6yaj68VX8lrSJ/UWKniEHPS/L4AoxmO0+ULJlymoIYLW7DEz6DBSqxr73+9MWudCyybfSDOy/h4l2fCr8unOzigjc/i9MbG9Zs6n5W69/kf61k43ZGqxxTtJmWAXlKLr5AlbrV0Z5zHfUbnqcYLlDRKXviYCfDE+mLnZQyHKK0cktD/GCnZ1u0AI5N+RibSt/+kvHTSC2HCgCNPS+Ew5lL2n/H4s4nGHfX8+aaz4Wf6S9bQ9XwO7y08Ztp9xci1/oT781ltnXAImPygJvOaZ5pExTTgPLs5inpfDvOROji+7Hy27iZxQ3RYvfbt87g9advQOFUH8PlqzlVd1m4rGiyg6KJDhp6XwJgTetPcHqHqe95Wct96Z/gZP37GC9caNRsVlgtMpmlC5tb2Gwi7bDq3PsU5j7Ks1PEkEpH0p6zy9wUQ8ysNclop4GUlI8eYah0JQAu7yAeVzkvbrqb2v43uOz126kZeIsL9n4+5rEPP3VB+NrjKGVqgW7SIF2SbdLOZEBtrCyclSNxXZmbruEpigrsM22KYo6iPLt5ivGpB8nDjumGI0NemJWiZ27+Lf0eV566j2te/BA1/bsBKPAO4XVVgBD0Vm4CSBC6eFy+ESZzeHKBlphau167MHbF58q6Era1VKVuI/qcvLT7zw3rFpZx/caFlLmdOepBMd9RYjdPma7VmGm1aXIkbRuYSFknHc+ubPQ4V7/wAbYe/H8AVA4fAsDlGcTr0ry0gM3JgWWfiHlusGQ5fWVrE9qLPn08E/RWSOp97sWuWC/I7bSnH47LMF2YQFBTWpDWs6nsKC5QgSZF7lB/XfOUyFyajB3wUohEOmfBRTeX7LFcnIxgtIlcBHxIW+yffX3vKzHbDCqHD4EM4PIO4ymoCJfvW/HneBxlbD78Lzx+3r0Mlq6gYuRdGruf4+1lH2f5qftZ2v4QPUEv0FqiT16w9kikTNvZsaqGCa/57C8KxUyixG6eIpG09o7x8rE+rju7gdK48JHheXfp9pPOAxZqnp7W2f2TfPjJczlVfwUvn/2NcIaTkom2mHoNvS+xpP132AjgLYgKCwrBoSW30Vb3XkaLtZMM+irOoq/iLAAOL/lDDi/5wzQt1Zu0SyxKx/tNLyNKeoTalkicdptuBhgjLlxRTWWxK80e85O5uFBnrqPCmPOYk/1aFojBmPPnkpOOeGl1p/mgT+mndOykrre49eD/xSb9tHQ8Sn3vq9j8U9h946xu/SkAnVXbaK+5mKKpHs7b/xUAxkvijucRIix06ZLuPup8HFCzManQZadEhSoVM4QSO4u497VT7D09ONNmmEbKzBYbvH6i33wfyGk5zy6aVSf+l+uev5bB428m3FvW9mD4+r27/pSrXrqRbW9/HYDDi/+Andt/wLGmG2KemYgXuywwmyJLr5pBUDYjO2IWqKSRG/O6sxsi9qj8lIpZhhI7C3n7zPBMm5AWuoNqcBALSE3AM82FGd2W6foZ96Sx9PSv2XxYOz5x3fHvUzJ2GodP815FIHGvYPlYK0vOPAzA28s+DsBA2Zrw/f6yNUyUWLjh2IKz4MxUT6dNs0JX6LJR6naGayutU8w2VExhHhMa6F482kt9mZtLVtcmhP8Gxj0p2znVp58UN9lRQjH1TNQxw7kHvhq+XtzxGIs7HqOzahs7t/+AAs8AAK+v/QrSZuesd/+NQk/ESw0dpDpW1Mjj5/2MgdJVBOwFVNucQOrPwAx6spKsLPqemS8OBQ6zKcpmb15KK7lsbS0Fjsz29c2hj2HeoDw7C8jo+JgZJnrwlBI6hvSP7jGTZmtf+6CpfnJJ2ejx8LVfRBbb1Pe/jpB+Cqd6AJh0V3Os+UYevPQ5TtdeAsBvL34kZhTvqziLgF1bVm/lvJleGNNsaDPVitVtSyozOvst07eXyRE9+SYQtaVuygvVvr75gvLs5ikSqbtgIn4M29s2mEUf5rYVhDeeZyGMmzp+AcBo4UIOLvkTBsrXct7eL1I2fpLavjfCpxaMuSNpvF7cdDflo8cYKzIOVVo5QNt0vlrqlenhCP6yhBC6n+jy2tLMDUuLfJMshcIc80rsdrX243baWd9Ybmm76e49ywfMbi3wZZBTMtKHyQUqGfegUT5ylMaj9zLWdBEPbfhuuPzRC37J+5+7mkvf+AQB4WDSWcFg2cpIvzYHg2WrkrZtZdjO7MGnemVrGsp46aj+MUNp2yGSv9Yj9HsM1c3kd5aPq0sV84d5FcZ8t2uUfW1DqSumidVSt+fUAEe6RixtMz7sJMHcnq1pGKDSCYkVB3Mn2vxTXPP8dbS0/54Fg/sB6D/nszF1/Y4iDgX3vdmkj9bGa5EivTkaS8XO3Ja6yJxd8MIm0jvsNB2TMw9jZvacQjFTZOXZCSFagRHAD/iklFuFEFXAL4AWoBX4iJRyIDsz84NAQPLq8T7WLSynvCgS67f6P/6hDk3oVtRZF5rSs1E/LVUmczH6I6Y06DeEwy4MbTPsSwiQkpqBNykfa+X8fV9ktHAhAXsBk7Ub4VTsithDLR+jcvgwFSPv8vayO8x3FOrPwrCd3heHdE/sjq6dqVDFtKHCkhmhnNTZhxVhzEuklL1Rr78APC2l/IYQ4gvB18mz584SesemaO0bZ2TKx/vW1YfLc5HuymriLdTShOW+U6PPprjAHklZFq6e+nO0C0F93yu8940/DZeVTJxhsnYjwp7otUmbk5ezOFvOSi3QmyNNV+xyQVqeYPDnTP7N5///NkU+kosw5vXAj4PXPwZuyEEfM0Joz1n8Eu/ZENLRC2PqDXLx7yUTTy/Sh/Gz2kIL/YUpDt84VYMHdJ9bMPAmF+/+8/Dr/uC+uInG95CLxRNWtqjnRekJYCr9S/UrSSf0nPF5dhn8Wcy8rCvmM9l6dhJ4Qgghge9JKe8B6qSUHQBSyg4hRK3eg0KIO4A7ABYtsi5LRS4JiZ0rjXyA+UKmuyPMPGY0YAak8cAsiE1GTdTrs478G6tbf8rhxbcwWtjElKuSoskOeivO5pxdn8IeiOx7e3PV/2HF6fsJnH27uTeUJtZuPTDXfkgU9Re0ZG9HdJ9KgDJDLbaZfWQrdhdIKc8EBe1JIcQhsw8GhfEegK1bt86ob3SgfYilNcUUuZJ/HKHTrl2z0LOLR0qT/2GzeG9SGq9UtUkPBD9PMTXCB5+6mCPnf4u2ou0UTXQCsOrkvQnPeV3l2D1TAOxffiddC86lq/o8zi2uykl8y1LPTjeMae65fPkbUxlUFLOVrMROSnkm+LNbCPEgsA3oEkI0BL26BqDbAjtzxsCYh31tQ3QMTXL52rqkdcOeXZzYpdp6kHCMzgyQaKP+nF1CGDPLYc1n4FLuePI6Jp0VcM4LFIycwO0dZOXrX2b/jqdweY3TrvU0Xsbx8nNx+Cc43vSBcLkQAv0daNlh7WpMnTBmuufPkb1XFrvIJb1WNi+u5PUT/SzI4PQC5QwpZpKMxU4IUQzYpJQjwesrgK8DDwG3Ad8I/vytFYbmCn9wdPebiPN5A5rY2aMGqIExT8LG6+6RSQJRKSU1Lyp7W61ELxH0oE5qsGw9Cr19ek7vMMVjpyjmFJ7JcXp7tfVNBRNdQOKRO9F4iuo51XBlQrkgNyE5S1dj6pRlMmeXitAqV7Oks8+uqtjFlevrk1dWKPKQbDy7OuDB4DdDB3CvlPIxIcQbwC+FELcDp4APZ29m7ojfLJvOMwCPHuhMuP/Uwel3ZruHJ7HbBAtK9E+P1stxGf+eH9nfyQXLF8SUZesr+QKJiaTXHv9h+Lrnt1+hoGB1+HV978uUTJxh//I7GSlaxPb9X+Whix9hwl1Ly5mHCay+TtvsEocQFhirQ649u2SeVeyZumnsRUwRjo859cB0qwrF7CZjsZNSHgfO1invAy7NxqjpJDSI5PI//XTMbzz1jiawt2zXX+zzzKGemNfaaky9fXbJX+uR7LPz+SUuzxD1fa9wuu4ypM1B9eBeBivPYsRdT/M7P2Ci6UPh+qEtBWPuBlobr6O18brwvdbG61jsKgISE08LRE6851yLgalTDNK0wp4iNDrdIXW308akN/PTMxQKK5hX6cJ0CXt25geAfFkskA1SEjqoOwav35pBqanzaRb2vkDnls9x5csfpWSiHYDWhqupHD5Ed/OVHGy+meaOJ1je9gAAZ5Z9lIXHgjkuDfJVGv2acuXZ5ZrQPrsl1cWc6B1LWV8Ic15eTWkBE14/o5PJE3lr3edW/EoKHEx6PXgs+ttSKDJh9q2ht5jQsJHqv3v74ET4dIN0F21ks1ctVxjZ9EarBclupOT8fV9k+ekH2Ljz1rDQAbR0PILLN8pY2XKGSlfxxrZvA+CzF9LbqAUEppzldC84R7dpe7KwX/aW57RR/Tk7c9sL0v0TunxtHecvW5Cy3nR4eeuCuWhLC9QJA4qZY957dqFVisn+zw+MeXjucCQMaLh3zKCNXErd0Lg37QUJIXI1zLmnenH4J/DZCigZPqpbZ6xsOQCn669g+Jz/ZqR4EVU1CzlTfQH7l3/SsG2j1Yu5GrPTDSGWuB0pvamY9kN7tKP+qJL1KSCjaMRM0lhRaBheVyimiznr2Y17fIxMek3XTzZu+OPUbbr8tJFJL+Oe5APnw/s7+O1bZwzvG30OkswHSyH9ICMhqfhmSsdPA/DOkj8Klz1ywf28uv7rtNdcCMBE+TJ8AcmkN0BX9bmMFy4k4HDz7Dn/RV9lwlRwGKP0WkLkZs4uzZ0BlBc60xrYQ+9nuv6mGisLp6knhSK/mLOe3W/e1AQg1cAT2nGQ7Nt0fOjMKARo1EKmUczf7e0AUr+HaELiGNogH/ocrLIJ4JoXbqC/bA2+c+5lVOdw15Kg2LUuvIZlZx7Ca3MzWLqKwbLVnGq4grq+N/CXNMLQVKxNJvo2WnwxEz7O1pZKdsWFfZPZoff+QufZJd2rKTITw/gmP7K1CccszP6jUFjBnBW7aAbHPYYnEofm4ZJ5BYnbsY3rDY2b9yYzwecPMOH1U+rWfz9mRd4MLs8gBZ4BRkqWhMtKR09QNtZK2Vgrrx58l+PjRZS4Y/+MmrqfQSIYK2risQsfxGN3E5IBn6OY9rodNGQoT0aelhC5yeBvFDa9eFUNjRWFiWKnUz0kinpfkkI2R98y66Fa9W51jx7Ko6wtCoUVzPmveaf7x3lkf6fhSjdpYv4jIYmywSggpRZWTCi3MEj14tFefre3I+tFLxKZso0rX76J6154Py7PELV9b7D54De5ZFdkPs317sNaW1HtLBjcT3PX0wgkAZuTSXsxNpvOd6oMR2rDMKaJBpssDOElWygDmhcVIqSXup5d8J6eZxezzy78h2rexmz+7mbHbKBCYZ4579kNTWie1oCBxxWaj0v2nzvTJMohUulSICDx+AO4nakPFj0zOAloGV8yXZiiGWV8q3LoINWD+8KrKK966cMUT0ZEvL3mQkrG22g68xiHmmJzBjR1PQVAb/mGcJnNJhI+RD3Lp0zsxTKes0vtEW1tqeSclioefLM9ecXodg3KjYQkJLqx4cJkG8d1PDuTNqTLbFnQkgrlcSoyYc57dsLgm/Ok1x9TnjSMGfdsuuLnD8ik+9dePtbHr/eYH4DBOOdkKmz+KTYeupvCgcP6noZ/iqte/ijnHPzHcFm00HUsOI83t36T9sYrqenbhXtKS/VVONnN8lO/pLnzafrL1vDEeT8lNN7rTRPpiVb/WGKWl4TnDP5izQzjNiFwO9P7k0//CBzjMr1BOuL1mft95iJUOzckUKFIzpwXu9AAE4gSh6PdI/x6TztDE97InF2S//LxupLuN8tHD3Ry/y7jfI+n+seD7aYOLYYwk8tTj7r+Xaw98T+c9diHqDzya0ATuBUn76Nq8AAl4xE7u6rO4bV1d+FxlHJi4TW8sPFbPLPtHtyl1bQtvApBgOufuRz8Ps569ztse/vvKRs/SV/5OhA2Chyap2rlAaWGbZnoQvP+YitWFTu5YdPCZE/pliY7usi4LPEhm55np/MepcF1UrLwgOrL3Zk/nCcUF6SOlCjmD3M+jBny3KK1oTO4CrB/zBMOYyZbYh7vFaY7XxY6LSEVAZ3kzEZk6tmVjZ0AYLKkmSUvfIay8vUsGIoclDpYou1/e3bLd+mu2oLPUcyxRbGhSrtNMFi6jJHiRZSOnaKm+0Uael9h0llBa+O1HG3W6hc4bIx7/LorKK0+oduMx6NXp8BpT3q0k5Xnx0X/2YQ+ktDnkOrXWVKg2VhR5DTlAevaYVDu1HG937O8mjGPn4f3aV79bIwcXrW+gV/tNv6SqZhfzHnPLiQK0QIVOqLnlWN97Dk5mLINs6sxs0VK80sKMvXsSsda8ThKOXz+PwHECB1Axai2Cby/fC0+R7FuGzabQErJkxc9wIRrAee/9imKprrpL1/HnjWfZ7hkKQAFwZCh2SwhZjB6zEx7enWcRnHRFP0Zffr6h7EmPhOqphdm1+uzrszNFevqWF1fmsRaczbGE39kFWhzjtErmDM50mem0XtfivnLnPTs7n3tVPg6FKaM1ganzsKOZJP3gUC8Z5e9favqS9myuDK2Hwk2k0OUL8M8gy7vMJOuKkYr1oXLTjZcye41n8dnL2Tt8R/gcZQxWVBt2IZdaCfH+e2F7Nz231zz4gcBOLLopth6QSFJHtqzBjPt6dWpKEqewsqKRR3683haYWhrQ9JtdsG61XEnWljlHLtM7L17zwrjvweFYjYwJ8UuGn9Y7CKjiV7YJvkCldjXJ3rHWNNQSkVR+t92Qx7m4c4RHbEzf8hrJmHM2tICnL5RvI5iEIJ9H3yG1t5xRosje/L2rfyLlO3YBKFjExgqXcHzm77NlKuCnqqtifUw+Gwz9ewM99mJhN+Ty2GLCSHHf7YXr6phoUVzU6FEzrrCprOXLuTthheoRHt2SRa0JCvPtF60B3TNWQ26dfT+zyig0GVjwqMSXM8G5vxfsJ7Y6c0hJRsY9PZAdQ5PZmRPKo0yO0BlEsa02wVO3xg+RzFtAxMcmKiOETrT7dhEzOrBtvrLEoQOovei6YX2MnVLzGdQuWZDA5euqQ2H4+LrNFYUZuy5xc/b1pS6DO0Ii1fUZ1Ya3IgfFsKMrLCGaLEzSr4w07x/40Leu7p2ps2I4bI1tVy5Tv/LgSL/mPOeXUgToscmPfE60TvG+sYy3cwkSVM5pUkykdI8O3PtZOLZFdhtOH1jjBVm9x9Um7MjpXcWCtHpvad0c06mIr4Pl8NGoctOocvOpWtq6RvzGGZDMcPCCjd1ZW5ae8cYGPcmSQaeLDkBXLa2lr5RD0uqtflQvU3lVu2HMzsDHPLaCl2J330vW1sbXlU7k5QUOCjIszm42rLZv2J1PjHnxC5+fi2yGjNSbjRQnRmcZFW9ntgl1t3XNkT7wERSW5zeIewBb3j+S0qZdGWnlOl4doG0V4UWOG04fGNaGDMLzK6kDHnQevUz/fpgvPMg4m3abXDV+vrwPbfTTmNFdtlTXA4baxrK6B6Z0k1QkOzE+9DnIIHaUje1pZFBUm+lZjy5TAsW4qKV1VTqhOWjbVUoZjMZf1USQjQLIZ4RQrwjhHhbCPGXwfKvCSHahRBvBf9dbZ25qYk/oUBv64EZT61reJLfvtWO168vKj6/pGt4SufJCDt2/Rkf3HkJDp+2j84fSBS7aA8tHQ/SH0h/oUyBw47TN4bdXZbVHiQtjJlasMJzdjr34r+UmMVwvI660VRZRHGBNd/jIoFY7Sq0mCM+FJ7s3YTq6r1nM+nCCgw2wlu5wdzKzyxXqM3vimzI5q/bB3xWSrlHCFEK7BZCPBm8920p5beyNy89TveP8/aZ4Ziy6PRaIcyMs2+dHmRsys/guNdUfRHwIoUjPEI5vUPUDL4FwEee3E5n1TYC63+F314U81y8XWYFLJDGNoUQVW1P4fYOYPeNsaymhH1tQ2m2oGETxnZuWlTBm6cGg/WMw5jxX0qyJVeJi+Nt37K4kooiJw1xC1uSpa4MfQ56IexkXnKBw862JZU0lGfmlaq0WgpFhIw9Oyllh5RyT/B6BHgHaLTKsEx44Uiv4Ybb6HIzHlRkb5RMOfchAj4+9PTFbHnn/wGw/NQv+fBT7wHAL7SwaH3/64z8+KN0dnXGPBszAEr9eZaAzl7BQBrZVkLUvngXAIMV65OGtFJ5fWGPIq57h12wpqEs/NqeZM4u032CXr/+c1Z/628od7N5cUV4DjeU7i0UzoyfVwv93pKFMfX+7iKbyvXf1/La0ow9LqV1CkUES2Z8hRAtwCbgtWDRnwkh9gkhfiiEqDR45g4hxC4hxK6enh69KqZ5o7WfruFJ0xPY8SLh8gwiArHzMCKyhI5AipXFC3texOUbYdXJ+/jwE9vYdOju8L3nt3yHe6/cy/7ld7Kg+xVq7r8e96T2fnefHOD0wHjELqTut/GAlPgDkueP9Ea9h/RydNoCXuxjnby76KOcWPYHST2KrS1Vhve2LK40zDcaP7om6yNTr2PK59ctjxYfK4TP7bSzur6MwmBy7qkUWXAi7yfS+0Urq7lwRXXSUGWoeqq/sZi+MpSxuZIIWqHIhKzFTghRAjwA/JWUchj4T2AZsBHoAP5F7zkp5T1Syq1Syq01NTUZ9+/zBzjSNcrT73Sn9EgWdj9PXd9rMSJRNNHBjU9fyM2Pb6bkxOPh8tAA5Y9bVFI2epz6npe107qB6oE3uXjPn4fvO/0TOPzjvLDpbu69aj8dNe8BYWP/ik/z3OZ/pWL0GB985r2c/9bf0HqylT0nBxHSj3uyh95RD6+39ofbKh95l5r+XQQkdI9MxiyI0cKYxoNevKBXjRxGBHz0VmxEYjMUotUNpWFPUm/x4qr60vBpC6nENuy16AzkmXp2RqnXQlv/smVBibZII/TZhsQulDg8FdEfa1NlEc1VReFVoHpvOfr4nyvX13N2c3lmhs8DlFgrsiGrGWkhhBNN6H4mpfw1gJSyK+r+fwO/z8rCFHiiMonEpwdyT/XidZTgt2mZJ3bs/jQAB6ofpLGrjQl3DctOPxCu37DzL2DVZqhaGi7z+SPeVvF4O9e+cD0APRVn01txNmtafwLA4cV/wKGWjyGFjUlXFQF7bLYLgPa69/LyWf+X8/d9iZaOR2npeJRXNvwDDb0v09LxCM8Of5fO6vPYfuDrLO58HIdfEzdPvRff4uu0RqSktn8XgZrzkdI4dZS2sjMyum7v+AmyoJTOBdtxoH8SAcCK2hK6R7SFNwVO/Q2zDoMl/PHiG70KMZ5M5+wMxc6icXBVXSkvj/aFf+fu4HL8VPlNV9SWMDzhZf3CRLGyJ1lxGUkELakqdlFlYVqubM88VCjmEhmLndC+Zv0AeEdKeXdUeYOUMnQmzAeAA3rPW8Vk1Blo0d7KqhM/Ycuhfw6/HihdEb5e/8gHYto4VXcZe9Z8jute/jB8/zJ4/7/h8GvnsYVWUBaPn2bVyfvCz9QM7qVmcC+gnd325qrPELCnHqhaG6+jtfE6FvY8z3v2fJbz9n8lfC8kxiH8Nhf2gAfn7/+c4vM7qQispaH3ZTYdvpvBzsuRt/zMsJ/dpwbC1wsG9lJ6aifejR9j0l1DcZJMLUKIyPyU3c4EiYO8kVcYP7YmC9+lMxCXuB2MTvooLrBzdnMFrX2R0K/dpq1MFYiMDjgNcdmaWtoGJxLm1wocdlbVl7B4QfLtGg67je1LF+je00tiEELv1INUbF5UiV0IGisLOaup3DDlWUN5IS0LimI+rxAXrqgOn/U4W1B+nSIbsvHsLgA+BuwXQrwVLPsScLMQYiPaF/pW4E+z6CMpo51HGfvtl3ivZ5CJgmp6tn0eZAWN3c+y6VBs9LRy5Ag9C7ayZ+X/4dJdf4rDO8pQcQsDZWt4dcPfE7AX0PqBh1i285Pw81u4GBgpasZ35mqGNn6G65+L7KC498p9LOx5gYGKdXjsRfht7rRdizM1F3H/Fa+ypO0hbAEPrY3XsqjzCRZ1PM7C3pd4+pz/pmvBNtyefq4//HmqXriL6D0cFaeexPfIX9Js24rHVc5A6SqksFPf9yrF4+2cnLyKCXcda47/D5sO343PvQDftk9DqzawGp/4raW+6hyapGVBMS8f60uo4zBInnxJMMPFpkUVCJE872N0SK+5qpDu4SnDebGQpectXUBxgYOtLZXsah0I3tMCmNl6drVlbmrL3LQNRLaJhNiy2HgO0wzJ5i4N5z+TUFzg4Pzl2t7N9Y3GYU+7TXD+8mpa+04l3GuuKqLZdI8KxewnY7GTUr6I/petRzI3Jz0Ck8OUDb+LkAGqB/ay5DcPsy3q/hPn/i9j7npcvhEmXVU4SmsY8wR47YYXaB+YxOcsiWlvomwpfGInLzzyM9Ye/yHFk52U7v0elXu/F65zvPH9IARnai+i0GXDHxfmK3LZKS900jGUOp2YFHaON0e8zONNH+B44w2Ujp9kpGgxCMFkQTWHr76f8cM7KWx7ifq+V3h1w9+zaegpFu77Dy7kPt22Nx/+F4aLWygbawXg1NX/S33lImg9g0RGpfKKRQjNm9mxqpYug5RodoMT0uuCGSVCKzJbe8dC7zShbvycXfRgv6iqKHzGnx4r60rDYqf3F5jN/rNMPK1UmPLsrOtOoVDokN+7SFNQ1rKZ7jtf5amD3RSPt3H5nk8y7Kpj0lXFgeV3ho+amUDLpuG224EAHkcJPmfiW5cSpmwFnG54H6cb3gdAS/vv2X7w75myl/DEeT9lylURru922BPmtNxOG5esro05eSHl+yh0MDzh014IwUhxS8z9t9qGoXgrrNrKXv4SgJNLNlLV0ILryS9hk75w3fGCWt5e9nGKJ85QOn6a0cJG9q78SxZXb4hJSGzkbER7IUZ1jITSqC29hRkbmyvYdVJbjLN+YXnMl4OELDNhgxLbEXr1siDVVoDM2kx9T02v5YaVdSVJv2wo5g+zWuxAS2ekhbXgmcseZWjSZ1g3vIrQYCWgx+9PON+utfFa/Cuv4sywB789dnOv22kH4uc90v+PpZdR3mETSfNfSinxbP4Tfm27Erenj9UnfsLBpX+CJ0qMo1lMbDb9bPJEmh08nA6tnlfnOKLqkgI+sKkpUhD1VqMXr1QUObHbBKOZmZo2kXlG69pMtorQ6kNs5zKZfFTJttFYRXWJi+aqotQVFTPKrBc70DwsgPGo5eFupy1m8QpEVhEaDWSHO/WH1FHpxm9P3Nagt68vEw2JXqwRWnDhdtkZTSLcAQkyQDjU+dbqz6TsJ3p+yMjO6AHFaCA2K3ah07XHplIv24/2pKK/jFy9oYGOoQmePdxDRWHiAiBNSEKb7kNlpszTRaTp2a2sK8lqoUc2XzoU+cEV6+pTV1LMOPmVRjxD3MHcgaHsGg3lbj64uSmhXmiQHpsyFhE99BL/gn7OQj2BSHUUWLQHt6BY27LgTrFBfsLr5+2O9NJ9RR8nY7xAJfXga1bsil2J36VCB+fGbz2IHvTjv4w0lBdy87ZFMVtLQmcBbmyuAMwdQBqiqtgVPnUgHr3z5ZKxtaWKS9fUme5bkTlqn50iG+aEZ1cWdQZXWaEjvCowntAAP+6JeBqXrK7hSNcobSlOMNBDbw+anhA47Tb8SVJkhBZrCBHZK7iwopDeUf3UZwA9I1P0jCS3r7zQSUOFm0MdIwgR5fEkWY0Zbb7bqb9J32ifXUJbNoHTLlhWW8K7nSMEJGxoKmfPycEEr7ii0Bl+v2a8qlX1payq1/YZLq+NXWiUyror1xt/E082z5gNTrugxUBgAdY3lhneUygU2TNHPDs7O1ZpWVh8UbkTr9+4MKaeT0dwChx2LlpZQ6XOXqWlNZHBKXTWV01pZLN4c1Uh57TEZkOLHsTLCrXvEpPeADWlBYYisaxGG6ztQoTFzu2084FNsalGi1zpnVRw9YbYQT3U/9qFZUnCmJEbJQUOigvsCV5QOqdWf3hrM5sXVYZFZGl1CbdsX5TQxrqoJfRSSpoqC8NzrGYJ/W5S7YlLRigP5cq6khQ10+PDW5s5x2D+6JbtizirqcLS/uYy6ouBIhPmhNhBxCOKXgwR72XpJREOiVP8BPMt2xexeEGkrCyYEFhKicMuaCh36w6qodDmLdsXcc0G7ZDUFXUlXLamlhu3JIZWL11TG+7HbouInccXiLF/Y3OF4ebheEKejhAifI5bXZkbIQS3bF/E+sZyw1WX8fJy/cZGzlu2gOICOyXB07XjM9WYQe+07mgaKwr50BZN3JfWlHDRyho+sjW9nWDlhU5u2b6I+vLMz2BzOWzcsn0Ry2uNs9PMNqwW7plEfTFQZMqcCGNCZACO3r8V70nprQoMiV15VCi0OpgfMbqtujI3XcNTSOAjW5vDczoLSjRvotClpdaKPtVZCMHN25rD3pIQ2orQkgIHLrstnJYrdN9uEzRVFnKoY4TqUleM/RVFTkoKHOEji5KxbUkV25ZUhe2+ZfuihDrRXlNxgYPGCjeHO0cNF0xcvzHWy6wqdtI/5o15nYyVdaW8fWbYcEM6aF62nq2K7NjaUjUtqxIVinxm7nh2wbBY9FyLw24LeyNAzLlgLdVF4ToQCV+Vuh1cvrYueK0N4NuWVIY9uxAhgaoqdnHTOc3hDdXxU2Hxk+o3bm7iqvX1MSsjQ6JqtwlqS93cdE4ztaXusPA0VxWysKKQRQuK+Og51uS9KIk6NubaDQ1sWVzFTWm0fcXa+nCYuMBh430pVqSd3VzBTec0qz1PCoViRpg7np3BPNK1Gxp48/QghztHWN9YxobGcmxCE5ZtUd92QycmLKoqCgtUeaGTG7c04XLYODOoLWDRy35vswmW15TQ2jueIIp6dSE2U0fIuwyFM6O9q49sbYoRCLtNhHNFlhU6aFlQHHMI6zVnNSTtP0ToPRa6bBGb0hAiW1TItaW62NRKObXMXqFQzBRzRuxCA2lDhTuhfPOiCtYtLIsJMUJsKK/AYeeDmxsTVgmGBvSQ5xbaGhBPbZmbD25uNFzBGE99uZuOoUmKCxy4nfp9azYmll29vh6PXwuZ2kTsakSz/QN8eGtTVsl1nXYbH9rSmNayf4VCoZgJRD4cA7J161a5a9eurNuZ9Ppx2m05C5VNev04bEJXgDJh3OOjSGcvmkKhUJhBCLFbSrl1pu2YDcypkTYdryYf2ldCp1AoFNODij8pFAqFYs6jxE6hUCgUcx4ldgqFQqGY8yixUygUCsWcJ2diJ4S4UghxWAhxVAjxhVz1o1AoFApFKnIidkIIO/Bd4CpgLXCzEGJtLvpSKBQKhSIVufLstgFHpZTHpZQe4OfA9TnqS6FQKBSKpORqo1cjcDrqdRuwPbqCEOIO4I7gy1EhxOEs+qsGerN4Ptfku32gbLSKfLcx3+0DZWM6LJ5pA2YLuRI7vRQmMalapJT3APdY0pkQu/I5i0C+2wfKRqvIdxvz3T5QNipyQ67CmG1AdAr9JuBMjvpSKBQKhSIpuRK7N4AVQoglQggXcBPwUI76UigUCoUiKTkJY0opfUKIPwMeB+zAD6WUb+eiryCWhENzSL7bB8pGq8h3G/PdPlA2KnJAXpx6oFAoFApFLlEZVBQKhUIx51Fip1AoFIo5z6wWu3xJSSaE+KEQolsIcSCqrEoI8aQQ4kjwZ2XUvS8GbT4shHjfNNjXLIR4RgjxjhDibSHEX+ahjW4hxOtCiL1BG/8u32wM9mkXQrwphPh9PtoX7LdVCLFfCPGWEGJXvtkphKgQQvxKCHEo+Dd5Xp7Ztyr42YX+DQsh/iqfbFRkgJRyVv5DW/hyDFgKuIC9wNoZsuUiYDNwIKrsn4AvBK+/AHwzeL02aGsBsCT4Huw5tq8B2By8LgXeDdqRTzYKoCR47QReA87NJxuD/X4GuBf4fb79nqNsbAWq48ryxk7gx8DHg9cuoCKf7Iuz1Q50om3ezksb1T9z/2azZ5c3KcmklM8D/XHF16P9pyb484ao8p9LKaeklCeAo2jvJZf2dUgp9wSvR4B30LLc5JONUko5GnzpDP6T+WSjEKIJuAb4flRx3tiXgrywUwhRhvbl8AcAUkqPlHIwX+zT4VLgmJTyZB7bqDDBbBY7vZRkjTNkix51UsoO0MQGqA2Wz6jdQogWYBOa55RXNgZDhG8B3cCTUsp8s/H/A/4GCESV5ZN9ISTwhBBidzAtXz7ZuRToAf4nGA7+vhCiOI/si+cm4L7gdb7aqDDBbBa7lCnJ8pQZs1sIUQI8APyVlHI4WVWdspzbKKX0Syk3omXc2SaEWJ+k+rTaKIS4FuiWUu42+4hO2XT9fV4gpdyMdurIp4UQFyWpO912OtBC/v8ppdwEjKGFBI2Yyf8vLuD9wP2pquqUzYaxaF4xm8Uu31OSdQkhGgCCP7uD5TNitxDCiSZ0P5NS/jofbQwRDGs9C1yZRzZeALxfCNGKFjJ/rxDip3lkXxgp5Zngz27gQbSQWr7Y2Qa0Bb12gF+hiV++2BfNVcAeKWVX8HU+2qgwyWwWu3xPSfYQcFvw+jbgt1HlNwkhCoQQS4AVwOu5NEQIIdDmSN6RUt6dpzbWCCEqgteFwGXAoXyxUUr5RSllk5SyBe1vbaeU8tZ8sS+EEKJYCFEaugauAA7ki51Syk7gtBBiVbDoUuBgvtgXx81EQpghW/LNRoVZZnqFTDb/gKvRVhYeA748g3bcB3QAXrRvebcDC4CngSPBn1VR9b8ctPkwcNU02PcetLDKPuCt4L+r88zGs4A3gzYeAO4KlueNjVH97iCyGjOv7EObE9sb/Pd26P9FPtkJbAR2BX/XvwEq88m+YJ9FQB9QHlWWVzaqf+n9U+nCFAqFQjHnmc1hTIVCoVAoTKHETqFQKBRzHiV2CoVCoZjzKLFTKBQKxZxHiZ1CoVAo5jxK7BRzEiGEPy5zfdJTMYQQdwoh/tCCfluFENXZtqNQKKxFbT1QzEmEEKNSypIZ6LcV2Cql7J3uvhUKhTHKs1PMK4Ke1zeFdnbe60KI5cHyrwkh/jp4/RdCiINCiH1CiJ8Hy6qEEL8Jlr0qhDgrWL5ACPFEMKnx94jKkyiEuDXYx1tCiO8JIewz8JYVCgVK7BRzl8K4MOZHo+4NSym3Af+OdpJBPF8ANkkpzwLuDJb9HfBmsOxLwE+C5V8FXpRaUuOHgEUAQog1wEfRkjJvBPzAH1j5BhUKhXkcM22AQpEjJoIio8d9UT+/rXN/H/AzIcRv0NJZgZZy7UMAUsqdQY+uHO1stg8Gyx8WQgwE618KbAHe0FKTUkgkcbBCoZhmlNgp5iPS4DrENWgi9n7gb4UQ60h+jIteGwL4sZTyi9kYqlAorEGFMRXzkY9G/Xwl+oYQwgY0SymfQTuotQIoAZ4nGIYUQuwAeqV2JmB0+VVoSY1BSxR8oxCiNnivSgixOGfvSKFQJEV5doq5SmHw1PMQj0kpQ9sPCoQQr6F92bs57jk78NNgiFIA35ZSDgohvoZ2uvY+YJzIUS9/B9wnhNgDPAecApBSHhRCfAXtxHAb2okYnwZOWvw+FQqFCdTWA8W8Qm0NUCjmJyqMqVAoFIo5j/LsFAqFQjHnUZ6dQqFQKOY8SuwUCoVCMedRYqdQKBSKOY8SO4VCoVDMeZTYKRQKhWLO8/8D/IGGy1N2kD4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=.4, label='R')\n",
    "avg_rewards = plt.plot(R_avg,label='avg R')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0, 210)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99ff38a2a4ff5f958140c9ee6019db87",
     "grade": false,
     "grade_id": "cell-293ec5dfa636ff48",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Congratulations, you have now successfully implemented the DDQN algorithm. You are encouraged to explore different problems. There are a lot of different environments ready for you to implement your algorithms in. A few of these resources are:\n",
    "* [OpenAI gym](https://github.com/openai/gym)\n",
    "* [OpenAI Universe](https://github.com/openai/universe)\n",
    "* [DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)\n",
    "\n",
    "The model you implemented in this lab can be extended to solve harder problems. A good starting-point is to try to solve the Acrobot-problem, by loading the environment as \n",
    "\n",
    "**gym.make(\"Acrobot-v1\")**.\n",
    "\n",
    "The problem might require some modifications to how you decay $\\epsilon$, but otherwise, the code you have written within this lab should be sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fee8dd2891cacaa333ee01729bfe581",
     "grade": false,
     "grade_id": "cell-671cfb5a590863e9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 3.4 Atari games\n",
    "\n",
    "**(4 POE Optional)**\n",
    "\n",
    "A common benchmark for reinforcement learning algorithms is the old Atari games. Each timestep for the Atari games, the agent observes a screenshot as its current state.\n",
    "\n",
    "There is an issue with this definition of the agent state, what?\n",
    "\n",
    "Name at least two solutions to the problem, and why it wouldn't work well without these changes. \n",
    "\n",
    "Hint:\n",
    "- Imagine the game of pong. What is important for the algorithm to predict? What is the state of the agent? Is it possible to play the game optimally with this state formulation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28edaae5be0f298d97eff3247b7a5700",
     "grade": true,
     "grade_id": "cell-55e109dd6169612b",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "If each observation contains a single screenshot of the agents current state, it will not be possible for the agent to know what the expected state change in the screenshot will be. If one for example draws a parallel to the game of pong, it will not be possible for the agent to know the route of the ball from a screenshot since the direction, angle, acceleration and speed is not stated. Therefor an agent will not be able decide on what action to take if only a single screenshot is given.\n",
    "\n",
    "One solution might be to extract the sequential screenshots for the current timesteps as well as for the previous timesteps and use these in the algortihm by analysing the change between them. This information should be able to indicate the trajectory of the pong ball.\n",
    "\n",
    "Another solution might be to store the complete history of the sequence of screenshots, i.e. converting the problem into a non-markovian problem. The agent will be able to choose upcoming actions based on its previous history. This might not be the most effective way to adress the problem, however it should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dml]",
   "language": "python",
   "name": "conda-env-dml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
