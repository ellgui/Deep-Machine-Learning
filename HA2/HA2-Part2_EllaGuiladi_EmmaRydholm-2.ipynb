{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fa9f96d4ca0144b2db877078cf7b2f8",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = \"HA2-Part2_EllaGuiladi_EmmaRydholm.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"Ella Guiladi\" \n",
    "NAME2 = \"Emma Rydholm\"\n",
    "GROUP = \"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a5517d7993b4b35049f0013dd6a3f55",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','7'), \"You are not running Python 3.7. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d199303c73ec86d25177caf39e385f",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nb_dirname = os.path.abspath('')\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in ['IHA1', 'IHA2', 'HA1', 'HA2', 'HA3'], \\\n",
    "    '[ERROR] The notebook appears to have been moved from its original directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78c7227b049bb147e6c363affb6dae8",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>if(\"HA2-Part2_EllaGuiladi_EmmaRydholm.ipynb\" != IPython.notebook.notebook_name) { alert(\"You have filled in nb_fname = \\\"HA2-Part2_EllaGuiladi_EmmaRydholm.ipynb\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    display(HTML(r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(nb_fname=nb_fname)))\n",
    "except NameError:\n",
    "    assert False, 'Make sure to fill in the nb_fname variable above!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb700528d4644601c1a8c91ef1d84635",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching current notebook against the following URL:\n",
      "http://raw.githubusercontent.com/JulianoLagana/deep-machine-learning/master/home-assignments/HA2/HA2-Part2.ipynb\n",
      "[SUCCESS] No major notebook mismatch found when comparing to latest GitHub version. (There might be minor updates, but even that is the case, submitting your work based on this notebook version would be acceptable.)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5285d7fc7207e47d532f63ff1fb7a339",
     "grade": false,
     "grade_id": "cell-3c556a39514d3d6c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HA2:  Part 2 - Transformers and self-attention\n",
    "$$\n",
    "\\renewcommand{\\vec}[1]{#1}\n",
    "\\def\\x{\\vec{x}}\n",
    "\\def\\y{\\vec{y}}\n",
    "\\def\\dim{d}\n",
    "\\def\\w{W}\n",
    "\\def\\wu{Z}\n",
    "\\def\\R{\\mathbb{R}}\n",
    "\\def\\linMap{W}\n",
    "% Query, key and val\n",
    "\\def\\q{\\vec{q}}\n",
    "\\def\\k{\\vec{k}}\n",
    "\\def\\v{\\vec{v}}\n",
    "\\def\\Wq{\\linMap_Q}\n",
    "\\def\\Wk{\\linMap_K}\n",
    "\\def\\Wv{\\linMap_V}\n",
    "$$\n",
    "*You should have completed part 1 before starting with this one*\n",
    "\n",
    "In this part we will take a closer look at the transformer architecture and the self-attention operation.\n",
    "We will start with basic self-attention and gradually construct an actual self-attention module.\n",
    "Finally we will construct a complete transformer and test it on an actual problem.\n",
    "\n",
    "The focus is on a conceptual understanding of the transformer but you will have to implement a few key elements of a transformer. Along the way we will try to give some best practices for constructing a more complex network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e561aaa14e57e43514a3169d5142c9cb",
     "grade": false,
     "grade_id": "cell-1531dbb9354dac88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's start with importing the module's we are going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "809176a963532ded6fab7314251a8556",
     "grade": false,
     "grade_id": "cell-a4bff0f2271fff88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as torch_nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic self-attention\n",
    "\n",
    "The key-stone of the transformer architecture, self-attention is a sequence-sequence operation which transforms a sequence of input vectors $\\x_1, \\dots \\x_t$ to output vectors $\\y_1, \\dots \\y_t$.\n",
    "Remember that all vectors have the same dimension $\\dim$, i.e. $\\x_i, \\y_i \\in \\R^{\\dim}, \\forall i = 1, \\dots t$.\n",
    "\n",
    "## Weighted average\n",
    "The actual transformation is a simple weighted average\n",
    "$$\n",
    "\\y_i = \\sum_{j} \\x_j \\w_{ji}.\n",
    "$$\n",
    "\n",
    "In an actual transformer, weighted averages are computed often and for long sequences. Therefore, the implementation must be fast in order for training to be even possible.\n",
    "With high-level frameworks such as `pytorch`, the key to fast code is often to reduce loops and instead express computations as matrix operations.\n",
    "\n",
    "**(2 POE)** Complete the function snippet below to implement simple weight sharing.\n",
    "\n",
    "To pass this part of the assignment your implementation only has to be correct, not efficient, but to get the first POE, you must implement it with just a single for loop. For the second POE, do it without any loops at all.\n",
    "\n",
    "*Hint*: Take a look at how `torch.bmm` is used later in the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f9fc770ac37f0d03e737a5eefa8705c",
     "grade": true,
     "grade_id": "cell-f846c494c826a310",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_avg(x, weights):\n",
    "    \"\"\"Weighted average\n",
    "    Calculates a weighted average of a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        weights (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        y (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        \n",
    "    \"\"\"\n",
    "    #weights = F.softmax(weights, dim=2)\n",
    "    y = torch.bmm(x,weights)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e9dc459927bdd7a1c2ca05c946e71aa",
     "grade": false,
     "grade_id": "cell-c2fb9d37d6959f02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure to test your implementation with the unit tests below.\n",
    "The tests cover:\n",
    "\n",
    "1. Dimensionality\n",
    "2. Uniform weights $\\w_{ji} = \\frac{1}{t}$ should produce $y_i: y_i = \\frac{1}{\\dim} \\sum_{j} x_j,\\, \\forall i = 1, \\dots t$\n",
    " (i.e., every $y_i$ is an average of the input sequence).\n",
    "3. A specific numerical example with batch size = 2, $t = 2,\\, \\dim=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "854fbde40a06070cdb6d40423c9f49a1",
     "grade": false,
     "grade_id": "cell-1a58d8ec5834578f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_weighted_avg(function):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        function: Implementation to test\n",
    "    \"\"\"\n",
    "    # Testing dimension of averaged tensor.\n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    x = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = torch.rand(batch_size, seq_len, seq_len)\n",
    "    y = function(x, weights)\n",
    "    assert y.shape == (batch_size, dim, seq_len), \"Dimension error: expected y to have shape {}, got {}.\".format(\n",
    "        (batch_size, seq_len, dim), tuple(y.shape))\n",
    "    \n",
    "    # Testing uniform weights preserve x.\n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    x = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = torch.ones((batch_size, seq_len, seq_len)).float() / seq_len\n",
    "    y = function(x, weights)\n",
    "    assert all(torch.allclose(y_b.mean(1), y_b[:, 0]) for y_b in y),\\\n",
    "        \"Numerical error: With uniform weights, expected y_i = y_j forall i, j (within each batch).\"\n",
    "    assert all(torch.allclose(y_b.mean(1), x_b.mean(1)) for (x_b, y_b) in zip(x, y)),\\\n",
    "        \"Numerical error: With uniform weights, expected y_i = sum_j x_j, for all i\"\n",
    "    \n",
    "    # Actual numerical example.\n",
    "    x = torch.tensor([4, 1]).reshape((1, 1, 2)).float()\n",
    "    unnorm_weights = torch.arange(1, 5).reshape((1, 2, 2)).float()\n",
    "    scale = unnorm_weights.sum(1).reshape((1, 1, 2))\n",
    "    weights = unnorm_weights / scale\n",
    "\n",
    "    y = function(x, weights)\n",
    "    y_true = torch.tensor([7/4, 2]).reshape(1, 1, 2).float()\n",
    "    assert torch.allclose(y, y_true), \"Numerical error, expected: {}, got {}\".format(y_true, y)\n",
    "    \n",
    "    print(\"Test passed.\")\n",
    "\n",
    "test_weighted_avg(function=weighted_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9af181fb1e50970e08d65e6da4f51c1f",
     "grade": false,
     "grade_id": "cell-06c2f57520370ab9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Defining weights through the dot product\n",
    "A simple way to define $\\w_{ji}$ is with the dot product\n",
    "\n",
    "$$\n",
    "\\wu_{ji} = \\x_j^T \\x_i.\n",
    "$$\n",
    "which maps the pair of input vectors to a non-negative scalar, $\\R^{\\dim \\times \\dim} \\to [0, \\infty)$.\n",
    "We then use a softmax to obtain normalised $\\w_{ji} \\in (0, 1]$:\n",
    "\n",
    "$$\n",
    "\\w_{ji} = \\frac{ e^{\\wu_{ji}} }{ \\sum_j e^{\\wu_{ji}} }.\n",
    "$$\n",
    "\n",
    "**(1 POE)**\n",
    "What is the difference between these weights and the weights in ordinary networks, e.g. a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c1421d9efd882dc949ef5293a4cf9c4",
     "grade": true,
     "grade_id": "cell-c9cb3ec23bbe5c66",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "These weigths are not learnable, but instead calculated only using the input vector x. Also, these weights sums to 1, because we apply the softmax function, which is not the case for weights in a ordinary network. Lastly, these weights depend on each other in order to give words with similar word embeddings larger weights, which weights in ordinary networks does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b5c80f2d75dcacf639bb3102a94456e",
     "grade": false,
     "grade_id": "cell-39d421f43d0c02c4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** \n",
    "The dot product is essential for calculating the weights. As we progress, we will make slight modifications to the inputs but we will still base it around a function which calculates a softmax-normalized dot product.\n",
    "Therefore, you need to complete the implementation below:\n",
    "\n",
    "Again, this function will be evaluated often and for long sequences in the transformer block. For POE's, implement it without using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0d24b818f400320a93a31e22555ed14",
     "grade": true,
     "grade_id": "cell-2a548cd9fdb03d8f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalized_dot_product(v_1, v_2):\n",
    "    \"\"\"Normalized dot products between all pairs of vectors in a sequence\n",
    "    Takes two batches of sequences of vectors as input.\n",
    "    Sequences in the batch are processed independently.\n",
    "    The normalization is done with a softmax function along the columns of the weight matrices.\n",
    "    \n",
    "    Args:\n",
    "        v_1 (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        v_2 (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        norm_dot_prod (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    #v_1_transpose (torch.Tensor): Shape (batch_size, seq_len, dim)\n",
    "    v_1_transpose = torch.transpose(v_1, 1, 2)\n",
    "    \n",
    "    dot_prod = torch.bmm(v_1_transpose, v_2)\n",
    "    \n",
    "    \n",
    "    norm_dot_prod = F.softmax(dot_prod, dim=1)\n",
    "    \n",
    "    \n",
    "    return norm_dot_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "609f29304963a4e99b92f240233ba272",
     "grade": false,
     "grade_id": "cell-9a0498b4a61bc45e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure to test your implementation with the unit tests below.\n",
    "The tests cover:\n",
    "\n",
    "1. Dimensionality\n",
    "2. Normalized in the correct dimension\n",
    "3. A specific numerical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab2cff4a1da3ac7edec034bce652b746",
     "grade": false,
     "grade_id": "cell-17aa1999bb0519d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_normalized_dot_product(function):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        function: Implementation to test\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    v_1 = torch.rand(batch_size, dim, seq_len)\n",
    "    v_2 = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = function(v_1, v_2)\n",
    "    \n",
    "    # Testing dimension of weights.\n",
    "    assert weights.shape == (batch_size, seq_len, seq_len),\\\n",
    "    \"Dimension error: expected weights to have shape {}, got {}.\".format(\n",
    "        (batch_size, seq_len, seq_len), tuple(weights.shape))\n",
    "    \n",
    "    # Testing weights non-negative\n",
    "    # (Boolean tensor's can be reduced to a single boolean)\n",
    "    assert not (weights < 0.0).any() ,\\\n",
    "    \"Value error: expected weights to be non-negative.\"\n",
    "    \n",
    "    # Testing weights smaller than one\n",
    "    assert (weights < 1.0).all() ,\\\n",
    "    \"Value error: expected weights to be non-negative.\"\n",
    "    \n",
    "    assert torch.allclose(weights.sum(1), torch.ones((batch_size, seq_len))),\\\n",
    "        \"ValueError: expected columns (dim 1) to sum to 1.0\"\n",
    "    \n",
    "    # Actual numerical example\n",
    "    v_1 = torch.tensor([[1, 2], [-1, 1]]).float().reshape((1, 2, 2))\n",
    "    v_2 = torch.tensor([[1, 0], [1, -1]]).float().reshape((1, 2, 2))\n",
    "    e = np.exp(1)\n",
    "    true_weights = torch.tensor([\n",
    "        [1 / (e**3 + 1), e**2 / (e**2 + 1)],\n",
    "        [e**3 / (e**3 + 1), 1 / (e**2 + 1)]\n",
    "    ]).reshape((1, 2, 2))\n",
    "    weights =  function(v_1, v_2)\n",
    "    assert torch.allclose(true_weights, weights),\\\n",
    "    \"Numerical error: expected {}, got {}.\".format(true_weights, weights)\n",
    "    \n",
    "    print(\"Test passed.\")   \n",
    "    \n",
    "test_normalized_dot_product(function=normalized_dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bea90e7cfd9acf3023bd00744e551f9a",
     "grade": false,
     "grade_id": "cell-746bafddcbfd9aba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "That's it, we have now the building blocks needed for basic self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d52434db03615e9fe1e3c6ff3070a472",
     "grade": false,
     "grade_id": "cell-4bb35eab7404cf63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def basic_self_attention(x):\n",
    "    \"\"\"Basic self-attention\n",
    "    Transforms a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        y (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \"\"\"\n",
    "    weights = normalized_dot_product(x, x)\n",
    "    return weighted_avg(x, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb6a1b8ddffbfb578599f99b30ef658d",
     "grade": false,
     "grade_id": "cell-648a50a17d0269e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 2. A self-attention module\n",
    "Like you saw in the video lectures, self-attention is rarely used in the basic form we have created above.\n",
    "Let's do the modifications needed to construct an actual transformer.\n",
    "\n",
    "We will wrap it in a proper `torch.nn` module to create a building block that we can use in a network.\n",
    "Creating your own module is actually not that common, frameworks like `pytorch` are built to be *modular* and we can often create very specific networks by combining standard modules. That is a good thing, since it enables us to express interesting models in a high-level interface and as a bonus, we build a model from well-tested and efficient parts.\n",
    "With that said, you might find yourself in a situation (perhaps already in the project) where no off-the-shelf module suits your need and you have to create one yourself. View this latter part as an example/inspiration of how to construct a non-trivial custom module.\n",
    "\n",
    "## Queries, keys and values\n",
    "The self-attention is extended with three linear mappings $\\Wq, \\Wk, \\Wv \\in \\R^{\\dim \\times \\dim}$ .\n",
    "These give us learnable parameters and make self-attention more flexible.\n",
    "The three matrices map the input $\\x_i$ into a query, key and value respectively:\n",
    "\n",
    "\\begin{align}\n",
    "    \\q_i = \\Wq \\x_i \\\\\n",
    "    \\k_i = \\Wk \\x_i \\\\\n",
    "    \\v_i = \\Wv \\x_i\n",
    "\\end{align}\n",
    "\n",
    "First, we modify the self-attention by redefining the unnormalized weights (while reusing the notation):\n",
    "\n",
    "\\begin{align}\n",
    "    \\wu_{ji} = \\q_j^T \\k_i \\Big{/} \\sqrt{\\dim}\n",
    "\\end{align}\n",
    "The normalized weights are still obtained by applying the softmax function.\n",
    "\n",
    "**(1 POE)** Explain why we scale the dot product with the factor $1 / \\sqrt{\\dim}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bf491e786332986bb598f8e10a442c2",
     "grade": true,
     "grade_id": "cell-d351e9c8c79ee5df",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "The self-attention is defined as inner product of Query and Key divided by square root of the dimension. The longer the sentence, the more words there are, resulting in a larger number of an inner product. Dividing with that factor, i.e. the square root of the dimension, will act as a variance balance that will lead to better learning since it will stabilize gradients during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d765a71cc1bfa1048bc4d2c664d1b402",
     "grade": false,
     "grade_id": "cell-eeac94c3ecfcc5e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, the weighted average modified and is now based on the values $\\v_j$, instead of on $\\x_j$ directly:\n",
    "$$\n",
    "\\y_i = \\sum_{j} \\v_j \\w_{ji}.\n",
    "$$\n",
    "\n",
    "We can reuse our dot product calculation by simple *wrapping* it in a function that takes queries and keys as the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a73b126acb49c060f096918bdb48cf5",
     "grade": false,
     "grade_id": "cell-e1459474d3e61f00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def query_key_weights(queries, keys):\n",
    "    \"\"\"Weights from query-key dot product.\n",
    "    Softmax-normalised dot product weights\n",
    "    Calculates weights for a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        queries (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        keys (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        weights (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    dim = queries.shape[2]\n",
    "    queries = queries / (dim ** (1/4))\n",
    "    keys    = keys / (dim ** (1/4))\n",
    "    return normalized_dot_product(queries, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c6a63e908bd4a4bb842cd976e42cf63",
     "grade": false,
     "grade_id": "cell-4473215afd0827d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Multi-head self-attention\n",
    "\n",
    "The model should be able to find different patterns in the input sequence, which is why we use multiple heads.\n",
    "\n",
    "Now, we'll create the actual self-attention function, which includes multiple heads.\n",
    "For implementation simplicity and efficiency we will do a version called *narrow* self-attention, where the input vector is split into parts and each attention head is applied to just one part of the vector.\n",
    "Imagine that we have $\\d = 64$ and four heads, then each head would operate on a vector with dimension $64 / 4 = 16$.\n",
    "\n",
    "## Constructing the module\n",
    "Below is an implementation of our self-attention module. We try to show you how a typical custom model looks like. Part of that is to do full vectorization (i.e. no loops). The result is a lot of manipulation of shapes and dimension order of intermediate tensors. It is not very readable and it is quite difficult to wrap your head around it but since you are likely to use and modify other peoples code (in the project or some later time), it is good that you get exposed to it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ace34fbf2dc8bbb3b664159fe0b5cd2",
     "grade": false,
     "grade_id": "cell-535e46dd571b9284",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(torch_nn.Module):\n",
    "    def __init__(self, dim, heads):\n",
    "        \"\"\"(Narrow) Self-attention module\n",
    "\n",
    "        Args:\n",
    "            dim (int): The full embedding dimension of the input vectors\n",
    "            heads (int): The number of heads in the multi-head attention.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not dim % heads == 0:\n",
    "            raise ValueError(\n",
    "                \"The embedding dim. must be divisible by the number of heads for the vectorization to work.\"\n",
    "            )\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        part_dim = dim // heads\n",
    "        # Linear maps for q, k and v\n",
    "        self.Wq = torch_nn.Linear(part_dim, part_dim, bias=False)\n",
    "        self.Wk = torch_nn.Linear(part_dim, part_dim, bias=False)\n",
    "        self.Wv = torch_nn.Linear(part_dim, part_dim, bias=False)\n",
    "        # Linear mapping to return to the original \n",
    "        self.WO = torch_nn.Linear(heads * part_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Multi-headed self attention\n",
    "\n",
    "        Each head operates on a part of the embedding, i.e. we have q, k and v with shape\n",
    "        (batch_size, seq_length, heads, dim / heads)\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input with shape (batch_size, seq_length, dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, dim = x.shape\n",
    "        part_dim = dim // self.heads\n",
    "        x = x.reshape(batch_size, seq_length, self.heads, part_dim)\n",
    "        \n",
    "        keys = self.Wk(x)\n",
    "        queries = self.Wq(x)\n",
    "        values = self.Wv(x)\n",
    "        \n",
    "        keys = self._restructure_tensor(keys, batch_size, seq_length, part_dim)\n",
    "        queries = self._restructure_tensor(queries, batch_size, seq_length, part_dim)\n",
    "        values = self._restructure_tensor(values, batch_size, seq_length, part_dim)\n",
    "\n",
    "        weights = query_key_weights(queries, keys)\n",
    "\n",
    "        y_tilde = weighted_avg(values, weights)\n",
    "        y_tilde = (\n",
    "            y_tilde.transpose(2, 1)\n",
    "            .reshape(batch_size, self.heads, seq_length, part_dim)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .reshape(batch_size, seq_length, part_dim * self.heads)\n",
    "        )\n",
    "        return self.WO(y_tilde)\n",
    "\n",
    "    def _restructure_tensor(self, x, batch_size, seq_length, part_dim):\n",
    "        \"\"\" Reshaping q, k and v tensors\n",
    "\n",
    "        For efficient vectorisation we stack the different heads in the batch_size dimension.\n",
    "        Think of it as temporarily expanding the batch_size with every head.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .reshape(batch_size * self.heads, seq_length, part_dim)\n",
    "            .transpose(2, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1a1cebeb15a37888ce9cf8c421a9270",
     "grade": false,
     "grade_id": "cell-c6938740fc6cf9db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# The transformer block\n",
    "\n",
    "The majority of the implementation complexity is actually in the `SelfAttention` module. The transformer block is rather straight forward, it is just like the one described in the video lectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e517abaace6db4a5af44c710024bf601",
     "grade": false,
     "grade_id": "cell-9b67db79adeb75d3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(torch_nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = SelfAttention(dim, heads)\n",
    "\n",
    "        self.normalization_1 = torch_nn.LayerNorm(dim)\n",
    "        self.normalization_2 = torch_nn.LayerNorm(dim)\n",
    "        \n",
    "        # The size of the hidden layer is a hyper-parameter,\n",
    "        # but the consensus is that it should at least be larger than the input/output size\n",
    "        self.feed_forward = torch_nn.Sequential(\n",
    "            torch_nn.Linear(dim, 4 * dim),\n",
    "            torch_nn.ReLU(),\n",
    "            torch_nn.Linear(4 * dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.self_attention(x)\n",
    "        # Note how the residual (skip) connections are implemented as simple addition.\n",
    "        x = self.normalization_1(x + y)\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        return self.normalization_2(fed_forward + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9800f2211ff3b5b80a0e2bae1fa04b4",
     "grade": false,
     "grade_id": "cell-a68000a248c4d4f7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now, we are done with the general module. To create an actual transformer yet we must choose an actual problem so that we can specify input, embedding and output.\n",
    "Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e16a26217bacc6bf649499dcd9bd0b4",
     "grade": false,
     "grade_id": "cell-5fcb1a036ccf47fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 3. IMDB Classification\n",
    "\n",
    "Transformers are often very complex models. Whenever you see impressive transformer results they are likely produced with a transformer with many millions, if not billions, of parameters. We don't really have that computational budget for a part of a home assignment. Instead, we will show a classifying task that is reasonable but still not a toy example: classification of IMDB reviews. Even this small example takes a considerable time to train.\n",
    "\n",
    "The purpose is to build on the computer labs and to give you some inspiration for how to solve a general problem with `pytorch`. It will show you how to install additional python libraries (useful for the project) and some advice on how to construct a training/validation loop. We do not expect you to modify the code, **you don't even have to run it** if you feel that your cloud credits are starting to run low. However, you should read and understand the code, it will help you answer the questions at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e91acbf40690b9e9414cfcffe4e897d5",
     "grade": false,
     "grade_id": "cell-782a05ee9fec69cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The data\n",
    "\n",
    "The IMDB data is provided by an external python module called `torchtext`.\n",
    "You can add it to the dml conda environment with:\n",
    "```\n",
    "conda install -c pytorch torchtext\n",
    "```\n",
    "Make sure that you have activated the dml environment before your run it.\n",
    "\n",
    "Processing text data can be tedious and error prone. For prototyping it is nice to use some third-party library which has done most of the work for you. You do not really need to focus on the data processing here, since it will be different for every task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e2478c2155979c606309d025969e754",
     "grade": false,
     "grade_id": "cell-5554838bcc3faced",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is one the entire family will enjoy... even those who consider themselves too old for fairy tales. shelley duvall outdid herself with this unique, imaginative take on nearly all of the popular fairy tales of childhood. the scripts offer new twists on the age-old fables we grew up on and they feature a handful of stars in each episode. \"cinderella\" is no exception to duvall's standard and in my opinion it's one of the top five of the series, highlighted by jennifer beals (remember her from \"flashdance\"--and she's still in hollywood today making a movie here and there) in the title role, jean stapleton as the fairy godmother with a southern accent and eve arden as the embodiment of wicked stepmotherhood. edie mcclurg (\"ferris bueller's day off\") and jane alden make for a hilarious duo as the stepsisters. matthew broderick is an affable prince henry. you'll all keep coming back for this one!\n",
      "\n",
      "Label:  pos\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "def get_loaders(vocabulary_len, batch_size, device, split_ratio=0.8):\n",
    "    \"\"\"Load the IMDB data\"\"\"\n",
    "    tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    train, test = tdata.split(split_ratio)\n",
    "\n",
    "    TEXT.build_vocab(\n",
    "        # We have to leave space for two special tokens.\n",
    "        train, max_size=vocabulary_len - 2\n",
    "    )\n",
    "    LABEL.build_vocab(train)\n",
    "\n",
    "    train_loader, test_loader = data.BucketIterator.splits(\n",
    "        (train, test), batch_size=batch_size, device=device\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def view_example_text(index):\n",
    "    \"\"\"Helper function to look at a sample.\n",
    "    \n",
    "    The dataset is quite slow to load. \n",
    "    \"\"\"\n",
    "    train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    sample_text = train[index].text\n",
    "    sample_label = train[index].label\n",
    "    # Simply print the list of words, separated by space.\n",
    "    print(\" \".join(sample_text))\n",
    "    print(\"\\nLabel: \", sample_label)\n",
    "\n",
    "view_example_text(118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23e7fe517ddbd840d59b0863cd1c75ca",
     "grade": false,
     "grade_id": "cell-d1ffc03b003a1689",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The transformer\n",
    "We will create a simple transformer that takes as input text in the form of a python list of words and which outputs a  probability vector over the two classes \"pos\" and \"neg\" (technically, the output will be the input to a log-softmax).\n",
    "We make the simplest (and less memory efficient) version of position embedding as described in the video lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1595f37785d61071e5731927850a415",
     "grade": false,
     "grade_id": "cell-ff83666a724761f0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(torch_nn.Module):\n",
    "    def __init__(self, dim, heads, depth, seq_length, num_tokens, num_classes, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        self.pos_emb = torch_nn.Embedding(seq_length, dim)\n",
    "        self.token_emb = torch_nn.Embedding(num_tokens, dim)\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for _ in range(depth):\n",
    "            transformer_blocks.append(TransformerBlock(dim=dim, heads=heads))\n",
    "\n",
    "        # The Sequential wrapper is convenient when you want to repeat similar blocks.\n",
    "        # A down-side is that it is harder access intermediate values for debugging.\n",
    "        self.transformer_blocks = torch_nn.Sequential(*transformer_blocks)\n",
    "\n",
    "        # The last part is problem specific. Here we want to map our transformer embeddings\n",
    "        # to a probability distribution.\n",
    "        # We will use a linear layer to produce log logits (the input to a  log-softmax function).\n",
    "        self.output_map = torch_nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Transformer forward method\n",
    "\n",
    "        Args:\n",
    "            x Tensor(batch_size, seq_length): Word indices representing sequence of words.\n",
    "        Returns:\n",
    "            Tensor(batch_size, num_classes): Log logits\n",
    "        \"\"\"\n",
    "        tokens = self.token_emb(x)\n",
    "        batch_size, seq_length, dim = tokens.size()\n",
    "\n",
    "        # Note that we create a completely new tensor which must be moved to the proper device.\n",
    "        # This is why we must store the device in self.device.\n",
    "        pos = torch.arange(seq_length, device=self.device)\n",
    "        pos = self.pos_emb(pos)[None, :, :].expand(batch_size, seq_length, dim)\n",
    "\n",
    "        x = tokens + pos\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        x = self.output_map(x.mean(dim=1))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26237d0c1cce97be356e04085a87fc75",
     "grade": false,
     "grade_id": "cell-957262dc380f1c7c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now, for the train./val loop. This can be written in many ways but based on common misstakes in HA1, hints might be in order:\n",
    "\n",
    "- Separate your code into smaller pieces, i.e. functions. It makes it easier to find bugs and easier to reuse code.\n",
    "- Use separate functions to calculate metrics. If you want to calculate, say accuracy, during both training and validation, don't copy the code. Write one function and make sure that it works, then reuse it.\n",
    "- Adding measurements to a running metrics can be tricky. Below is a solution that is a bit overkill but that is okay, since it is hard to use it incorrectly.\n",
    "\n",
    "Note 1: the code below can be modified so that you can play around with it.\n",
    "\n",
    "Note 2: timing this on Azure, a single epoch took ~5 min. Feel free to reduce the number of epochs or just study the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d76c90279076a8b6b6db7db8fd6a8419",
     "grade": true,
     "grade_id": "cell-db83c03e65ea6f71",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch: 1/5: time: 305.8, train loss: 0.698, train acc: 0.514, val. loss 0.681, val. acc: 0.513\n",
      "Epoch: 2/5: time: 305.5, train loss: 0.629, train acc: 0.639, val. loss 0.594, val. acc: 0.725\n",
      "Epoch: 3/5: time: 305.7, train loss: 0.536, train acc: 0.735, val. loss 0.501, val. acc: 0.762\n",
      "Epoch: 4/5: time: 306.2, train loss: 0.472, train acc: 0.777, val. loss 0.484, val. acc: 0.794\n",
      "Epoch: 5/5: time: 305.1, train loss: 0.419, train acc: 0.812, val. loss 0.474, val. acc: 0.804\n",
      "You have now trained a transformer!\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, max_seq_len):\n",
    "    \"\"\"Train epoch\"\"\"\n",
    "    train_loss = AccumulatingMetric()\n",
    "    train_acc = AccumulatingMetric()\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_, label = batch.text[0], batch.label - 1\n",
    "\n",
    "        input_ = _truncate_input(input_, max_seq_len)\n",
    "        pred = model(input_)\n",
    "        loss = F.nll_loss(pred, label)\n",
    "        loss.backward()\n",
    "        train_loss.add(loss.item())\n",
    "\n",
    "        train_acc.add(accuracy(pred, label))\n",
    "\n",
    "        # Gradient clipping is a way to ensure\n",
    "        # torch_nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return train_loss.avg(), train_acc.avg()\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, max_seq_len):\n",
    "    val_loss = AccumulatingMetric()\n",
    "    val_acc = AccumulatingMetric()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_, label = batch.text[0], batch.label - 1\n",
    "\n",
    "            input_ = _truncate_input(input_, max_seq_len)\n",
    "            pred = model(input_)\n",
    "            val_loss.add(F.nll_loss(pred, label).item())\n",
    "\n",
    "            val_acc.add(accuracy(pred, label))\n",
    "\n",
    "    return val_loss.avg(), val_acc.avg()  # TODO: loss\n",
    "\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    hard_pred = pred.argmax(1)\n",
    "    return (hard_pred == label).float().mean().item()\n",
    "\n",
    "\n",
    "def _truncate_input(input_, max_seq_len):\n",
    "    if input_.size(1) > max_seq_len:\n",
    "        input_ = input_[:, :max_seq_len]\n",
    "    return input_\n",
    "\n",
    "\n",
    "class AccumulatingMetric:\n",
    "    \"\"\"Accumulate samples of a metric and automatically keep track of the number of samples.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metric = 0.0\n",
    "        self.counter = 0\n",
    "\n",
    "    def add(self, value):\n",
    "        self.metric += value\n",
    "        self.counter += 1\n",
    "        \n",
    "    def avg(self):\n",
    "        return self.metric / self.counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_tokens = 50_000\n",
    "max_length = 512\n",
    "embedding_size = 128\n",
    "num_heads = 8\n",
    "num_classes = 2\n",
    "depth = 6\n",
    "\n",
    "model = Transformer(\n",
    "    dim=embedding_size,\n",
    "    heads=num_heads,\n",
    "    depth=depth,\n",
    "    seq_length=max_length,\n",
    "    num_tokens=num_tokens,\n",
    "    num_classes=num_classes,\n",
    "    device=device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "lr_warmup = 1e4\n",
    "num_epochs = 5\n",
    "batch_size = 6\n",
    "\n",
    "train_loader, test_loader = get_loaders(num_tokens, batch_size, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "# A scheduler is a principled way of controlling (often decreasing) the learning rate as time progresses.\n",
    "# Read more: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: min(i / (lr_warmup / batch_size), 1.0)\n",
    ")\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start = time()\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, max_length)\n",
    "    val_loss, val_acc = validate_epoch(model, test_loader, max_length)\n",
    "    end = time()\n",
    "    print(\n",
    "        \"Epoch: {}/{}: time: {:.1f}, train loss: {:.3f}, train acc: {:.3f}, val. loss {:.3f}, val. acc: {:.3f}\".format(\n",
    "            epoch, num_epochs, end - start, train_loss, train_acc, val_loss, val_acc\n",
    "        )\n",
    "    )\n",
    "print(\"You have now trained a transformer!\")\n",
    "\n",
    "# I'm adding a ''# YOUR CODE HERE' tag so that the code above is not hidden when the assignment is generated.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f311c2e99d7752a8d8ef63761947fe02",
     "grade": false,
     "grade_id": "cell-80c4afb2321f429a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Transformers and RNN\n",
    "\n",
    "Now, that you have gotten a practical feel for the transformer it is time to reflect on some of its important properties:\n",
    "\n",
    "**(2 POE)** Why are the significant differences between a transformer and an RNN?\n",
    "In particular, how do the differences make it easier to train a transformer, compared to an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fb3b20db662b0fee51e22cb504f318a",
     "grade": true,
     "grade_id": "cell-34c7a1017392b6bc",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "- Both RNNs and Transformers are designed to handle sequential data. One difference between them is however that Transformers do not require that the sequential data be processed in order, in difference to RNN. This indicated that if the input data for example is a sentence, the Transformer does not need to process the beginning of it before the end since Transformers are non-sequential (process the whole sentence rather than word by word). This makes Transformers able to perform parallelization, which decreases the training time.\n",
    "\n",
    "\n",
    "- Transformers also has the attention mechanism, which the RNNs lack. The attention mechanism doesn't suffer from short term memory, i.e. it can access words generated earlier in the sequence. RNN’s have a shorter window to reference from, so when the sentences/input text gets longer, they can not access words generated earlier in the sequence. LSTMs and GRUs has a bigger capcity to capture long term dependencies, however they still fail when the input sequence is to long.\n",
    "\n",
    "\n",
    "- A transformer also has access to all of the hidden states of the entire encoding part, in contrast to the RNNs.\n",
    "\n",
    "\n",
    "- The main reason for why its easier to train a transformer compared to a RNN is due to transformers avoiding recursion by enabling paralellization in computations as well as increasing performance due to long term dependencies. The underlying differences are the that transformers are non-sequential, transformers has self-attention and that both multi-head attention as well as positional embeddings provides information about the relationships between words. All of these characteristicts together, makes the training of a transformer compared to a RNN easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93a1649c87cd6998dc859a76ba077425",
     "grade": false,
     "grade_id": "cell-ae42248d6749eff6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** Self-attention maps sets to sets. It is an important part of what makes transformers so useful and general. Explain what this property means?\n",
    "\n",
    "Ironically, this property is actually a bit of an issue when we want to process text (or any NLP problem).\n",
    "Why, and how do we try to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2aa39c293a378a15af1618551c992fa",
     "grade": true,
     "grade_id": "cell-5cc2112f32acf469",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** \n",
    "\n",
    "This property means that the transformers do not require the sequential data to be processed in order and therefor does not take the order of the inputs into account. The transformers doesn't have recurrence such as for example the RNNs. \n",
    "\n",
    "In a NLP problem, this can become an issue when the input is a sentence with several words, since the order of the words will not be taken into account in the transformer. This problem is however taken care of by positional encoding, where one include information about the positions of the words into the input embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09c94fdea543e09bd55225a780f4090b",
     "grade": false,
     "grade_id": "cell-08639295de373894",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Wrapping up\n",
    "\n",
    "The transformer architecture has become incredibly popular and has produced truly amazing results.\n",
    "You should now have a good insight for how they can be implemented in `pytorch`. If you are interested, here are some more resources:\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "- https://github.com/huggingface/transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
